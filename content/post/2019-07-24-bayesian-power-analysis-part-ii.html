---
title: 'Bayesian power analysis: Part II. Some might prefer precision to power'
author: A. Solomon Kurz
date: '2019-07-24'
slug: bayesian-power-analysis-part-ii
categories: []
tags:
  - Bayesian
  - brms
  - power
  - R
  - tutorial
  - tidyverse
authors: []
header:
  caption: ''
  image: ''
  preview: yes
---



<div id="version-1.0.0" class="section level2">
<h2>Version 1.0.0</h2>
</div>
<div id="tldr" class="section level2">
<h2>tl;dr</h2>
<p>When researchers decide on a sample size for an upcoming project, there are more things to consider than null-hypothesis-oriented power. Bayesian researchers might like to frame their concerns in terms of precision. Stick around to learn what and how.</p>
</div>
<div id="are-bayesians-doomed-to-refer-to-h_0-1-with-sample-size-planning" class="section level2">
<h2>Are Bayesians doomed to refer to <span class="math inline">\(H_0\)</span> <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> with sample-size planning?</h2>
<p>If you read my last post, you may have found yourself thinking: <em>Sure, last time you avoided computing <span class="math inline">\(p\)</span>-values with your 95% Bayesian credible intervals. But weren’t you still operating like a NHSTesting frequentist with all that <span class="math inline">\(H_0 / H_1\)</span> talk?</em></p>
<p>Solid criticism. We didn’t even bother discussing all the type-I versus type-II error details. Yet they too were lurking in the background the way we just chose the typical .8 power benchmark. That’s not to say the null-hypothesis-oriented approach isn’t legitimate. It’s certainly congruent with what most reviewers would expect. But this all seems at odds with a model-oriented Bayesian approach, which is what I generally prefer. Happily, we have other options to explore.</p>
</div>
<div id="lets-just-pick-up-where-we-left-off." class="section level2">
<h2>Let’s just pick up where we left off.</h2>
<p>Load our primary statistical packages.</p>
<pre class="r"><code>library(tidyverse)
library(brms)
library(broom)</code></pre>
<p>As a recap, here’s how we performed the last simulation-based Bayesian power analysis from part I. First, we simulated a single data set and fit an initial model.</p>
<pre class="r"><code># define the means
mu_c &lt;- 0
mu_t &lt;- 0.5

# determine the group size
n &lt;- 50

# simulate the data
set.seed(1)
d &lt;-
  tibble(group     = rep(c(&quot;control&quot;, &quot;treatment&quot;), each = n)) %&gt;% 
  mutate(treatment = ifelse(group == &quot;control&quot;, 0, 1),
         y         = ifelse(group == &quot;control&quot;, 
                            rnorm(n, mean = mu_c, sd = 1),
                            rnorm(n, mean = mu_t, sd = 1)))
# fit the model
fit &lt;-
  brm(data = d,
      family = gaussian,
      y ~ 0 + intercept + treatment,
      prior = c(prior(normal(0, 2), class = b),
                prior(student_t(3, 1, 1), class = sigma)),
      seed = 1)</code></pre>
<p>Next, we made a custom function that both simulated data sets and used the <code>update()</code> function to update that initial fit in order to avoid additional compilation time.</p>
<pre class="r"><code>sim_d_and_fit &lt;- function(seed, n) {
  
  mu_c &lt;- 0
  mu_t &lt;- 0.5
  
  set.seed(seed)
  
  d &lt;-
    tibble(group     = rep(c(&quot;control&quot;, &quot;treatment&quot;), each = n)) %&gt;% 
    mutate(treatment = ifelse(group == &quot;control&quot;, 0, 1),
           y         = ifelse(group == &quot;control&quot;, 
                              rnorm(n, mean = mu_c, sd = 1),
                              rnorm(n, mean = mu_t, sd = 1)))
  
  update(fit,
         newdata = d, 
         seed = seed) %&gt;% 
    tidy(prob = .95) %&gt;% 
    filter(term == &quot;b_treatment&quot;)
}</code></pre>
<p>Then we finally iterated over <code>n_sim &lt;- 100</code> times.</p>
<pre class="r"><code>n_sim &lt;- 100

s3 &lt;-
  tibble(seed = 1:n_sim) %&gt;% 
  mutate(tidy = map(seed, sim_d_and_fit, n = 50)) %&gt;% 
  unnest(tidy)</code></pre>
<p>The results looked like so:</p>
<pre class="r"><code>theme_set(theme_grey() +
            theme(panel.grid = element_blank()))

s3 %&gt;% 
  ggplot(aes(x = seed, y = estimate, ymin = lower, ymax = upper)) +
  geom_hline(yintercept = c(0, .5), color = &quot;white&quot;) +
  geom_pointrange(fatten = 1/2) +
  labs(x = &quot;seed (i.e., simulation index)&quot;,
       y = expression(beta[1]))</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-3-1.png" width="768" /></p>
<p>It’s time to build on the foundation.</p>
</div>
<div id="we-might-evaluate-power-by-widths." class="section level2">
<h2>We might evaluate “power” by widths.</h2>
<p>Instead of just ordering the point-ranges by their <code>seed</code> values, we might instead arrange them by the <code>lower</code> levels.</p>
<pre class="r"><code>s3 %&gt;%
  ggplot(aes(x = reorder(seed, lower), y = estimate, ymin = lower, ymax = upper)) +
  geom_hline(yintercept = c(0, .5), color = &quot;white&quot;) +
  geom_pointrange(fatten = 1/2) +
  scale_x_discrete(&quot;reordered by the lower level of the 95% intervals&quot;, breaks = NULL) +
  ylab(expression(beta[1])) +
  coord_cartesian(ylim = c(-.5, 1.3))</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-4-1.png" width="768" /></p>
<p>Notice how this arrangement highlights the differences in widths among the intervals. The wider the interval, the less precise the estimate. Some intervals were wider than others, but all tended to hover in a similar range. We might quantify those ranges by computing a <code>width</code> variable.</p>
<pre class="r"><code>s3 &lt;-
  s3 %&gt;% 
  mutate(width = upper - lower)

head(s3)</code></pre>
<pre><code>## # A tibble: 6 x 7
##    seed term        estimate std.error   lower upper width
##   &lt;int&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1 b_treatment    0.515     0.182  0.136  0.868 0.732
## 2     2 b_treatment    0.299     0.238 -0.168  0.768 0.936
## 3     3 b_treatment    0.644     0.171  0.312  0.985 0.674
## 4     4 b_treatment    0.223     0.184 -0.132  0.580 0.712
## 5     5 b_treatment    0.426     0.194  0.0535 0.815 0.762
## 6     6 b_treatment    0.303     0.214 -0.113  0.720 0.833</code></pre>
<p>Here’s the <code>width</code> distribution.</p>
<pre class="r"><code>s3 %&gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .01)</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-6-1.png" width="384" /></p>
<p>The widths of our 95% intervals range from 0.6 to 0.95, with the bulk sitting around 0.8. Let’s focus a bit and take a random sample from one of the simulation iterations.</p>
<pre class="r"><code>set.seed(1)

s3 %&gt;% 
  sample_n(1) %&gt;% 
  mutate(seed = seed %&gt;% as.character()) %&gt;% 

  ggplot(aes(x = seed, y = estimate, ymin = lower, ymax = upper)) +
  geom_hline(yintercept = c(0, .5), color = &quot;white&quot;) +
  geom_pointrange() +
  coord_flip() +
  labs(x = &quot;seed #&quot;,
       y = expression(beta[1]))</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-7-1.png" width="384" /></p>
<p>Though the posterior mean suggests the most probable value for <span class="math inline">\(\beta_1\)</span> is about 0.6, the intervals suggest values from about 0.2 to almost 1 are within the 95% probability range. That’s a wide spread. Within psychology, a standardized mean difference of 0.2 would typically be considered small, whereas a difference of 1 would be large enough to raise a skeptical eyebrow or two.</p>
<p>So instead of focusing on rejecting a null hypothesis like <span class="math inline">\(\mu_\text{control} = \mu_\text{treatment}\)</span>, we might instead use our simulation skills to determine the sample size we need to have most of our 95% intervals come in at a certain level of precision. This has been termed the accuracy in parameter estimation (AIPE; <a href="https://www3.nd.edu/~kkelley/publications/articles/Maxwell_Kelley_Rausch_2008.pdf">Maxwell, Kelley, &amp; Rausch, 2008</a>; see also <a href="http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/">Kruschke, 2014</a>) approach to sample size planning.</p>
<p>Thinking in terms of AIPE, in terms of precision, let’s say we wanted widths of 0.7 or smaller. Here’s how we did with <code>s3</code>.</p>
<pre class="r"><code>s3 %&gt;% 
  mutate(check = ifelse(width &lt; .7, 1, 0)) %&gt;% 
  summarise(`width power` = mean(check))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   `width power`
##           &lt;dbl&gt;
## 1          0.08</code></pre>
<p>We did terrible. I’m not sure the term “width power” is even a thing. But hopefully you get the point. Our baby 100-iteration simulation suggests we have about a .08 probability of achieving 95% CI widths of 0.7 or smaller with <span class="math inline">\(n = 50\)</span> per group. Though we’re pretty good at excluding zero, we don’t tend to do so with precision above that.</p>
<p>That last bit about excluding zero brings up an important point. Once we’re concerned about width size, about precision, the null hypothesis is no longer of direct relevance. And since we’re no longer wed to thinking in terms of the null hypothesis, there’s no real need to stick with a .8 threshold for evaluating width power (okay, I’ll stop using that term). Now if we wanted to stick with .8, we could. Though a little nonsensical, the .8 criterion would give our AIPE analyses a sense of familiarity with traditional power analyses, which some reviewers might appreciate. But in his text, Kruschke mentioned several other alternatives. One would be to set maximum value for our CI widths and simulate to find the <span class="math inline">\(n\)</span> necessary so all our simulations pass that criterion. Another would follow Joseph, Wolfson, and du Berger (<a href="http://www.medicine.mcgill.ca/epidemiology/Joseph/publications%5CMethodological%5Css_binom.pdf">1995a</a>, <a href="http://www.med.mcgill.ca/epidemiology/Joseph/publications/Methodological/ss_hpd.pdf">1995b</a>), who suggested we shoot for an <span class="math inline">\(n\)</span> that produces widths that pass that criterion on average. Here’s how we did based on the average-width criterion.</p>
<pre class="r"><code>s3 %&gt;% 
  summarise(`average width` = mean(width))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   `average width`
##             &lt;dbl&gt;
## 1           0.786</code></pre>
<p>Close. Let’s see how increasing our sample size to 75 per group effects these metrics.</p>
<pre class="r"><code>s4 &lt;-
  tibble(seed = 1:n_sim) %&gt;% 
  mutate(tidy = map(seed, sim_d_and_fit, n = 75)) %&gt;% 
  unnest(tidy) %&gt;%
  mutate(width = upper - lower)</code></pre>
<p>Here’s what our new batch of 95% intervals looks like.</p>
<pre class="r"><code>s4 %&gt;% 
  ggplot(aes(x = reorder(seed, lower), y = estimate, ymin = lower, ymax = upper)) +
  geom_hline(yintercept = c(0, .5), color = &quot;white&quot;) +
  geom_pointrange(fatten = 1/2) +
  scale_x_discrete(&quot;reordered by the lower level of the 95% intervals&quot;, breaks = NULL) +
  ylab(expression(beta[1])) +
  # this kept the scale on the y-axis the same as the simulation with n = 50
  coord_cartesian(ylim = c(-.5, 1.3))</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-10-1.png" width="768" /></p>
<p>Some of the intervals are still more precise than others, but they all now hover more tightly around their true data-generating value of 0.5. Here’s our updated “power” for producing interval widths smaller than 0.7.</p>
<pre class="r"><code>s4 %&gt;% 
  mutate(check = ifelse(width &lt; .7, 1, 0)) %&gt;% 
  summarise(`proportion below 0.7` = mean(check),
            `average width`        = mean(width))</code></pre>
<pre><code>## # A tibble: 1 x 2
##   `proportion below 0.7` `average width`
##                    &lt;dbl&gt;           &lt;dbl&gt;
## 1                   0.95           0.638</code></pre>
<p>If we hold to the NHST-oriented .8 threshold, we did great and are even “overpowered”. We didn’t quite meet Kruschke’s strict limiting-worst-precision threshold, but we got close enough we’d have a good sense of what range of <span class="math inline">\(n\)</span> values we might evaluate over next. As far as the mean-precision criterion, we did great by that one and even beat it by about 0.04.</p>
<p>Here’s a look at how this batch of widths is distributed.</p>
<pre class="r"><code>s4 %&gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .02) +
  geom_rug(size = 1/6)</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-12-1.png" width="384" /></p>
<p>Let’s see if we can nail down the <span class="math inline">\(n\)</span>s for our three AIPE criteria. Since we’re so close to fulfilling Kruschke’s limiting-worst-precision criterion, we’ll start there. I’m thinking <span class="math inline">\(n = 85\)</span> should just about do it.</p>
<pre class="r"><code>s5 &lt;-
  tibble(seed = 1:n_sim) %&gt;% 
  mutate(tidy = map(seed, sim_d_and_fit, n = 85)) %&gt;% 
  unnest(tidy) %&gt;%
  mutate(width = upper - lower)</code></pre>
<p>Did we pass?</p>
<pre class="r"><code>s5 %&gt;% 
  mutate(check = ifelse(width &lt; .7, 1, 0)) %&gt;% 
  summarise(`proportion below 0.7` = mean(check))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   `proportion below 0.7`
##                    &lt;dbl&gt;
## 1                      1</code></pre>
<p>Success! We might look at how they’re distributed.</p>
<pre class="r"><code>s5 %&gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .01) +
  geom_rug(size = 1/6)</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-14-1.png" width="384" /></p>
<p>Two of our simulated widths were pretty close to the 0.7 boundary. If we were to do a proper simulation with 1000+ iterations, I’d worry one or two would creep over that boundary. So perhaps <span class="math inline">\(n = 90\)</span> would be a better candidate for a large-scale simulation.</p>
<p>If we just wanted to meet the mean-precision criterion, we might look at something like <span class="math inline">\(n = 65\)</span>.</p>
<pre class="r"><code>s6 &lt;-
  tibble(seed = 1:n_sim) %&gt;% 
  mutate(tidy = map(seed, sim_d_and_fit, n = 65)) %&gt;% 
  unnest(tidy) %&gt;%
  mutate(width = upper - lower)</code></pre>
<p>Did we pass?</p>
<pre class="r"><code>s6 %&gt;% 
  summarise(`average width` = mean(width))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   `average width`
##             &lt;dbl&gt;
## 1           0.686</code></pre>
<p>We got it! It looks like something like <span class="math inline">\(n = 65\)</span> would be a good candidate for a larger-scale simulation. Here’s the distribution.</p>
<pre class="r"><code>s6 %&gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .02) +
  geom_rug(size = 1/6)</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-16-1.png" width="384" /></p>
<p>For our final possible criterion, just get .8 of the widths below the threshold, we’ll want an <span class="math inline">\(n\)</span> somewhere between 65 and 85. 70, perhaps?</p>
<pre class="r"><code>s7 &lt;-
  tibble(seed = 1:n_sim) %&gt;% 
  mutate(tidy = map(seed, sim_d_and_fit, n = 70)) %&gt;% 
  unnest(tidy) %&gt;%
  mutate(width = upper - lower)</code></pre>
<p>Did we pass?</p>
<pre class="r"><code>s7 %&gt;% 
  mutate(check = ifelse(width &lt; .7, 1, 0)) %&gt;% 
  summarise(`proportion below 0.7` = mean(check))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   `proportion below 0.7`
##                    &lt;dbl&gt;
## 1                    0.8</code></pre>
<p>Yep. Here’s the distribution.</p>
<pre class="r"><code>s7 %&gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .02) +
  geom_rug(size = 1/6)</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-18-1.png" width="384" /></p>
</div>
<div id="how-are-we-defining-our-widths" class="section level2">
<h2>How are we defining our widths?</h2>
<p>In frequentist analyses, we typically work with 95% confidence intervals because of their close connection to the conventional <span class="math inline">\(p &lt; .05\)</span> threshold. Another consequence of dropping our focus on rejecting <span class="math inline">\(H_0\)</span> is that it no longer seems necessary to evaluate our posteriors with 95% intervals. And as it turns out, some Bayesians aren’t fans of the 95% interval. McElreath, for example, defiantly used 89% intervals in his <a href="http://xcelab.net/rm/statistical-rethinking/">text</a>. In contrast, Gelman has <a href="https://statmodeling.stat.columbia.edu/2016/11/05/why-i-prefer-50-to-95-intervals/">blogged</a> on his fondness for 50% intervals. Just for kicks, let’s follow Gelman’s lead and practice evaluating an <span class="math inline">\(n\)</span> based on 50% intervals. This will require us to update our <code>sim_d_and_fit()</code> function to allow us to change the <code>prob</code> setting in the <code>broom::tidy()</code> function.</p>
<pre class="r"><code>sim_d_and_fit &lt;- function(seed, n, prob) {
  
  mu_c &lt;- 0
  mu_t &lt;- 0.5
  
  set.seed(seed)
  
  d &lt;-
    tibble(group     = rep(c(&quot;control&quot;, &quot;treatment&quot;), each = n)) %&gt;% 
    mutate(treatment = ifelse(group == &quot;control&quot;, 0, 1),
           y         = ifelse(group == &quot;control&quot;, 
                              rnorm(n, mean = mu_c, sd = 1),
                              rnorm(n, mean = mu_t, sd = 1)))
  
  update(fit,
         newdata = d, 
         seed = seed) %&gt;% 
    tidy(prob = prob) %&gt;% 
    filter(term == &quot;b_treatment&quot;)
}</code></pre>
<p>Now simulate to examine those 50% intervals. We’ll start with the original <span class="math inline">\(n = 50\)</span></p>
<pre class="r"><code>n_sim &lt;- 100

s8 &lt;-
  tibble(seed = 1:n_sim) %&gt;% 
  mutate(tidy = map(seed, sim_d_and_fit, n = 50, prob = .5)) %&gt;% 
  unnest(tidy) %&gt;% 
  mutate(width = upper - lower)</code></pre>
<p>Here is the distribution of our 50% interval widths.</p>
<pre class="r"><code>s8 %&gt;% 
  mutate(width = upper - lower) %&gt;% 
  
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .01) +
  geom_rug(size = 1/6)</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-20-1.png" width="384" /></p>
<p>Since we’ve gone from 95% to 50% intervals, it should be no surprise that their widths are substantially more narrow. Accordingly, we should evaluate then with a higher standard. Perhaps it’s more reasonable to ask for an average width of 0.1. Let’s see how close <span class="math inline">\(n = 150\)</span> gets us.</p>
<pre class="r"><code>s9 &lt;-
  tibble(seed = 1:n_sim) %&gt;% 
  mutate(tidy = map(seed, sim_d_and_fit, n = 150, prob = .5)) %&gt;% 
  unnest(tidy) %&gt;% 
  mutate(width = upper - lower)</code></pre>
<p>Look at the distribution.</p>
<pre class="r"><code>s9 %&gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .0025) +
  geom_rug(size = 1/6)</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-21-1.png" width="384" /></p>
<p>Nope, we’re not there yet. Perhaps <span class="math inline">\(n = 200\)</span> or <span class="math inline">\(250\)</span> is the ticket. This is an iterative process. Anyway, once we’re talking that AIPE/precision/interval-width talk, we can get all kinds of creative with which intervals we’re even interested in. As far as I can tell, the topic is wide open for fights and collaborations between statisticians, methodologists, and substantive researchers to find sensible ways forward.</p>
<p>Maybe you should write a dissertation on it.</p>
<p>Regardless, stay tuned for part III where we’ll liberate ourselves from the tyranny of the Gauss.</p>
</div>
<div id="session-info" class="section level2">
<h2>Session info</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.6.2 (2019-12-12)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] broom_0.5.3     brms_2.12.0     Rcpp_1.0.3      forcats_0.4.0  
##  [5] stringr_1.4.0   dplyr_0.8.4     purrr_0.3.3     readr_1.3.1    
##  [9] tidyr_1.0.2     tibble_2.1.3    ggplot2_3.2.1   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] colorspace_1.4-1     ggridges_0.5.2       rsconnect_0.8.16    
##   [4] markdown_1.1         base64enc_0.1-3      fs_1.3.1            
##   [7] rstudioapi_0.10      farver_2.0.3         rstan_2.19.2        
##  [10] DT_0.11              fansi_0.4.1          mvtnorm_1.0-12      
##  [13] lubridate_1.7.4      xml2_1.2.2           bridgesampling_0.8-1
##  [16] codetools_0.2-16     knitr_1.26           shinythemes_1.1.2   
##  [19] bayesplot_1.7.1      jsonlite_1.6.1       dbplyr_1.4.2        
##  [22] shiny_1.4.0          compiler_3.6.2       httr_1.4.1          
##  [25] backports_1.1.5      assertthat_0.2.1     Matrix_1.2-18       
##  [28] fastmap_1.0.1        lazyeval_0.2.2       cli_2.0.1           
##  [31] later_1.0.0          htmltools_0.4.0      prettyunits_1.1.1   
##  [34] tools_3.6.2          igraph_1.2.4.2       coda_0.19-3         
##  [37] gtable_0.3.0         glue_1.3.1           reshape2_1.4.3      
##  [40] cellranger_1.1.0     vctrs_0.2.2          nlme_3.1-142        
##  [43] blogdown_0.17        crosstalk_1.0.0      xfun_0.12           
##  [46] ps_1.3.0             rvest_0.3.5          mime_0.8            
##  [49] miniUI_0.1.1.1       lifecycle_0.1.0      gtools_3.8.1        
##  [52] zoo_1.8-7            scales_1.1.0         colourpicker_1.0    
##  [55] hms_0.5.3            promises_1.1.0       Brobdingnag_1.2-6   
##  [58] parallel_3.6.2       inline_0.3.15        shinystan_2.5.0     
##  [61] yaml_2.2.1           gridExtra_2.3        loo_2.2.0           
##  [64] StanHeaders_2.19.0   stringi_1.4.6        dygraphs_1.1.1.6    
##  [67] pkgbuild_1.0.6       rlang_0.4.5          pkgconfig_2.0.3     
##  [70] matrixStats_0.55.0   evaluate_0.14        lattice_0.20-38     
##  [73] labeling_0.3         rstantools_2.0.0     htmlwidgets_1.5.1   
##  [76] tidyselect_1.0.0     processx_3.4.1       plyr_1.8.5          
##  [79] magrittr_1.5         bookdown_0.17        R6_2.4.1            
##  [82] generics_0.0.2       DBI_1.1.0            pillar_1.4.3        
##  [85] haven_2.2.0          withr_2.1.2          xts_0.12-0          
##  [88] abind_1.4-5          modelr_0.1.5         crayon_1.3.4        
##  [91] utf8_1.1.4           rmarkdown_2.0        grid_3.6.2          
##  [94] readxl_1.3.1         callr_3.4.1          threejs_0.3.3       
##  [97] reprex_0.3.0         digest_0.6.23        xtable_1.8-4        
## [100] httpuv_1.5.2         stats4_3.6.2         munsell_0.5.0       
## [103] shinyjs_1.1</code></pre>
</div>
<div id="footnote" class="section level2">
<h2>Footnote</h2>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>To be clear, one can consider the null hypothesis within the Bayesian paradigm. I don’t tend to take this approach, but it’d be unfair not to at least mention some resources. Kurschke covered the topic in chapters 11 and 12 in his text, <a href="http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/"><em>Doing Bayesian Data Analysis, Second Edition:
A Tutorial with R, JAGS, and Stan</em></a>. You might also check out Rouder et al’s <a href="https://link.springer.com/content/pdf/10.3758/PBR.16.2.225.pdf"><em>Bayesian t tests for accepting and rejecting the null hypothesis</em></a> or Morey and Rouder’s <a href="https://s3.amazonaws.com/academia.edu.documents/45416179/Bayes_Factor_Approaches_for_Testing_Inte20160506-23207-1t89l96.pdf?response-content-disposition=inline%3B%20filename%3DBayes_factor_approaches_for_testing_inte.pdf&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWOWYYGZ2Y53UL3A%2F20190724%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20190724T201328Z&amp;X-Amz-Expires=3600&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=76ce5abb6bf9fb41c9c677038f79bb75e19e65bc21d97233519701035304d712"><em>Bayes Factor Approaches for Testing Interval Null Hypotheses</em></a>.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
