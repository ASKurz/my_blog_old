---
title: 'Bayesian power analysis: Part II. Some might prefer precision to power'
author: A. Solomon Kurz
date: '2019-07-24'
slug: bayesian-power-analysis-part-ii
categories: []
tags:
  - Bayesian
  - brms
  - power
  - R
  - tutorial
  - tidyverse
authors: []
header:
  caption: ''
  image: ''
  preview: yes
bibliography: my_blog.bib
biblio-style: apalike
csl: apa.csl  
link-citations: yes
output:
  blogdown::html_page:
    toc: true 
---


<div id="TOC">
<ul>
<li><a href="#version-1.0.1">Version 1.0.1</a></li>
<li><a href="#tldr">tl;dr</a></li>
<li><a href="#are-bayesians-doomed-to-refer-to-h_0-1-with-sample-size-planning">Are Bayesians doomed to refer to <span class="math inline">\(H_0\)</span> with sample-size planning?</a></li>
<li><a href="#lets-just-pick-up-where-we-left-off.">Let’s just pick up where we left off.</a></li>
<li><a href="#we-might-evaluate-power-by-widths.">We might evaluate “power” by widths.</a></li>
<li><a href="#how-are-we-defining-our-widths">How are we defining our widths?</a></li>
<li><a href="#session-info">Session info</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<div id="version-1.0.1" class="section level2">
<h2>Version 1.0.1</h2>
</div>
<div id="tldr" class="section level2">
<h2>tl;dr</h2>
<p>When researchers decide on a sample size for an upcoming project, there are more things to consider than null-hypothesis-oriented power. Bayesian researchers might like to frame their concerns in terms of precision. Stick around to learn what and how.</p>
</div>
<div id="are-bayesians-doomed-to-refer-to-h_0-1-with-sample-size-planning" class="section level2">
<h2>Are Bayesians doomed to refer to <span class="math inline">\(H_0\)</span><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> with sample-size planning?</h2>
<p>If you read the first post in this series (click <a href="https://solomonkurz.netlify.app/post/bayesian-power-analysis-part-i/">here</a> for a refresher), you may have found yourself thinking: <em>Sure, last time you avoided computing <span class="math inline">\(p\)</span>-values with your 95% Bayesian credible intervals. But weren’t you still operating like a NHSTesting frequentist with all that <span class="math inline">\(H_0 / H_1\)</span> talk?</em></p>
<p>Solid criticism. We didn’t even bother discussing all the type-I versus type-II error details. Yet they too were lurking in the background the way we just chose the typical .8 power benchmark. That’s not to say that a <span class="math inline">\(p\)</span>-value oriented approach isn’t legitimate. It’s certainly congruent with what most reviewers would expect.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> But this all seems at odds with a model-oriented Bayesian approach, which is what I generally prefer. Happily, we have other options to explore.</p>
</div>
<div id="lets-just-pick-up-where-we-left-off." class="section level2">
<h2>Let’s just pick up where we left off.</h2>
<p>Load our primary statistical packages.</p>
<pre class="r"><code>library(tidyverse)
library(brms)
library(broom)</code></pre>
<p>As a recap, here’s how we performed the last simulation-based Bayesian power analysis from part I. First, we simulated a single data set and fit an initial model.</p>
<pre class="r"><code># define the means
mu_c &lt;- 0
mu_t &lt;- 0.5

# determine the group size
n &lt;- 50

# simulate the data
set.seed(1)
d &lt;-
  tibble(group     = rep(c(&quot;control&quot;, &quot;treatment&quot;), each = n)) %&gt;% 
  mutate(treatment = ifelse(group == &quot;control&quot;, 0, 1),
         y         = ifelse(group == &quot;control&quot;, 
                            rnorm(n, mean = mu_c, sd = 1),
                            rnorm(n, mean = mu_t, sd = 1)))
# fit the model
fit &lt;-
  brm(data = d,
      family = gaussian,
      y ~ 0 + intercept + treatment,
      prior = c(prior(normal(0, 2), class = b),
                prior(student_t(3, 1, 1), class = sigma)),
      seed = 1)</code></pre>
<p>Next, we made a custom function that both simulated data sets and used the <code>update()</code> function to update that initial fit in order to avoid additional compilation time.</p>
<pre class="r"><code>sim_d_and_fit &lt;- function(seed, n) {
  
  mu_c &lt;- 0
  mu_t &lt;- 0.5
  
  set.seed(seed)
  
  d &lt;-
    tibble(group     = rep(c(&quot;control&quot;, &quot;treatment&quot;), each = n)) %&gt;% 
    mutate(treatment = ifelse(group == &quot;control&quot;, 0, 1),
           y         = ifelse(group == &quot;control&quot;, 
                              rnorm(n, mean = mu_c, sd = 1),
                              rnorm(n, mean = mu_t, sd = 1)))
  
  update(fit,
         newdata = d, 
         seed = seed) %&gt;% 
    tidy(prob = .95) %&gt;% 
    filter(term == &quot;b_treatment&quot;)
}</code></pre>
<p>Then we finally iterated over <code>n_sim &lt;- 100</code> times.</p>
<pre class="r"><code>n_sim &lt;- 100

s3 &lt;-
  tibble(seed = 1:n_sim) %&gt;% 
  mutate(tidy = map(seed, sim_d_and_fit, n = 50)) %&gt;% 
  unnest(tidy)</code></pre>
<p>The results looked like so:</p>
<pre class="r"><code>theme_set(theme_grey() +
            theme(panel.grid = element_blank()))

s3 %&gt;% 
  ggplot(aes(x = seed, y = estimate, ymin = lower, ymax = upper)) +
  geom_hline(yintercept = c(0, .5), color = &quot;white&quot;) +
  geom_pointrange(fatten = 1/2) +
  labs(x = &quot;seed (i.e., simulation index)&quot;,
       y = expression(beta[1]))</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-3-1.png" width="768" /></p>
<p>It’s time to build on the foundation.</p>
</div>
<div id="we-might-evaluate-power-by-widths." class="section level2">
<h2>We might evaluate “power” by widths.</h2>
<p>Instead of just ordering the point-ranges by their <code>seed</code> values, we might instead arrange them by the <code>lower</code> levels.</p>
<pre class="r"><code>s3 %&gt;%
  ggplot(aes(x = reorder(seed, lower), y = estimate, ymin = lower, ymax = upper)) +
  geom_hline(yintercept = c(0, .5), color = &quot;white&quot;) +
  geom_pointrange(fatten = 1/2) +
  scale_x_discrete(&quot;reordered by the lower level of the 95% intervals&quot;, breaks = NULL) +
  ylab(expression(beta[1])) +
  coord_cartesian(ylim = c(-.5, 1.3))</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-4-1.png" width="768" /></p>
<p>Notice how this arrangement highlights the differences in widths among the intervals. The wider the interval, the less precise the estimate. Some intervals were wider than others, but all tended to hover in a similar range. We might quantify those ranges by computing a <code>width</code> variable.</p>
<pre class="r"><code>s3 &lt;-
  s3 %&gt;% 
  mutate(width = upper - lower)

head(s3)</code></pre>
<pre><code>## # A tibble: 6 x 7
##    seed term        estimate std.error   lower upper width
##   &lt;int&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     1 b_treatment    0.513     0.183  0.153  0.874 0.721
## 2     2 b_treatment    0.300     0.241 -0.170  0.776 0.945
## 3     3 b_treatment    0.640     0.174  0.297  0.990 0.693
## 4     4 b_treatment    0.225     0.183 -0.135  0.583 0.717
## 5     5 b_treatment    0.432     0.194  0.0552 0.810 0.755
## 6     6 b_treatment    0.305     0.209 -0.101  0.714 0.815</code></pre>
<p>Here’s the <code>width</code> distribution.</p>
<pre class="r"><code>s3 %&gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .01)</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-6-1.png" width="384" /></p>
<p>The widths of our 95% intervals range from 0.6 to 0.95, with the bulk sitting around 0.8. Let’s focus a bit and take a random sample from one of the simulation iterations.</p>
<pre class="r"><code>set.seed(1)

s3 %&gt;% 
  sample_n(1) %&gt;% 
  mutate(seed = seed %&gt;% as.character()) %&gt;% 

  ggplot(aes(x = estimate, xmin = lower, xmax = upper, y = seed)) +
  geom_vline(xintercept = c(0, .5), color = &quot;white&quot;) +
  geom_pointrange() +
  labs(x = expression(beta[1]),
       y = &quot;seed #&quot;) +
  xlim(0, 1)</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-7-1.png" width="384" /></p>
<p>Though the posterior mean suggests the most probable value for <span class="math inline">\(\beta_1\)</span> is about 0.6, the intervals suggest values from about 0.2 to almost 1 are within the 95% probability range. That’s a wide spread. Within psychology, a standardized mean difference of 0.2 would typically be considered small, whereas a difference of 1 would be large enough to raise a skeptical eyebrow or two.</p>
<p>So instead of focusing on rejecting a null hypothesis like <span class="math inline">\(\mu_\text{control} = \mu_\text{treatment}\)</span>, we might instead use our simulation skills to determine the sample size we need to have most of our 95% intervals come in at a certain level of precision. This has been termed the accuracy in parameter estimation <span class="citation">(AIPE; Maxwell et al., <a href="#ref-maxwellSampleSizePlanning2008">2008</a>; see also Kruschke, <a href="#ref-kruschkeDoingBayesianData2015">2015</a>)</span> approach to sample size planning.</p>
<p>Thinking in terms of AIPE, in terms of precision, let’s say we wanted widths of 0.7 or smaller. Here’s how we did with <code>s3</code>.</p>
<pre class="r"><code>s3 %&gt;% 
  mutate(check = ifelse(width &lt; .7, 1, 0)) %&gt;% 
  summarise(`width power` = mean(check))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   `width power`
##           &lt;dbl&gt;
## 1          0.07</code></pre>
<p>We did terrible. I’m not sure the term “width power” is even a thing. But hopefully you get the point. Our baby 100-iteration simulation suggests we have about a .08 probability of achieving 95% CI widths of 0.7 or smaller with <span class="math inline">\(n = 50\)</span> per group. Though we’re pretty good at excluding zero, we don’t tend to do so with precision above that.</p>
<p>That last bit about excluding zero brings up an important point. Once we’re concerned about width size, about precision, the null hypothesis is no longer of direct relevance. And since we’re no longer wed to thinking in terms of the null hypothesis, there’s no real need to stick with a .8 threshold for evaluating width power (okay, I’ll stop using that term). Now if we wanted to stick with .8, we could. Though a little nonsensical, the .8 criterion would give our AIPE analyses a sense of familiarity with traditional power analyses, which some reviewers might appreciate. But in his text, Kruschke mentioned several other alternatives. One would be to set maximum value for our CI widths and simulate to find the <span class="math inline">\(n\)</span> necessary so all our simulations pass that criterion. Another would follow Joseph, Wolfson, and du Berger <span class="citation">(<a href="#ref-josephSampleSizeCalculations1995">1995</a><a href="#ref-josephSampleSizeCalculations1995">a</a>, <a href="#ref-josephCommentsBayesianSample1995">1995</a><a href="#ref-josephCommentsBayesianSample1995">b</a>)</span>, who suggested we shoot for an <span class="math inline">\(n\)</span> that produces widths that pass that criterion on average. Here’s how we did based on the average-width criterion.</p>
<pre class="r"><code>s3 %&gt;% 
  summarise(`average width` = mean(width))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   `average width`
##             &lt;dbl&gt;
## 1           0.784</code></pre>
<p>Close. Let’s see how increasing our sample size to 75 per group effects these metrics.</p>
<pre class="r"><code>s4 &lt;-
  tibble(seed = 1:n_sim) %&gt;% 
  mutate(tidy = map(seed, sim_d_and_fit, n = 75)) %&gt;% 
  unnest(tidy) %&gt;%
  mutate(width = upper - lower)</code></pre>
<p>Here’s what our new batch of 95% intervals looks like.</p>
<pre class="r"><code>s4 %&gt;% 
  ggplot(aes(x = reorder(seed, lower), y = estimate, ymin = lower, ymax = upper)) +
  geom_hline(yintercept = c(0, .5), color = &quot;white&quot;) +
  geom_pointrange(fatten = 1/2) +
  scale_x_discrete(&quot;reordered by the lower level of the 95% intervals&quot;, breaks = NULL) +
  ylab(expression(beta[1])) +
  # this kept the scale on the y-axis the same as the simulation with n = 50
  coord_cartesian(ylim = c(-.5, 1.3))</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-10-1.png" width="768" /></p>
<p>Some of the intervals are still more precise than others, but they all now hover more tightly around their true data-generating value of 0.5. Here’s our updated “power” for producing interval widths smaller than 0.7.</p>
<pre class="r"><code>s4 %&gt;% 
  mutate(check = ifelse(width &lt; .7, 1, 0)) %&gt;% 
  summarise(`proportion below 0.7` = mean(check),
            `average width`        = mean(width))</code></pre>
<pre><code>## # A tibble: 1 x 2
##   `proportion below 0.7` `average width`
##                    &lt;dbl&gt;           &lt;dbl&gt;
## 1                   0.96           0.639</code></pre>
<p>If we hold to the NHST-oriented .8 threshold, we did great and are even “overpowered”. We didn’t quite meet Kruschke’s strict limiting-worst-precision threshold, but we got close enough we’d have a good sense of what range of <span class="math inline">\(n\)</span> values we might evaluate over next. As far as the mean-precision criterion, we did great by that one and even beat it by about 0.04.</p>
<p>Here’s a look at how this batch of widths is distributed.</p>
<pre class="r"><code>s4 %&gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .02) +
  geom_rug(size = 1/6)</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-12-1.png" width="384" /></p>
<p>Let’s see if we can nail down the <span class="math inline">\(n\)</span>s for our three AIPE criteria. Since we’re so close to fulfilling Kruschke’s limiting-worst-precision criterion, we’ll start there. I’m thinking <span class="math inline">\(n = 85\)</span> should just about do it.</p>
<pre class="r"><code>s5 &lt;-
  tibble(seed = 1:n_sim) %&gt;% 
  mutate(tidy = map(seed, sim_d_and_fit, n = 85)) %&gt;% 
  unnest(tidy) %&gt;%
  mutate(width = upper - lower)</code></pre>
<p>Did we pass?</p>
<pre class="r"><code>s5 %&gt;% 
  mutate(check = ifelse(width &lt; .7, 1, 0)) %&gt;% 
  summarise(`proportion below 0.7` = mean(check))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   `proportion below 0.7`
##                    &lt;dbl&gt;
## 1                      1</code></pre>
<p>Success! We might look at how they’re distributed.</p>
<pre class="r"><code>s5 %&gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .01) +
  geom_rug(size = 1/6)</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-14-1.png" width="384" /></p>
<p>Two of our simulated widths were pretty close to the 0.7 boundary. If we were to do a proper simulation with 1,000+ iterations, I’d worry one or two would creep over that boundary. So perhaps <span class="math inline">\(n = 90\)</span> would be a better candidate for a large-scale simulation.</p>
<p>If we just wanted to meet the mean-precision criterion, we might look at something like <span class="math inline">\(n = 65\)</span>.</p>
<pre class="r"><code>s6 &lt;-
  tibble(seed = 1:n_sim) %&gt;% 
  mutate(tidy = map(seed, sim_d_and_fit, n = 65)) %&gt;% 
  unnest(tidy) %&gt;%
  mutate(width = upper - lower)</code></pre>
<p>Did we pass the mean-precision criterion?</p>
<pre class="r"><code>s6 %&gt;% 
  summarise(`average width` = mean(width))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   `average width`
##             &lt;dbl&gt;
## 1           0.690</code></pre>
<p>We got it! It looks like something like <span class="math inline">\(n = 65\)</span> would be a good candidate for a larger-scale simulation. Here’s the distribution.</p>
<pre class="r"><code>s6 %&gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .02) +
  geom_rug(size = 1/6)</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-16-1.png" width="384" /></p>
<p>For our final possible criterion, just get .8 of the widths below the threshold, we’ll want an <span class="math inline">\(n\)</span> somewhere between 65 and 85. 70, perhaps?</p>
<pre class="r"><code>s7 &lt;-
  tibble(seed = 1:n_sim) %&gt;% 
  mutate(tidy = map(seed, sim_d_and_fit, n = 70)) %&gt;% 
  unnest(tidy) %&gt;%
  mutate(width = upper - lower)</code></pre>
<p>Did we pass the .8-threshold criterion?</p>
<pre class="r"><code>s7 %&gt;% 
  mutate(check = ifelse(width &lt; .7, 1, 0)) %&gt;% 
  summarise(`proportion below 0.7` = mean(check))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   `proportion below 0.7`
##                    &lt;dbl&gt;
## 1                    0.8</code></pre>
<p>Yep. Here’s the distribution.</p>
<pre class="r"><code>s7 %&gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .02) +
  geom_rug(size = 1/6)</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-18-1.png" width="384" /></p>
</div>
<div id="how-are-we-defining-our-widths" class="section level2">
<h2>How are we defining our widths?</h2>
<p>In frequentist analyses, we typically work with 95% confidence intervals because of their close connection to the conventional <span class="math inline">\(p &lt; .05\)</span> threshold. Another consequence of dropping our focus on rejecting <span class="math inline">\(H_0\)</span> is that it no longer seems necessary to evaluate our posteriors with 95% intervals. And as it turns out, some Bayesians aren’t fans of the 95% interval. McElreath, for example, defiantly used 89% intervals in both editions of his <span class="citation">(<a href="#ref-mcelreathStatisticalRethinkingBayesian2020">2020</a>, <a href="#ref-mcelreathStatisticalRethinkingBayesian2015">2015</a>)</span> <a href="http://xcelab.net/rm/statistical-rethinking/">text</a>. In contrast, Gelman has <a href="https://statmodeling.stat.columbia.edu/2016/11/05/why-i-prefer-50-to-95-intervals/">blogged</a> on his fondness for 50% intervals. Just for kicks, let’s follow Gelman’s lead and practice evaluating an <span class="math inline">\(n\)</span> based on 50% intervals. This will require us to update our <code>sim_d_and_fit()</code> function to allow us to change the <code>prob</code> setting in the <code>broom::tidy()</code> function.</p>
<pre class="r"><code>sim_d_and_fit &lt;- function(seed, n, prob) {
  
  mu_c &lt;- 0
  mu_t &lt;- 0.5
  
  set.seed(seed)
  
  d &lt;-
    tibble(group     = rep(c(&quot;control&quot;, &quot;treatment&quot;), each = n)) %&gt;% 
    mutate(treatment = ifelse(group == &quot;control&quot;, 0, 1),
           y         = ifelse(group == &quot;control&quot;, 
                              rnorm(n, mean = mu_c, sd = 1),
                              rnorm(n, mean = mu_t, sd = 1)))
  
  update(fit,
         newdata = d, 
         seed = seed) %&gt;% 
    tidy(prob = prob) %&gt;% 
    filter(term == &quot;b_treatment&quot;)
}</code></pre>
<p>Now simulate to examine those 50% intervals. We’ll start with the original <span class="math inline">\(n = 50\)</span></p>
<pre class="r"><code>n_sim &lt;- 100

s8 &lt;-
  tibble(seed = 1:n_sim) %&gt;% 
  mutate(tidy = map(seed, sim_d_and_fit, n = 50, prob = .5)) %&gt;% 
  unnest(tidy) %&gt;% 
  mutate(width = upper - lower)</code></pre>
<p>Here is the distribution of our 50% interval widths.</p>
<pre class="r"><code>s8 %&gt;% 
  mutate(width = upper - lower) %&gt;% 
  
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .01) +
  geom_rug(size = 1/6)</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-20-1.png" width="384" /></p>
<p>Since we’ve gone from 95% to 50% intervals, it should be no surprise that their widths are narrower. Accordingly, we should evaluate then with a higher standard. Perhaps it’s more reasonable to ask for an average width of 0.1. Let’s see how close <span class="math inline">\(n = 150\)</span> gets us.</p>
<pre class="r"><code>s9 &lt;-
  tibble(seed = 1:n_sim) %&gt;% 
  mutate(tidy = map(seed, sim_d_and_fit, n = 150, prob = .5)) %&gt;% 
  unnest(tidy) %&gt;% 
  mutate(width = upper - lower)</code></pre>
<p>Look at the distribution.</p>
<pre class="r"><code>s9 %&gt;% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .0025) +
  geom_rug(size = 1/6)</code></pre>
<p><img src="/post/2019-07-24-bayesian-power-analysis-part-ii_files/figure-html/unnamed-chunk-21-1.png" width="384" /></p>
<p>Nope, we’re not there yet. Perhaps <span class="math inline">\(n = 200\)</span> or <span class="math inline">\(250\)</span> is the ticket. This is an iterative process. Anyway, once we’re talking that AIPE/precision/interval-width talk, we can get all kinds of creative with which intervals we’re even interested in. As far as I can tell, the topic is wide open for fights and collaborations between statisticians, methodologists, and substantive researchers to find sensible ways forward.</p>
<p>Maybe you should write a dissertation on it.</p>
<p>Regardless, get ready for <a href="https://solomonkurz.netlify.app/post/bayesian-power-analysis-part-iii-a/">part III</a> where we’ll liberate ourselves from the tyranny of the Gauss.</p>
</div>
<div id="session-info" class="section level2">
<h2>Session info</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.6.3 (2020-02-29)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Catalina 10.15.3
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] broom_0.5.5     brms_2.13.0     Rcpp_1.0.5      forcats_0.5.0  
##  [5] stringr_1.4.0   dplyr_1.0.1     purrr_0.3.4     readr_1.3.1    
##  [9] tidyr_1.1.1     tibble_3.0.3    ggplot2_3.3.2   tidyverse_1.3.0
## 
## loaded via a namespace (and not attached):
##   [1] TH.data_1.0-10       colorspace_1.4-1     ellipsis_0.3.1      
##   [4] ggridges_0.5.2       rsconnect_0.8.16     estimability_1.3    
##   [7] markdown_1.1         base64enc_0.1-3      fs_1.4.1            
##  [10] rstudioapi_0.11      farver_2.0.3         rstan_2.19.3        
##  [13] DT_0.13              fansi_0.4.1          mvtnorm_1.1-0       
##  [16] lubridate_1.7.8      xml2_1.3.1           codetools_0.2-16    
##  [19] splines_3.6.3        bridgesampling_1.0-0 knitr_1.28          
##  [22] shinythemes_1.1.2    bayesplot_1.7.1      jsonlite_1.7.0      
##  [25] dbplyr_1.4.2         shiny_1.5.0          compiler_3.6.3      
##  [28] httr_1.4.1           emmeans_1.4.5        backports_1.1.8     
##  [31] assertthat_0.2.1     Matrix_1.2-18        fastmap_1.0.1       
##  [34] cli_2.0.2            later_1.1.0.1        htmltools_0.5.0     
##  [37] prettyunits_1.1.1    tools_3.6.3          igraph_1.2.5        
##  [40] coda_0.19-3          gtable_0.3.0         glue_1.4.1          
##  [43] reshape2_1.4.4       cellranger_1.1.0     vctrs_0.3.2         
##  [46] nlme_3.1-144         blogdown_0.18        crosstalk_1.1.0.1   
##  [49] xfun_0.13            ps_1.3.4             rvest_0.3.5         
##  [52] mime_0.9             miniUI_0.1.1.1       lifecycle_0.2.0     
##  [55] gtools_3.8.2         MASS_7.3-51.5        zoo_1.8-7           
##  [58] scales_1.1.1         colourpicker_1.0     hms_0.5.3           
##  [61] promises_1.1.1       Brobdingnag_1.2-6    sandwich_2.5-1      
##  [64] parallel_3.6.3       inline_0.3.15        shinystan_2.5.0     
##  [67] yaml_2.2.1           gridExtra_2.3        loo_2.2.0           
##  [70] StanHeaders_2.21.0-1 stringi_1.4.6        dygraphs_1.1.1.6    
##  [73] pkgbuild_1.1.0       rlang_0.4.7          pkgconfig_2.0.3     
##  [76] matrixStats_0.56.0   evaluate_0.14        lattice_0.20-38     
##  [79] labeling_0.3         rstantools_2.0.0     htmlwidgets_1.5.1   
##  [82] tidyselect_1.1.0     processx_3.4.3       plyr_1.8.6          
##  [85] magrittr_1.5         bookdown_0.18        R6_2.4.1            
##  [88] generics_0.0.2       multcomp_1.4-13      DBI_1.1.0           
##  [91] pillar_1.4.6         haven_2.2.0          withr_2.2.0         
##  [94] xts_0.12-0           survival_3.1-12      abind_1.4-5         
##  [97] modelr_0.1.6         crayon_1.3.4         utf8_1.1.4          
## [100] rmarkdown_2.1        grid_3.6.3           readxl_1.3.1        
## [103] callr_3.4.3          threejs_0.3.3        reprex_0.3.0        
## [106] digest_0.6.25        xtable_1.8-4         httpuv_1.5.4        
## [109] stats4_3.6.3         munsell_0.5.0        shinyjs_1.1</code></pre>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-josephSampleSizeCalculations1995">
<p>Joseph, L., Wolfson, D. B., &amp; Berger, R. D. (1995a). Sample size calculations for binomial proportions via highest posterior density intervals. <em>Journal of the Royal Statistical Society: Series D (the Statistician)</em>, <em>44</em>(2), 143–154. <a href="https://doi.org/10.2307/2348439">https://doi.org/10.2307/2348439</a></p>
</div>
<div id="ref-josephCommentsBayesianSample1995">
<p>Joseph, L., Wolfson, D. B., &amp; Berger, R. D. (1995b). Some comments on Bayesian sample size determination. <em>Journal of the Royal Statistical Society: Series D (the Statistician)</em>, <em>44</em>(2), 167–171. <a href="https://doi.org/10.2307/2348442">https://doi.org/10.2307/2348442</a></p>
</div>
<div id="ref-kruschkeDoingBayesianData2015">
<p>Kruschke, J. K. (2015). <em>Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan</em>. Academic Press. <a href="https://sites.google.com/site/doingbayesiandataanalysis/">https://sites.google.com/site/doingbayesiandataanalysis/</a></p>
</div>
<div id="ref-maxwellSampleSizePlanning2008">
<p>Maxwell, S. E., Kelley, K., &amp; Rausch, J. R. (2008). Sample size planning for statistical power and accuracy in parameter estimation. <em>Annual Review of Psychology</em>, <em>59</em>(1), 537–563. <a href="https://doi.org/10.1146/annurev.psych.59.103006.093735">https://doi.org/10.1146/annurev.psych.59.103006.093735</a></p>
</div>
<div id="ref-mcelreathStatisticalRethinkingBayesian2020">
<p>McElreath, R. (2020). <em>Statistical rethinking: A Bayesian course with examples in R and Stan</em> (Second edition). CRC Press. <a href="https://xcelab.net/rm/statistical-rethinking/">https://xcelab.net/rm/statistical-rethinking/</a></p>
</div>
<div id="ref-mcelreathStatisticalRethinkingBayesian2015">
<p>McElreath, R. (2015). <em>Statistical rethinking: A Bayesian course with examples in R and Stan</em>. CRC press. <a href="https://xcelab.net/rm/statistical-rethinking/">https://xcelab.net/rm/statistical-rethinking/</a></p>
</div>
<div id="ref-moreyBayesFactorApproaches2011">
<p>Morey, R. D., &amp; Rouder, J. N. (2011). Bayes factor approaches for testing interval null hypotheses. <em>Psychological Methods</em>, <em>16</em>(4), 406–419. <a href="https://doi.org/10.1037/a0024377">https://doi.org/10.1037/a0024377</a></p>
</div>
<div id="ref-rouderBayesianTestsAccepting2009">
<p>Rouder, J. N., Speckman, P. L., Sun, D., Morey, R. D., &amp; Iverson, G. (2009). Bayesian t tests for accepting and rejecting the null hypothesis. <em>Psychonomic Bulletin &amp; Review</em>, <em>16</em>(2), 225–237. <a href="https://doi.org/10.3758/PBR.16.2.225">https://doi.org/10.3758/PBR.16.2.225</a></p>
</div>
<div id="ref-wassersteinMovingWorld052019">
<p>Wasserstein, R. L., Schirm, A. L., &amp; Lazar, N. A. (2019). Moving to a World Beyond “p <span class="math inline">\(&lt;\)</span> 0.05”. <em>The American Statistician</em>, <em>73</em>(sup1), 1–19. <a href="https://doi.org/10.1080/00031305.2019.1583913">https://doi.org/10.1080/00031305.2019.1583913</a></p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>To be clear, one can consider the null hypothesis within the Bayesian paradigm. I don’t tend to take this approach, but it’d be unfair not to at least mention some resources. Kurschke covered the topic in chapters 11 and 12 in his <span class="citation">(<a href="#ref-kruschkeDoingBayesianData2015">2015</a>)</span> text, <a href="http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/"><em>Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan</em></a>. You might also check out <span class="citation">Rouder et al. (<a href="#ref-rouderBayesianTestsAccepting2009">2009</a>)</span>, <a href="https://link.springer.com/content/pdf/10.3758/PBR.16.2.225.pdf"><em>Bayesian t tests for accepting and rejecting the null hypothesis</em></a>, or <span class="citation">Morey &amp; Rouder (<a href="#ref-moreyBayesFactorApproaches2011">2011</a>)</span>, <a href="https://d1wqtxts1xzle7.cloudfront.net/45416179/Bayes_Factor_Approaches_for_Testing_Inte20160506-23207-1t89l96.pdf?1462571611=&amp;response-content-disposition=inline%3B+filename%3DBayes_factor_approaches_for_testing_inte.pdf&amp;Expires=1597530412&amp;Signature=QAJQOISIvwxUlHd2uTfzgOMzf2TRcuWTcfwgki7JL4AIoYDziVCAfmDFOgUDi-h1mMEViTKFhOLTJF0-9u2IEyF2lR7-yhM67CYdKhqs8EEJOnhT9iK9MaaM2FBwZM8QoVtOXkOUaOXRHIt7C76UV5dbErTUx0r5Y1yym4a~-hDClb0696a6EB~dj0arYeDdylP7a3tfczmSxbIvrH8pOE4kQeHwsZXoANSh-eKXKYIYf6VD1yed~CSVPRkqlhMq6udOjg4INPZ33QBv3QQqYCk2esRC2DxxNmDF~rRVrIp0ebr6VMZkuMflVaj2~I2BFz7WS32Lb2hGFHT3jHskDA__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"><em>Bayes factor approaches for testing interval null hypotheses</em></a>.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>For a contemporary discussion of the uses and misuses of <span class="math inline">\(p\)</span>-values, see <span class="citation">Wasserstein et al. (<a href="#ref-wassersteinMovingWorld052019">2019</a>)</span> and the other articles contained in that <a href="https://www.tandfonline.com/toc/utas20/73/sup1?nav=tocList">special issue of <em>The American Statistician</em></a>.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
