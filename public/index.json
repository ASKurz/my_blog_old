[{"authors":null,"categories":[],"content":"tl;dr You too can make sideways Gaussian density curves within the tidyverse. Here’s how.\n Here’s the deal: I like making pictures. Over the past several months, I’ve been slowly chipping away at John Kruschke’s Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Kruschke has a unique plotting style. One of the quirks is once in a while he likes to express the results of his analyses in plots where he shows the data alongside density curves of the model-implied data-generating distributions. Here’s an example from chapter 19 (p. 563).\n In this example, he has lifespan data (i.e., Longevity) for fruit flies from five experimental conditions (i.e., CompanionNumber). Those are the black circles. In this section of the chapter, he used a Gaussian multilevel model in which the mean value for Longevity had a grand mean in addition to random effects for the five experimental conditions. Those sideways-turned blue Gaussians are his attempt to express the model-implied data generating distributions for each group.\nIf you haven’t gone through Kruschke’s text, you should know he relies on base R and all its loopy glory. If you carefully go through his code, you can reproduce his plots in that fashion. I’m a tidyverse man and prefer to avoid writing a for() loop at all costs. At first, I tried to work with convenience functions within ggplot2 and friends, but only had limited success. After staring long and hard at Kruschke’s base code, I came up with a robust solution, which I’d like to share here.\nIn this post, we’ll practice making sideways Gaussians in the Kruschke style. We’ll do so with a simple intercept-only single-level model and then expand our approach to an intercept-only multilevel model like the one in the picture, above.\n My assumptions For the sake of this post, I’m presuming you’re familiar with R, aware of the tidyverse, and have fit a Bayesian model or two. Yes. I admit that’s a narrow crowd. Sometimes the target’s a small one.\n We need data. First, we need data. Here we’ll borrow code from Matthew Kay’s nice tutorial on how to use his great tidybayes package.\nlibrary(tidyverse) set.seed(5) n \u0026lt;- 10 n_condition \u0026lt;- 5 abc \u0026lt;- tibble(condition = rep(letters[1:5], times = n), response = rnorm(n * 5, mean = c(0, 1, 2, 1, -1), sd = 0.5)) The data structure looks like so.\nstr(abc) ## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 50 obs. of 2 variables: ## $ condition: chr \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;d\u0026quot; ... ## $ response : num -0.42 1.692 1.372 1.035 -0.144 ... With Kay’s code, we have response values for five conditions. All follow the normal distribution and share a common standard deviation. However, they differ in their group means.\nabc %\u0026gt;% group_by(condition) %\u0026gt;% summarise(mean = mean(response) %\u0026gt;% round(digits = 2)) ## # A tibble: 5 x 2 ## condition mean ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 a 0.18 ## 2 b 1.01 ## 3 c 1.87 ## 4 d 1.03 ## 5 e -0.94 Altogether, the data look like this.\ntheme_set(theme_grey() + theme(panel.grid = element_blank())) abc %\u0026gt;% ggplot(aes(y = condition, x = response)) + geom_point(shape = 1) Let’s get ready to model.\n Just one intercept If you’ve read this far, you know we’re going Bayesian. Let’s open up our favorite Bayesian modeling package, Bürkner’s brms.\nlibrary(brms) For our first model, we’ll ignore the groups and just estimate a grand mean and a standard deviation. Relative to the scale of the abc data, our priors are modestly regularizing.\nfit1 \u0026lt;- brm(data = abc, response ~ 1, prior = c(prior(normal(0, 1), class = Intercept), prior(student_t(3, 0, 1), class = sigma))) Extract the posterior draws and save them as a data frame we’ll call post.\npost \u0026lt;- posterior_samples(fit1) glimpse(post) ## Observations: 4,000 ## Variables: 3 ## $ b_Intercept \u0026lt;dbl\u0026gt; 0.5987480, 0.5987480, 0.7314609, 0.6445826, 0.5576... ## $ sigma \u0026lt;dbl\u0026gt; 1.1177155, 1.1177155, 1.0092032, 1.0192299, 1.1700... ## $ lp__ \u0026lt;dbl\u0026gt; -76.96158, -76.96158, -77.46269, -77.09759, -77.26... If all you want is a quick and dirty way to plot a few of the model-implied Gaussians from the simple model, you can just nest stat_function() within mapply() and tack on the original data in a geom_jitter().\n# How many Gaussians would you like? n_iter \u0026lt;- 20 tibble(response = c(-4, 4)) %\u0026gt;% ggplot(aes(x = response)) + mapply(function(mean, sd) { stat_function(fun = dnorm, args = list(mean = mean, sd = sd), alpha = 1/2, color = \u0026quot;steelblue\u0026quot;) }, # Enter means and standard deviations here mean = post[1:n_iter, \u0026quot;b_Intercept\u0026quot;], sd = post[1:n_iter, \u0026quot;sigma\u0026quot;] ) + geom_jitter(data = abc, aes(y = -0.02), height = .025, shape = 1, alpha = 2/3) + scale_y_continuous(NULL, breaks = NULL) This works pretty okay. But notice the orientation is the usual horizontal. Kruschke’s Gaussians were on their sides. If we switch out our scale_y_continuous() line with scale_y_reverse() and add in coord_flip(), we’ll have it.\ntibble(response = c(-4, 4)) %\u0026gt;% ggplot(aes(x = response)) + mapply(function(mean, sd) { stat_function(fun = dnorm, args = list(mean = mean, sd = sd), alpha = 1/2, color = \u0026quot;steelblue\u0026quot;) }, mean = post[1:n_iter, \u0026quot;b_Intercept\u0026quot;], sd = post[1:n_iter, \u0026quot;sigma\u0026quot;] ) + geom_jitter(data = abc, aes(y = -0.02), height = .025, shape = 1, alpha = 2/3) + scale_y_reverse(NULL, breaks = NULL) + coord_flip()  Boom. It won’t always be this easy, though.\n Multiple intercepts Since the response values are from a combination of five condition groups, we can fit a multilevel model to compute both the grand mean and the group-level deviations from the grand mean.\nfit2 \u0026lt;- brm(data = abc, response ~ 1 + (1 | condition), prior = c(prior(normal(0, 1), class = Intercept), prior(student_t(3, 0, 1), class = sigma), prior(student_t(3, 0, 1), class = sd)), cores = 4) “Wait. Whoa. I’m so confused”—you say. “What’s a multilevel model, again?” Read this book, or this book; start here on this lecture series; or even check out my project, starting with chapter 12.\nOnce again, extract the posterior draws and save them as a data frame, post.\npost \u0026lt;- posterior_samples(fit2) str(post) ## \u0026#39;data.frame\u0026#39;: 4000 obs. of 9 variables: ## $ b_Intercept : num 0.575 -0.127 -0.212 0.737 1.099 ... ## $ sd_condition__Intercept : num 0.996 1.181 1.27 1.108 0.797 ... ## $ sigma : num 0.746 0.462 0.47 0.662 0.494 ... ## $ r_condition[a,Intercept]: num 0.00792 0.09071 0.4438 -0.69639 -0.84514 ... ## $ r_condition[b,Intercept]: num 0.242 1.174 1.118 0.187 -0.297 ... ## $ r_condition[c,Intercept]: num 0.889 1.951 1.983 1.169 0.518 ... ## $ r_condition[d,Intercept]: num 0.3866 1.1388 1.2904 0.0681 0.28 ... ## $ r_condition[e,Intercept]: num -0.979 -0.513 -1.037 -1.284 -2.105 ... ## $ lp__ : num -57.8 -52.3 -51.8 -52.5 -56 ... This is where our task becomes difficult. Now each level of condition has its own mean estimate, which is a combination of the grand mean b_Intercept and the group-specific deviation, r_condition[a,Intercept] through r_condition[e,Intercept]. If all we wanted to do was show the model-implied Gaussians for, say, condition == a, that’d be a small extension of our last approach.\ntibble(response = c(-4, 4)) %\u0026gt;% ggplot(aes(x = response)) + mapply(function(mean, sd) { stat_function(fun = dnorm, args = list(mean = mean, sd = sd), alpha = 1/2, color = \u0026quot;steelblue\u0026quot;) }, # Here\u0026#39;s the small extension, part a mean = post[1:n_iter, \u0026quot;b_Intercept\u0026quot;] + post[1:n_iter, \u0026quot;r_condition[a,Intercept]\u0026quot;], sd = post[1:n_iter, \u0026quot;sigma\u0026quot;] ) + # The small extension, part b: geom_jitter(data = abc %\u0026gt;% filter(condition == \u0026quot;a\u0026quot;), aes(y = 0), height = .025, shape = 1, alpha = 2/3) + scale_y_reverse(NULL, breaks = NULL) + coord_flip() + labs(subtitle = \u0026quot;This is just for condition a\u0026quot;) The main thing we did was add to the definition of the mean within mapply(): mean = post[1:n_iter, \u0026quot;b_Intercept\u0026quot;] + post[1:n_iter, \u0026quot;r_condition[a,Intercept]\u0026quot;]. Within geom_jitter(), we also isolated the condition == \u0026quot;a\u0026quot; cases with filter(). Simple. However, it’s more of a pickle if we want multiple densities stacked atop/next to one another within the same plot.\nUnfortunately, we can’t extend our mapply(stat_function()) method to the group-level estimates–at least not that I’m aware. But there are other ways. We’ll need a little help from tidybayes::spread_draws(), about which you can learn more here.\nlibrary(tidybayes) sd \u0026lt;- fit2 %\u0026gt;% spread_draws(b_Intercept, sigma, r_condition[condition,]) head(sd) ## # A tibble: 6 x 7 ## # Groups: condition [5] ## .chain .iteration .draw b_Intercept sigma condition r_condition ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 1 0.575 0.746 a 0.00792 ## 2 1 1 1 0.575 0.746 b 0.242 ## 3 1 1 1 0.575 0.746 c 0.889 ## 4 1 1 1 0.575 0.746 d 0.387 ## 5 1 1 1 0.575 0.746 e -0.979 ## 6 1 2 2 -0.127 0.462 a 0.0907 In our sp tibble, we have much of the same information we’d get from brms::posterior_samples(), but in the long format with respect to the random effects for condition. Also notice that each row is indexed by the chain, iteration, and draw number. Among those, .draw is the column that corresponds to a unique row like what we’d get from brms::posterior_samples(). This is the index that ranges from 1 to the number of chains multiplied by the number of post-warmup iterations (i.e., default 4000 in our case).\nBut we need to wrangle a bit. Within the expand() function, we’ll select the columns we’d like to keep within the nesting() function and then expand the tibble by adding a sequence of response values ranging from -4 to 4, for each. This sets us up to use the dnorm() function in the next line to compute the density for each of those response values based on 20 unique normal distributions for each of the five condition groups. “Why 20?” Because we need some reasonably small number and 20’s the one Kruschke tended to use in his text and because, well, we set filter(.draw \u0026lt; 21). But choose whatever number you like.\nThe difficulty, however, is that all of these densities will have a minimum value of around 0 and all will be on the same basic scale. So we need a way to serially shift the density values up the y-axis in such a way that they’ll be sensibly separated by group. As far as I can figure, this’ll take us a couple steps. For the first step, we’ll create an intermediary variable, g, with which we’ll arbitrarily assign each of our five groups an integer index ranging from 0 to 4.\nThe second step is tricky. There we use our g integers to sequentially shift the density values up. Since our g value for a == 0, those we’ll keep 0 as their baseline. As our g value for b == 1, the baseline for those will now increase by 1. And so on for the other groups. But we still need to do a little more fiddling. What we want is for the maximum values of the density estimates to be a little lower than the baselines of the ones one grouping variable up. That is, we want the maximum values for the a densities to fall a little bit below 1 on the y-axis. It’s with the * .75 / max(density) part of the code that we accomplish that task. If you want to experiment with more or less room between the top and bottom of each density, play around with increasing/decreasing that .75 value.\nsd \u0026lt;- sd %\u0026gt;% filter(.draw \u0026lt; 21) %\u0026gt;% expand(nesting(.draw, b_Intercept, sigma, condition, r_condition), response = seq(from = -4, to = 4, length.out = 200)) %\u0026gt;% mutate(density = dnorm(response, mean = b_Intercept + r_condition, sd = sigma), g = recode(condition, a = 0, b = 1, c = 2, d = 3, e = 4)) %\u0026gt;% mutate(density = g + density * .75 / max(density)) glimpse(sd) ## Observations: 20,000 ## Variables: 8 ## $ .draw \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ b_Intercept \u0026lt;dbl\u0026gt; 0.5752072, 0.5752072, 0.5752072, 0.5752072, 0.5752... ## $ sigma \u0026lt;dbl\u0026gt; 0.7463794, 0.7463794, 0.7463794, 0.7463794, 0.7463... ## $ condition \u0026lt;chr\u0026gt; \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, ... ## $ r_condition \u0026lt;dbl\u0026gt; 0.007922818, 0.007922818, 0.007922818, 0.007922818... ## $ response \u0026lt;dbl\u0026gt; -4.000000, -3.959799, -3.919598, -3.879397, -3.839... ## $ density \u0026lt;dbl\u0026gt; 3.018427e-09, 4.195532e-09, 5.814782e-09, 8.035630... ## $ g \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... Since we’ll now be using the same axis for both the densities and the five condition groups, we’ll need to add a density column to our abc data.\nabc \u0026lt;- abc %\u0026gt;% mutate(density = recode(condition, a = 0, b = 1, c = 2, d = 3, e = 4)) Time to plot.\nsd %\u0026gt;% ggplot(aes(x = response, y = density)) + # here we make our density lines geom_line(aes(group = interaction(.draw, g)), alpha = 1/2, size = 1/3, color = \u0026quot;steelblue\u0026quot;) + # use the original data for the jittered points geom_jitter(data = abc, height = .05, shape = 1, alpha = 2/3) + scale_y_continuous(\u0026quot;condition\u0026quot;, breaks = 0:4, labels = letters[1:5]) Now we’re rolling. Let’s make a cosmetic adjustment. Recall that the full range of the normal distribution spans from \\(-\\infty\\) to \\(\\infty\\). At a certain point, it’s just not informative to show the left and right tails. If you look back up at our motivating example, you’ll note Kruschke’s densities stopped well before trailing off into the tails. If you look closely to the code from his text, you’ll see he’s just showing the inner 95-percentile range for each. To follow suit, we can compute those ranges with qnorm().\nsd \u0026lt;- sd %\u0026gt;% mutate(ll = qnorm(.025, mean = b_Intercept + r_condition, sd = sigma), ul = qnorm(.975, mean = b_Intercept + r_condition, sd = sigma)) Now we have our lower- and upper-level points for each iteration, we can limit the ranges of our Gaussians with filter().\nsd %\u0026gt;% filter(response \u0026gt; ll, response \u0026lt; ul) %\u0026gt;% ggplot(aes(x = response, y = density)) + geom_line(aes(group = interaction(.draw, g)), alpha = 1/2, size = 1/3, color = \u0026quot;steelblue\u0026quot;) + geom_jitter(data = abc, height = .05, shape = 1, alpha = 2/3) + scale_y_continuous(\u0026quot;condition\u0026quot;, breaks = 0:4, labels = letters[1:5]) Oh man, just look how sweet that is. Although I prefer our current method, another difference between it and Kruschke’s example is all of his densities are the same relative height. In all our plots so far, though, the densities differ by their heights. We’ll need a slight adjustment in our sd workflow for that. All we need to do is insert a group_by() statement between the two mutate() lines.\nsd \u0026lt;- sd %\u0026gt;% mutate(density = dnorm(response, mean = b_Intercept + r_condition, sd = sigma), g = recode(condition, a = 0, b = 1, c = 2, d = 3, e = 4)) %\u0026gt;% # here\u0026#39;s the new line group_by(.draw) %\u0026gt;% mutate(density = g + density * .75 / max(density)) # now plot sd %\u0026gt;% filter(response \u0026gt; ll, response \u0026lt; ul) %\u0026gt;% ggplot(aes(x = response, y = density)) + geom_line(aes(group = interaction(.draw, g)), alpha = 1/2, size = 1/3, color = \u0026quot;steelblue\u0026quot;) + geom_jitter(data = abc, height = .05, shape = 1, alpha = 2/3) + scale_y_continuous(\u0026quot;condition\u0026quot;, breaks = 0:4, labels = letters[1:5]) Nice. “But wait!”, you say. “We wanted our Gaussians to be on their sides.” We can do that in at least two ways. At this point, the quickest way is to use our scale_y_reverse() + coord_flip() combo from before.\nsd %\u0026gt;% filter(response \u0026gt; ll, response \u0026lt; ul) %\u0026gt;% ggplot(aes(x = response, y = density)) + geom_line(aes(group = interaction(.draw, g)), alpha = 1/2, size = 1/3, color = \u0026quot;steelblue\u0026quot;) + geom_jitter(data = abc, height = .05, shape = 1, alpha = 2/3) + scale_y_reverse(\u0026quot;condition\u0026quot;, breaks = 0:4, labels = letters[1:5]) + coord_flip() Another way to get those sideways Gaussians is to alter our sd data workflow. The main differene is this time we change the original mutate(density = g + density * .75 / max(density)) line to mutate(density = g - density * .75 / max(density)). In case you missed it, the only difference is we changed the + to a -.\nsd \u0026lt;- sd %\u0026gt;% # step one: starting fresh mutate(density = dnorm(response, mean = b_Intercept + r_condition, sd = sigma)) %\u0026gt;% group_by(.draw) %\u0026gt;% # step two: now SUBTRACTING density from g within the equation mutate(density = g - density * .75 / max(density)) Now in our global aes() statement in the plot, we put density on the x and response on the y. We need to take a few other subtle steps:\n Switch out geom_line() for geom_path() (see here). Drop the height argument within geom_jitter() for width. Switch out scale_y_continuous() for scale_x_continuous().  Though totally not necessary, we’ll add a little something extra by coloring the Gaussians by their means.\nsd %\u0026gt;% filter(response \u0026gt; ll, response \u0026lt; ul) %\u0026gt;% ggplot(aes(x = density, y = response)) + geom_path(aes(group = interaction(.draw, g), color = b_Intercept + r_condition), alpha = 1/2, size = 1/3, show.legend = F) + geom_jitter(data = abc, width = .05, shape = 1, alpha = 2/3) + scale_x_continuous(\u0026quot;condition\u0026quot;, breaks = 0:4, labels = letters[1:5]) + scale_color_viridis_c(option = \u0026quot;A\u0026quot;, end = .92) There you have it–Kruschke-style sideways Gaussians for your model plots.\n Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_1.0.3 brms_2.7.0 Rcpp_1.0.0 bindrcpp_0.2.2 ## [5] forcats_0.3.0 stringr_1.3.1 dplyr_0.7.6 purrr_0.2.5 ## [9] readr_1.1.1 tidyr_0.8.1 tibble_1.4.2 ggplot2_3.1.0 ## [13] tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.3-2 ggridges_0.5.0 ## [3] rsconnect_0.8.8 rprojroot_1.3-2 ## [5] ggstance_0.3 markdown_0.8 ## [7] base64enc_0.1-3 rstudioapi_0.7 ## [9] rstan_2.18.2 svUnit_0.7-12 ## [11] DT_0.4 mvtnorm_1.0-8 ## [13] lubridate_1.7.4 xml2_1.2.0 ## [15] bridgesampling_0.4-0 mnormt_1.5-5 ## [17] knitr_1.20 shinythemes_1.1.1 ## [19] bayesplot_1.6.0 jsonlite_1.5 ## [21] broom_0.4.5 shiny_1.1.0 ## [23] compiler_3.5.1 httr_1.3.1 ## [25] backports_1.1.2 assertthat_0.2.0 ## [27] Matrix_1.2-14 lazyeval_0.2.1 ## [29] cli_1.0.1 later_0.7.3 ## [31] htmltools_0.3.6 prettyunits_1.0.2 ## [33] tools_3.5.1 igraph_1.2.1 ## [35] coda_0.19-2 gtable_0.2.0 ## [37] glue_1.3.0 reshape2_1.4.3 ## [39] cellranger_1.1.0 nlme_3.1-137 ## [41] blogdown_0.8 crosstalk_1.0.0 ## [43] psych_1.8.4 xfun_0.3 ## [45] ps_1.2.1 rvest_0.3.2 ## [47] mime_0.5 miniUI_0.1.1.1 ## [49] gtools_3.8.1 MASS_7.3-50 ## [51] zoo_1.8-2 scales_1.0.0 ## [53] colourpicker_1.0 hms_0.4.2 ## [55] promises_1.0.1 Brobdingnag_1.2-5 ## [57] parallel_3.5.1 inline_0.3.15 ## [59] shinystan_2.5.0 yaml_2.1.19 ## [61] gridExtra_2.3 loo_2.0.0 ## [63] StanHeaders_2.18.0-1 stringi_1.2.3 ## [65] dygraphs_1.1.1.5 pkgbuild_1.0.2 ## [67] rlang_0.3.0.1 pkgconfig_2.0.1 ## [69] matrixStats_0.54.0 evaluate_0.10.1 ## [71] lattice_0.20-35 bindr_0.1.1 ## [73] rstantools_1.5.0 htmlwidgets_1.2 ## [75] labeling_0.3 tidyselect_0.2.4 ## [77] processx_3.2.1 plyr_1.8.4 ## [79] magrittr_1.5 bookdown_0.7 ## [81] R6_2.3.0 pillar_1.2.3 ## [83] haven_1.1.2 foreign_0.8-70 ## [85] withr_2.1.2 xts_0.10-2 ## [87] abind_1.4-5 modelr_0.1.2 ## [89] crayon_1.3.4 arrayhelpers_1.0-20160527 ## [91] utf8_1.1.4 rmarkdown_1.10 ## [93] grid_3.5.1 readxl_1.1.0 ## [95] callr_3.1.0 threejs_0.3.1 ## [97] digest_0.6.18 xtable_1.8-2 ## [99] httpuv_1.4.4.2 stats4_3.5.1 ## [101] munsell_0.5.0 viridisLite_0.3.0 ## [103] shinyjs_1.0  ","date":1545264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545264000,"objectID":"42ba31c0d9f0e8429d86371da5384421","permalink":"/post/make-rotated-gaussians-kruschke-style/","publishdate":"2018-12-20T00:00:00Z","relpermalink":"/post/make-rotated-gaussians-kruschke-style/","section":"post","summary":"tl;dr You too can make sideways Gaussian density curves within the tidyverse. Here’s how.\n Here’s the deal: I like making pictures. Over the past several months, I’ve been slowly chipping away at John Kruschke’s Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Kruschke has a unique plotting style. One of the quirks is once in a while he likes to express the results of his analyses in plots where he shows the data alongside density curves of the model-implied data-generating distributions.","tags":["Bayesian","brms","Kruschke","plot","R","tutorial","tydiverse"],"title":"Make rotated Gaussians, Kruschke style","type":"post"},{"authors":null,"categories":[],"content":"Preamble I released the first bookdown version of my Statistical Rethinking with brms, ggplot2, and the tidyverse project a couple weeks ago. I consider it the 0.9.0 version. I wanted a little time to step back from the project before giving it a final edit for the first major edition. I also wanted to give others a little time to take a look and suggest edits, which some thankfully have.\nNow some time has passed, it’s become clear I’d like to add a bonus section on Bayesian meta-analysis. IMO, this is a natural extension of the hierarchical models McElreath introduced in chapter’s 12 and 13 of his text and of the measurement-error models he introduced in chapter 14. So the purpose of this post is to present a rough draft of how I’d like to introduce fitting meta-analyses with Bürkner’s great brms package.\nI intend to tack this section onto the end of chapter 14. If you have any constrictive criticisms, please pass them along.\nHere’s the rough draft (which I updated on 2018-11-12):\n Rough draft: Meta-analysis If your mind isn’t fully blown by those measurement-error and missing-data models, let’s keep building. As it turns out, meta-analyses are often just special kinds of multilevel measurement-error models. Thus, you can use brms::brm() to fit Bayesian meta-analyses, too.\nBefore we proceed, I should acknowledge that this section is heavily influenced by Matti Vourre’s great blog post, Meta-analysis is a special case of Bayesian multilevel modeling. And since McElreath’s text doesn’t directly address meta-analyses, we’ll take further inspiration from Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin’s Bayesian data analysis, Third edition. We’ll let Gelman and colleagues introduce the topic:\n Discussions of meta-analysis are sometimes imprecise about the estimands of interest in the analysis, especially when the primary focus is on testing the null hypothesis of no effect in any of the studies to be combined. Our focus is on estimating meaningful parameters, and for this objective there appear to be three possibilities, accepting the overarching assumption that the studies are comparable in some broad sense. The first possibility is that we view the studies as identical replications of each other, in the sense we regard the individuals in all the studies as independent samples from a common population, with the same outcome measures and so on. A second possibility is that the studies are so different that the results of any one study provide no information about the results of any of the others. A third, more general, possibility is that we regard the studies as exchangeable but not necessarily either identical or completely unrelated; in other words we allow differences from study to study, but such that the differences are not expected a priori to have predictable effects favoring one study over another.… This third possibility represents a continuum between the two extremes, and it is this exchangeable model (with unknown hyperparameters characterizing the population distribution) that forms the basis of our Bayesian analysis…\nThe first potential estimand of a meta-analysis, or a hierarchically structured problem in general, is the mean of the distribution of effect sizes, since this represents the overall ‘average’ effect across all studies that could be regarded as exchangeable with the observed studies. Other possible estimands are the effect size in any of the observed studies and the effect size in another, comparable (exchangeable) unobserved study. (pp. 125—126, emphasis in the original)\n The basic version of a Bayesian meta-analysis follows the form\n\\[y_i \\sim \\text{Normal}(\\theta_i, \\sigma_i)\\]\nwhere \\(y_i\\) = the point estimate for the effect size of a single study, \\(i\\), which is presumed to have been a draw from a Normal distribution centered on \\(\\theta_i\\). The data in meta-analyses are typically statistical summaries from individual studies. The one clear lesson from this chapter is that those estimates themselves come with error and those errors should be fully expressed in the meta-analytic model. Which we do. The standard error from study \\(i\\) is specified \\(\\sigma_i\\), which is also a stand-in for the standard deviation of the Normal distribution from which the point estimate was drawn. Do note, we’re not estimating \\(\\sigma_i\\), here. Those values we take directly from the original studies.\nBuilding on the model, we further presume that study \\(i\\) is itself just one draw from a population of related studies, each of which have their own effect sizes. As such. we presume \\(\\theta_i\\) itself has a distribution following the form\n\\[\\theta_i \\sim \\text{Normal} (\\mu, \\tau)\\]\nwhere \\(\\mu\\) is the meta-analytic effect (i.e., the population mean) and \\(\\tau\\) is the variation around that mean, what you might also think of as \\(\\sigma_\\tau\\).\nSince there’s no example of a meta-analysis in the text, we’ll have to look elsewhere. We’ll focus on Gershoff and Grogan-Kaylor’s (2016) paper, Spanking and Child Outcomes: Old Controversies and New Meta-Analyses. From their introduction, we read:\n Around the world, most children (80%) are spanked or otherwise physically punished by their parents (UNICEF, 2014). The question of whether parents should spank their children to correct misbehaviors sits at a nexus of arguments from ethical, religious, and human rights perspectives both in the U.S. and around the world (Gershoff, 2013). Several hundred studies have been conducted on the associations between parents’ use of spanking or physical punishment and children’s behavioral, emotional, cognitive, and physical outcomes, making spanking one of the most studied aspects of parenting. What has been learned from these hundreds of studies? (p. 453)\n Our goal will be to learn Bayesian meta-analysis by answering part of that question. I’ve transcribed the values directly from Gershoff and Grogan-Kaylor’s paper and saved them as a file called spank. You can find the data in this project’s GitHub repository. Let’s load them and glimpse().\nspank \u0026lt;- readxl::read_excel(\u0026quot;spank.xlsx\u0026quot;) library(tidyverse) glimpse(spank) ## Observations: 111 ## Variables: 8 ## $ study \u0026lt;chr\u0026gt; \u0026quot;Bean and Roberts (1981)\u0026quot;, \u0026quot;Day and Roberts (1983)\u0026quot;, \u0026quot;... ## $ year \u0026lt;dbl\u0026gt; 1981, 1983, 1971, 1988, 1990, 1961, 1962, 1990, 2002, ... ## $ outcome \u0026lt;chr\u0026gt; \u0026quot;Immediate defiance\u0026quot;, \u0026quot;Immediate defiance\u0026quot;, \u0026quot;Immediate... ## $ between \u0026lt;dbl\u0026gt; 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, ... ## $ within \u0026lt;dbl\u0026gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, ... ## $ d \u0026lt;dbl\u0026gt; -0.74, 0.36, 0.34, -0.08, 0.10, 0.63, 0.19, 0.47, 0.14... ## $ ll \u0026lt;dbl\u0026gt; -1.76, -1.04, -0.09, -1.01, -0.82, 0.16, -0.14, 0.20, ... ## $ ul \u0026lt;dbl\u0026gt; 0.28, 1.77, 0.76, 0.84, 1.03, 1.10, 0.53, 0.74, 0.70, ... In this paper, the effect size of interest is a Cohen’s d, derived from the formula\n\\[d = \\frac{\\mu_\\text{treatment} - \\mu_\\text{comparison}}{\\sigma_{pooled}}\\]\nwhere\n\\[\\sigma_{pooled} = \\sqrt{\\frac{((n_1 - 1) \\sigma_1^2) + ((n_2 - 1) \\sigma_2^2)}{n_1 + n_2 -2}}\\]\nTo help make the equation for \\(d\\) clearer for our example, we might re-express it as\n\\[d = \\frac{\\mu_\\text{spanked} - \\mu_\\text{not spanked}}{\\sigma_{pooled}}\\]\nMcElreath didn’t really focus on effect sizes in his text. If you need a refresher, you might check out Kelley and Preacher’s On effect size. But in words, Cohen’s d is a standardized mean difference between two groups.\nSo if you look back up at the results of glimpse(spank), you’ll notice the column d, which is indeed a vector of Cohen’s d effect sizes. The last two columns, ll and ul are the lower and upper limits of the associated 95% frequentist confidence intervals. But we don’t want confidence intervals for our d-values; we want their standard errors. Fortunately, we can compute those with the following formula\n\\[SE = \\frac{\\text{upper limit } – \\text{lower limit}}{3.92}\\]\nHere it is in code.\nspank \u0026lt;- spank %\u0026gt;% mutate(se = (ul - ll) / 3.92) glimpse(spank) ## Observations: 111 ## Variables: 9 ## $ study \u0026lt;chr\u0026gt; \u0026quot;Bean and Roberts (1981)\u0026quot;, \u0026quot;Day and Roberts (1983)\u0026quot;, \u0026quot;... ## $ year \u0026lt;dbl\u0026gt; 1981, 1983, 1971, 1988, 1990, 1961, 1962, 1990, 2002, ... ## $ outcome \u0026lt;chr\u0026gt; \u0026quot;Immediate defiance\u0026quot;, \u0026quot;Immediate defiance\u0026quot;, \u0026quot;Immediate... ## $ between \u0026lt;dbl\u0026gt; 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, ... ## $ within \u0026lt;dbl\u0026gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, ... ## $ d \u0026lt;dbl\u0026gt; -0.74, 0.36, 0.34, -0.08, 0.10, 0.63, 0.19, 0.47, 0.14... ## $ ll \u0026lt;dbl\u0026gt; -1.76, -1.04, -0.09, -1.01, -0.82, 0.16, -0.14, 0.20, ... ## $ ul \u0026lt;dbl\u0026gt; 0.28, 1.77, 0.76, 0.84, 1.03, 1.10, 0.53, 0.74, 0.70, ... ## $ se \u0026lt;dbl\u0026gt; 0.52040816, 0.71683673, 0.21683673, 0.47193878, 0.4719... Now our data are ready, we can express our first Bayesian meta-analysis with the formula\n\\[ \\begin{eqnarray} \\text{d}_i \u0026amp; \\sim \u0026amp; \\text{Normal}(\\theta_i, \\sigma_i = \\text{se}_i) \\\\ \\theta_i \u0026amp; \\sim \u0026amp; \\text{Normal} (\\mu, \\tau) \\\\ \\mu \u0026amp; \\sim \u0026amp; \\text{Normal} (0, 1) \\\\ \\tau \u0026amp; \\sim \u0026amp; \\text{HalfCauchy} (0, 1) \\end{eqnarray} \\]\nThe last two lines, of course, spell out our priors. In psychology, it’s pretty rare to see Cohen’s d-values greater than the absolute value of \\(\\pm 1\\). So in the absence of more specific domain knowledge–which I don’t have–, it seems like \\(\\text{Normal} (0, 1)\\) is a reasonable place to start. And just like McElreath used \\(\\text{HalfCauchy} (0, 1)\\) as the default prior for the group-level standard deviations, it makes sense to use it here for our meta-analytic \\(\\tau\\) parameter.\nLet’s load brms.\nlibrary(brms) Here’s the code for the first model.\nb14.5 \u0026lt;- brm(data = spank, family = gaussian, d | se(se) ~ 1 + (1 | study), prior = c(prior(normal(0, 1), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 2000, warmup = 1000, cores = 4, chains = 4) One thing you might notice is our se(se) function excluded the sigma argument. If you recall from section 14.1, we specified sigma = T in our measurement-error models. The brms default is that within se(), sigma = FALSE. As such, we have no estimate for sigma the way we would if we were doing this analysis with the raw data from the studies. Hopefully this makes sense. The uncertainty around the d-value for each study \\(i\\) has already been encoded in the data as se.\nThis brings us to another point. We typically perform meta-analyses on data summaries. In my field and perhaps in yours, this is due to the historical accident that it has not been the norm among researchers to make their data publically available. So effect size summaries were the best we typically had. However, times are changing (e.g., here, here). If the raw data from all the studies for your meta-analysis are available, you can just fit a multilevel model in which the data are nested in the studies. Heck, you could even allow the studies to vary by \\(\\sigma\\) by taking the distributional modeling approach and specify something like sigma ~ 0 + study or even sigma ~ 1 + (1 | study).\nBut enough technical talk. Let’s look at the model results.\nprint(b14.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: d | se(se) ~ 1 + (1 | study) ## Data: spank (Number of observations: 111) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~study (Number of levels: 76) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 0.27 0.03 0.21 0.33 773 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.38 0.03 0.30 0.44 482 1.01 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Thus, in our simple Bayesian meta-analysis, we have a population Cohen’s d of about 0.38. Our estimate for \\(\\tau\\), 0.27, suggests we have quite a bit of between-study variability. One question you might ask is: What exactly are these Cohen’s ds measuring, anyways? We’ve encoded that in the outcome vector of the spank data.\nspank %\u0026gt;% distinct(outcome) %\u0026gt;% knitr::kable()   outcome    Immediate defiance  Low moral internalization  Child aggression  Child antisocial behavior  Child externalizing behavior problems  Child internalizing behavior problems  Child mental health problems  Child alcohol or substance abuse  Negative parent–child relationship  Impaired cognitive ability  Low self-esteem  Low self-regulation  Victim of physical abuse  Adult antisocial behavior  Adult mental health problems  Adult alcohol or substance abuse  Adult support for physical punishment    There are a few things to note. First, with the possible exception of Adult support for physical punishment, all of the outcomes are negative. We prefer conditions associated with lower values for things like Child aggression and Adult mental health problems. Second, the way the data are coded, larger effect sizes are interpreted as more negative outcomes associated with children having been spanked. That is, our analysis suggests spanking children is associated with worse outcomes. What might not be immediately apparent is that even though there are 111 cases in the data, there are only 76 distinct studies.\nspank %\u0026gt;% distinct(study) %\u0026gt;% count() ## # A tibble: 1 x 1 ## n ## \u0026lt;int\u0026gt; ## 1 76 In other words, some studies have multiple outcomes. In order to better accommodate the study- and outcome-level variances, let’s fit a cross-classified Bayesian meta-analysis reminiscent of the cross-classified chimp model from Chapter 13.\nb14.6 \u0026lt;- brm(data = spank, family = gaussian, d | se(se) ~ 1 + (1 | study) + (1 | outcome), prior = c(prior(normal(0, 1), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 2000, warmup = 1000, cores = 4, chains = 4) print(b14.6) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: d | se(se) ~ 1 + (1 | study) + (1 | outcome) ## Data: spank (Number of observations: 111) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~outcome (Number of levels: 17) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 0.08 0.02 0.04 0.14 876 1.00 ## ## ~study (Number of levels: 76) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 0.25 0.03 0.20 0.32 828 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.35 0.04 0.28 0.44 550 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we have two \\(\\tau\\) parameters. We might plot them to get a sense of where the variance is at.\nposterior_samples(b14.6) %\u0026gt;% select(starts_with(\u0026quot;sd\u0026quot;)) %\u0026gt;% gather(key, tau) %\u0026gt;% mutate(key = str_remove(key, \u0026quot;sd_\u0026quot;) %\u0026gt;% str_remove(., \u0026quot;__Intercept\u0026quot;)) %\u0026gt;% ggplot(aes(x = tau, fill = key)) + geom_density(color = \u0026quot;transparent\u0026quot;, alpha = 2/3) + scale_fill_viridis_d(NULL, end = .85) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(tau)) + theme(panel.grid = element_blank()) So at this point, the big story is there’s more variability between the studies than there is the outcomes. But I still want to get a sense of the individual outcomes. Here we’ll use tidybayes::geom_halfeyeh() to help us make our version of a forest plot.\n# load tidybayes library(tidybayes) b14.6 %\u0026gt;% spread_draws(b_Intercept, r_outcome[outcome,]) %\u0026gt;% # add the grand mean to the group-specific deviations mutate(mu = b_Intercept + r_outcome) %\u0026gt;% ungroup() %\u0026gt;% mutate(outcome = str_replace_all(outcome, \u0026quot;[.]\u0026quot;, \u0026quot; \u0026quot;)) %\u0026gt;% # plot ggplot(aes(x = mu, y = outcome, fill = mu)) + # ggplot(aes(x = mu, y = reorder(outcome, mu))) + geom_halfeyeh(.width = .95, size = 2/3) + labs(x = expression(italic(\u0026quot;Cohen\u0026#39;s d\u0026quot;)), y = NULL) + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) Nope, not a lot of variability, there. Let’s go one step further with the model. Doubling back to Gelman and colleagues, we read:\n When assuming exchangeability we assume there are no important covariates that might form the basis of a more complex model, and this assumption (perhaps misguidedly) is widely adopted in meta-analysis. What if other information (in addition to the data \\((n, y)\\)) is available to distinguish among the \\(J\\) studies in a meta-analysis, so that an exchangeable model is inappropriate? In this situation, we can expand the framework of the model to be exchangeable in the observed data and covariates, for example using a hierarchical regression model. (p. 126)\n One important covariate Gershoff and Grogan-Kaylor addressed in their meta-analysis was the type of study. The 76 papers they based their meta-analysis on contained both between- and within-participants designs. In the spank data, we’ve dummy coded that information with the between and within vectors. Both are dummy variables and within = 1 - between. Here are the counts.\nspank %\u0026gt;% count(between) ## # A tibble: 2 x 2 ## between n ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 0 71 ## 2 1 40 When I use dummies in my models, I prefer to have the majority group stand as the reference category. As such, I typically name those variables by the minority group. In this case, most occasions are based on within-participant designs. Thus, we’ll go ahead and add the between variable to the model. While we’re at it, we’ll practice using the 0 + intercept syntax.\nb14.7 \u0026lt;- brm(data = spank, family = gaussian, d | se(se) ~ 0 + intercept + between + (1 | study) + (1 | outcome), prior = c(prior(normal(0, 1), class = b), prior(cauchy(0, 1), class = sd)), iter = 2000, warmup = 1000, cores = 4, chains = 4) print(b14.7) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: d | se(se) ~ 0 + intercept + between + (1 | study) + (1 | outcome) ## Data: spank (Number of observations: 111) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~outcome (Number of levels: 17) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 0.08 0.02 0.04 0.14 1319 1.00 ## ## ~study (Number of levels: 76) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 0.25 0.03 0.20 0.32 1088 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept 0.38 0.05 0.29 0.48 1017 1.00 ## between -0.07 0.07 -0.21 0.07 1194 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s take a closer look at b_between.\nposterior_samples(b14.7) %\u0026gt;% ggplot(aes(x = b_between, y = 0)) + geom_halfeyeh(point_interval = median_qi, .width = c(.5, .95)) + labs(x = \u0026quot;Overall difference for between- vs within-participant designs\u0026quot;, y = NULL) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) That difference isn’t as large I’d expect it to be. But then again, I’m no spanking researcher. So what do I know?\nThere are other things you might do with these data. For example, you might check for trends by year or, as the authors did in their manuscript, distinguish among different severities of corporal punishment. But I think we’ve gone far enough to get you started.\nIf you’d like to learn more about these methods, do check out Vourre’s Meta-analysis is a special case of Bayesian multilevel modeling. From his blog, you’ll learn additional tricks, like making a more traditional-looking forest plot with the brmstools::forest() function and how our Bayesian brms method compares with Frequentist meta-analyses via the metafor package. You might also check out Williams, Rast, and Bürkner’s manuscript, Bayesian Meta-Analysis with Weakly Informative Prior Distributions to give you an empirical justification for using a half-Cauchy prior for your meta-analysis \\(\\tau\\) parameters.\n Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_1.0.3 brms_2.7.0 Rcpp_1.0.0 bindrcpp_0.2.2 ## [5] forcats_0.3.0 stringr_1.3.1 dplyr_0.7.6 purrr_0.2.5 ## [9] readr_1.1.1 tidyr_0.8.1 tibble_1.4.2 ggplot2_3.1.0 ## [13] tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.3-2 ggridges_0.5.0 ## [3] rsconnect_0.8.8 rprojroot_1.3-2 ## [5] ggstance_0.3 markdown_0.8 ## [7] base64enc_0.1-3 rstudioapi_0.7 ## [9] rstan_2.18.2 svUnit_0.7-12 ## [11] DT_0.4 mvtnorm_1.0-8 ## [13] lubridate_1.7.4 xml2_1.2.0 ## [15] bridgesampling_0.4-0 mnormt_1.5-5 ## [17] knitr_1.20 shinythemes_1.1.1 ## [19] bayesplot_1.6.0 jsonlite_1.5 ## [21] broom_0.4.5 shiny_1.1.0 ## [23] compiler_3.5.1 httr_1.3.1 ## [25] backports_1.1.2 assertthat_0.2.0 ## [27] Matrix_1.2-14 lazyeval_0.2.1 ## [29] cli_1.0.1 later_0.7.3 ## [31] htmltools_0.3.6 prettyunits_1.0.2 ## [33] tools_3.5.1 igraph_1.2.1 ## [35] coda_0.19-2 gtable_0.2.0 ## [37] glue_1.3.0 reshape2_1.4.3 ## [39] cellranger_1.1.0 nlme_3.1-137 ## [41] blogdown_0.8 crosstalk_1.0.0 ## [43] psych_1.8.4 xfun_0.3 ## [45] ps_1.2.1 rvest_0.3.2 ## [47] mime_0.5 miniUI_0.1.1.1 ## [49] gtools_3.8.1 MASS_7.3-50 ## [51] zoo_1.8-2 scales_1.0.0 ## [53] colourpicker_1.0 hms_0.4.2 ## [55] promises_1.0.1 Brobdingnag_1.2-5 ## [57] parallel_3.5.1 inline_0.3.15 ## [59] shinystan_2.5.0 yaml_2.1.19 ## [61] gridExtra_2.3 loo_2.0.0 ## [63] StanHeaders_2.18.0-1 stringi_1.2.3 ## [65] highr_0.7 dygraphs_1.1.1.5 ## [67] pkgbuild_1.0.2 rlang_0.3.0.1 ## [69] pkgconfig_2.0.1 matrixStats_0.54.0 ## [71] evaluate_0.10.1 lattice_0.20-35 ## [73] bindr_0.1.1 labeling_0.3 ## [75] rstantools_1.5.0 htmlwidgets_1.2 ## [77] tidyselect_0.2.4 processx_3.2.1 ## [79] plyr_1.8.4 magrittr_1.5 ## [81] bookdown_0.7 R6_2.3.0 ## [83] pillar_1.2.3 haven_1.1.2 ## [85] foreign_0.8-70 withr_2.1.2 ## [87] xts_0.10-2 abind_1.4-5 ## [89] modelr_0.1.2 crayon_1.3.4 ## [91] arrayhelpers_1.0-20160527 utf8_1.1.4 ## [93] rmarkdown_1.10 grid_3.5.1 ## [95] readxl_1.1.0 callr_3.1.0 ## [97] threejs_0.3.1 digest_0.6.18 ## [99] xtable_1.8-2 httpuv_1.4.4.2 ## [101] stats4_3.5.1 munsell_0.5.0 ## [103] viridisLite_0.3.0 shinyjs_1.0 set.seed(5) n = 10 n_condition = 5 ABC = data_frame( condition = rep(c(\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;,\u0026quot;C\u0026quot;,\u0026quot;D\u0026quot;,\u0026quot;E\u0026quot;), n), response = rnorm(n * 5, c(0,1,2,1,-1), 0.5) ) m \u0026lt;- brm(response ~ (1|condition), data = ABC, control = list(adapt_delta = .99), prior = c( prior(normal(0, 1), class = Intercept), prior(student_t(3, 0, 1), class = sd), prior(student_t(3, 0, 1), class = sigma) )) ## Compiling the C++ model ## Start sampling ## ## SAMPLING FOR MODEL \u0026#39;aed3c976cefe3e319d41eeba97e15379\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 3.5e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.35 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.698616 seconds (Warm-up) ## Chain 1: 0.594893 seconds (Sampling) ## Chain 1: 1.29351 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;aed3c976cefe3e319d41eeba97e15379\u0026#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 1.5e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.713423 seconds (Warm-up) ## Chain 2: 0.803974 seconds (Sampling) ## Chain 2: 1.5174 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;aed3c976cefe3e319d41eeba97e15379\u0026#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 1.2e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.735096 seconds (Warm-up) ## Chain 3: 0.674966 seconds (Sampling) ## Chain 3: 1.41006 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL \u0026#39;aed3c976cefe3e319d41eeba97e15379\u0026#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 1.4e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.680389 seconds (Warm-up) ## Chain 4: 0.778703 seconds (Sampling) ## Chain 4: 1.45909 seconds (Total) ## Chain 4: m %\u0026gt;% spread_draws(b_Intercept, r_condition[condition,]) %\u0026gt;% mutate(condition_mean = b_Intercept + r_condition) %\u0026gt;% ggplot(aes(y = condition, x = condition_mean, fill = condition_mean)) + geom_halfeyeh()  ","date":1539475200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539475200,"objectID":"e4e0cca48a080bc20b2083715d19b5bf","permalink":"/post/bayesian-meta-analysis/","publishdate":"2018-10-14T00:00:00Z","relpermalink":"/post/bayesian-meta-analysis/","section":"post","summary":"Preamble I released the first bookdown version of my Statistical Rethinking with brms, ggplot2, and the tidyverse project a couple weeks ago. I consider it the 0.9.0 version. I wanted a little time to step back from the project before giving it a final edit for the first major edition. I also wanted to give others a little time to take a look and suggest edits, which some thankfully have.","tags":["Bayesian","brms","R","meta-analysis","spanking","Statistical Rethinking","tutorial"],"title":"Bayesian meta-analysis in brms","type":"post"},{"authors":null,"categories":[],"content":"The other day, my Twitter feed informed me Penn Jillette just clocked in 1000 consecutive days of meditation using the Headspace app. Now he’s considering checking out Sam Harris’s new Waking Up meditation app. Sam left a congratulatory comment on Penn’s tweet.\nWell done, Penn! https://t.co/SZkKocIPEH\n\u0026mdash; Sam Harris (@SamHarrisOrg) October 7, 2018  The whole thing was rainbows and kittens. And it reminded me to pass on some advice: Stop using fee-based meditation apps!\nWhat? The Headspace app is popular and highly-rated. It’s free to download and has some nice features, like reminders to meditate. However, if you want full access to its library of guided meditation audio recordings, you’ll need to pay a fee at a monthly, yearly, or lifetime rate.\n Sam Harris’s Waking Up app app is also free to download. It’s new and, in fairness, we’ll have to wait and see how its format will unfold. But at present it has more of a course-type format. The free version gives you access to five guided meditations and three lessons. But if you want full access to the app’s content, you also have to subscribe.\n I have no problem with the Headspace and Waking Up apps. They have many fine features. And I’m even a fan of a lot Harris’s work. But we have a cheaper, high-quality option:\n Consider Insight Timer  Insight Timer comes with a free and pay versions, too. But just download the free version. It’s excellent and all you need for your meditation needs. Let me list the reasons why.\nThe free library is extensive. At the time of this writing, my free subscription to Insight Timer gives me access to some 12,000 guided meditation audios. Most of them are in English. But many are offered in other languages, such as Hebrew, Malay, and Spanish.\n Duration. Their durations vary. The bulk of the guided meditations seem to be in the 5-to-20-minute range. But some last more than an hour.\n  People and their voices. Insight Timer’s deep library boasts an impressive cast of meditation teachers. I was happy to see some familiar high-profile meditation teachers, such as Tara Brach and Joseph Goldstein. But I have also discovered new favorites, like Stephen Pende Wormland and Dawn Mauricio. To be sure, the quality of the audio recordings varies. But more importantly, they also vary in terms of vocal tone and pacing. There should be a vocal style to suit just about everyone.\n  Emphasis. I’m an academic and generally prefer meditations that are secular and connected to the clinical literature (e.g., this recent meta-analysis). Happily, the Insight Timer library contains offerings based on mindfulness-based stress reduction (MBSR) and its derivatives (e.g., mindfulness-based cognitive therapy, mindfulness-based relapse prevention).\n But you can also find meditations grounded within a number of faith traditions.\n  Music. I’ve focused mainly on vocally-driven meditations. However, Insight Timer also contains music/sound-based recordings. I’m partial to recordings featuring Tibetan singing bowls.\n   It’s a timer, too. Sometimes you just want a silent meditation. For those occasions, Insight Timer offers a nice timer feature. You can set it like a stopwatch to whatever duration you prefer, and choose among an array of sounds to mark the beginning and end of your sit.\n  It tracks. If you go to the Profile section of the app, you’ll discover it keeps track of your use and displays various summaries in attractive bar plots.\n [And yes, I generally agree with Richard McElreath: “The only problem with barplots is that they have bars” (p. 203, Statistical Rethinking). You can’t have everything.]\n  Insight Timer is great for researchers Insight Timer allows you to download your data as a CSV file, which you can keep for yourself or email to others. Over the years, I’ve used the app to run group meditations within my research protocols. Since I selected the audios from the app, it allowed me to standardize the instructions across meditation sessions. Although my research assistants and I used the app on our phones to play the audios, our participants would use the timer functions on the apps on their phones to record their sessions. This gave us duplicate attendance records: one on a sign-in sheet and another on their phones. In longitudinal studies, participants could use the app on their own time and each of those sessions were recorded in the app. At the end of our studies, we were then able to download their use records as CSV files ready for pre-analysis data wrangling.\n Parting thoughts Insight Timer has other functions, such as social networking and dharma talks. I just don’t care about those things, so you can learn about them on your own. But if you’re interested in learning about meditation or even if you’re a veteran meditator looking for a convenient app to augment your practice with, do consider Insight Timer. There’s no reason to spend your money on the alternatives before you capitalize on such a powerful resource that’s available to you for free.\n ","date":1538870400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538870400,"objectID":"798a524eefe980436a075f3a7f77ccee","permalink":"/post/dont-pay-to-meditate/","publishdate":"2018-10-07T00:00:00Z","relpermalink":"/post/dont-pay-to-meditate/","section":"post","summary":"The other day, my Twitter feed informed me Penn Jillette just clocked in 1000 consecutive days of meditation using the Headspace app. Now he’s considering checking out Sam Harris’s new Waking Up meditation app. Sam left a congratulatory comment on Penn’s tweet.\nWell done, Penn! https://t.co/SZkKocIPEH\n\u0026mdash; Sam Harris (@SamHarrisOrg) October 7, 2018  The whole thing was rainbows and kittens. And it reminded me to pass on some advice: Stop using fee-based meditation apps!","tags":["app","Insight Timer","MBCT","MBRP","MBSR","meditation","mindfulness","smartphone"],"title":"Don't pay to meditate","type":"post"},{"authors":null,"categories":[],"content":"tl;dr I just self-published a book-length version of my project Statistical Rethinking with brms, ggplot2, and the tidyverse. By using Yihui Xie’s bookdown package, I was able to do it for free. If you’ve never heard of it, bookdown enables R users to write books and other long-form articles with R Markdown. You can save your bookdown products in a variety of formats (e.g., PDF, HTML) and publish them in several ways, too. The purpose of this post is to give readers a sense of how I used bookdown to make my project. I propose there are three fundamental skill sets you need basic fluency in before playing with bookdown. Those three are\n R and R Studio, Scripts and R Markdown files, and Git and GitHub.   Start with R First things first. Since bookdown is a package for use in the R environment, you’re going to have to use R. If you’re unfamiliar with it, R is a freely-available programming language particularly well-suited for data analysis. If you’ve not used R before, learning how to self-publish books is a great incentive to start learning. But unless you already have a background in programming, I think bookdown is poorly-suited for novices. R newbies should check out Roger Peng’s R Programming for Data Science or Grolemund and Wickham’s R for Data Science. Both are freely available online and, as it would turn out, made with bookdown. Also, new users should be aware that although you can interact with R directly, there are a variety of other ways to interface with R. I recommend using R Studio. You can find some nice reasons, here. For basic instructions on how to install R and R Studio, you might start here. And if you prefer video tutorials to help you with the installation, just do a simple search in your favorite video-sharing website and several should pop up.\nPersonally, I started using R—via R Studio—during the 2015/2016 winter break before taking a spring semester statistics course based around an R package. [In case you’re curious, it was a structural equation modeling course based around a text by Beaujean which featured the lavaan package]. At the time, I was already familiar with structural equation modeling, so the course was a nice opportunity to learn R. In addition, I was concurrently enrolled in a course on multilevel modeling based on Singer and Willet’s classic text. The professor of that course primarily used SAS to teach the material, but he was flexible and allowed me to do the work with R, instead. So that was my introduction to R–a semester of immersion in #rstats. Here are some other tips on how to learn R.\n bookdown uses Markdown If you work with R through R Studio, you can do a handful of things through dropdowns. But really, if you’re going to be using R, you’re going to be coding. As it turns out, there are a variety of ways to code in R. One of the most basic ways is via the console, which I’m not going to cover in any detail.\nThe console is fine for quick operations, but you’re going to want to do most of your coding in some kind of a script. R Studio allows users to save and execute code in script files, which you can learn more about here. Basic script files are nice in that they allow you to both save and annotate your code.\nHowever, the annotation options in R Studio script files are limited. After using R Studio scripts for about a year, I learned about R Notebooks. These are special files that allow you to intermingle your R code with prose and the results of the code. R Notebooks also allow users to transform the working documents into professional-looking reports in various formats (e.g., PDF, HTML). And unlike the primitive annotation options with simple script files, R Notebooks use Markdown to allow users to format their prose with things like headers, italicized font, insert hyperlinks, and even embed images. So Markdown, then, is a simple language that allows for many of those functions.\nWithin the R Studio environment, you can use Markdown with two basic file types: R Markdown files and R Notebook files. R Notebook files are just special kinds of R Markdown files that have, IMO, a better interface. That is, R Notebooks are the newer nicer version of R Markdown files. The main point here is that when I say “bookdown uses Markdown”, I’m pointing out that one of the important skills you’ll want to develop before making content with bookdown is how to use Markdown within R Studio. It’s not terribly complicated to learn, and you can get an overview of the basics here or here or here, or an exhaustive treatment here.\nIf you’re a novice, it’ll take you a few days, weeks, or months to get a firm grasp of R. Not so with R Markdown files. You’ll have the basics of those down in an afternoon. That said, I had been an R Notebook user for more than a year before trying my hand at bookdown.\nThe first big edition of my Statistical Rethinking with brms, ggplot2, and the tidyverse project came in the form of R Notebook files and their HTML counterparts stored in one of my projects on the Open Science Framework. I don’t update it very often, but you can still find it here. If you’re not familiar with it, the OSF is a “free, open source web application that connects and supports the research workflow, enabling scientists to increase the efficiency and effectiveness of their research.” In addition to their wiki, you might check out some of their video tutorials.\n You’ll need GitHub, too I’m actually not sure whether you need to know how to use Git and GitHub to use bookdown. In his authoritative book, bookdown: Authoring Books and Technical Documents with R Markdown, Yihui Xie mentioned GitHub in every chapter. If you go to your favorite video-sharing website to look for instructional videos on bookdown, you’ll see the instructors take GitHub as a given, too. If you’re stubborn and have enough ingenuity, you might find a way to successfully use bookdown without GitHub, but you may as well go with the flow on this one.\nIf you’ve never heard of it before, Git is a system for version control. By version control, I mean a system by which you can keep track of changes to your code, over time. Even if you don’t have a background in programming, consider a scenario where you had to keep track of many versions of a writing project, perhaps saving your files as first_draft.docx, second_draft.docx, final_draft.docx, final_draft_2.docx… This was your own make-shift attempt at version control for writing. I’ve seen a lot of introductory material recommend Git and GitHub by leading with version control. And indeed, they do serve that purpose. But IMO, leading with version control is a rhetorical mistake when talking to non-programmers. I haven’t found Git and GitHub the most intuitive and if version control was the only benefit, they wouldn’t be worth the effort. But there are other good reasons to learn.\nIMO, the best reason to learn Git and GitHub is because they allow you to make your work publically available. When you just use Git, the work stays on your computer. But GitHub allows you to save your files online, too. This makes it easy for others to review them and give you feedback. GitHub also allows you to save things like data files online. So if you’re a working scientist, Git and GitHub might allow you to make a site—a repository—to house the de-identified data and statistical code for one of your projects. It’s another way to do open science. In addition, you can repurpose GitHub to work as blog or an analytic portfolio. And if you’d like to use bookdown, Git and GitHub will be a part of how you manage the files for your projects and make your work more accessible to others.\nIf you’re new to all this, you could probably blindly follow along with the steps in Yihui Xie’s bookdown manual or any of the online video tutorials. But I suspect that’d be pretty confusing. Before attempting a bookdown project, spend some time getting comfortable with Git and GitHub, first. The best introduction to the topic I’ve seen is Jenny Bryan’s Happy Git and GitHub for the useR, which, you guessed it, is also freely available and powered by bookdown.\nAs I hinted, I found Git and GitHub baffling, at first. I checked out a few online video tutorials, but found them of little help. It really was Bryan’s book that finally got me going. And I’m glad I did. I’ve been slowly working with GitHub for about a year—here’s my profile—and my first major project was putting together the files for the individual chapters in the Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse project. They originally lived as R Notebook files, eventually rendered in a GitHub-friendly .md file format. After a while, I started playing around with README-only projects, which are basically a poor man’s GitHub version of blog posts (e.g., check out this one). For me, and probably for your future bookdown projects, the most important GitHub skills to learn are commits, pushes, and forkes.\nI’d fooled around with GitHub a tiny bit before launching my Statistical Rethinking with brms, ggplot2, and the tidyverse project on the OSF. But it was confusing and after an hour or two of trying to make sense of it, I gave up and just figured the OSF would be good enough. After folks started noticing the project, I got a few comments that it’d be more accessible on GitHub. That was what finally influenced me to buckle down learn it in earnest. I’m still a little clunky with it, but I’m functional enough to do things like make this blog. With a little patience and practice, you can get there, too.\n Let Yihui Xie guide you So far we’ve covered\n R and R Studio Scripts and R Markdown files Git and GitHub  You don’t have become an expert, but you’ll need to become roughly fluent in all three to make good use of bookdown. Basically, if you are able to load data into R, document a rudimentary analysis in an R Notebook file, and then share the project in a non-embarrassing way in GitHub, you’re ready to use bookdown.\nI’ve already mentioned it, but the authoritative work on bookdown is Yihui Xie’s bookdown: Authoring Books and Technical Documents with R Markdown. Yihui Xie, of course, is the author of the package. It’s probably best to just start there, going bit by bit. He also gave an RStudio webinar, Authoring Books with R Markdown, which I found to be a helpful supplement.\nThe complete version of my Statistical Rethinking with brms, ggplot2, and the tidyverse project has 15 chapters and several preamble sections. Almost all the chapters files include a lot of computationally-intensive code, with the simulations for chapter 6 taking multiple hours to compute. I do not recommend starting off with a project like that, at least not all at once. If you follow along with Yihui Xie’s guide, you’ll practice stitching together simple files, first. After learning those basics, I then picked up other helpful tricks, like caching analyses.\nAlthough I didn’t use these resources while I was learning bookdown, you might also benefit from checking out\n Sean Kross’s How to Start a Bookdown Book, Karl Broman’s omg, bookdown!, Rachael Lappan’s Using Bookdown for tidy documentation, or Pablo Casas’s How to self-publish a book: A handy list of resources.   ","date":1538611200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538611200,"objectID":"f9676350628488b9d5a7ed32711cdf5f","permalink":"/post/how-bookdown/","publishdate":"2018-10-04T00:00:00Z","relpermalink":"/post/how-bookdown/","section":"post","summary":"tl;dr I just self-published a book-length version of my project Statistical Rethinking with brms, ggplot2, and the tidyverse. By using Yihui Xie’s bookdown package, I was able to do it for free. If you’ve never heard of it, bookdown enables R users to write books and other long-form articles with R Markdown. You can save your bookdown products in a variety of formats (e.g., PDF, HTML) and publish them in several ways, too.","tags":["Bayesian","bookdown","brms","Git","GitHub","Markdown","R","Statistical Rethinking","tidyerse","tutorial"],"title":"bookdown, My Process","type":"post"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536469200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536469200,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00-05:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]