[{"authors":[],"categories":[],"content":" tl;dr You too can make model diagrams with the tidyverse and patchwork packages. Here’s how.\n Diagrams can help us understand statistical models. I’ve been working through John Kruschke’s Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan and translating it into brms and tidyverse-style workflow. At this point, the bulk of the work is done and you can check it out at https://bookdown.org/content/3686/. One of Kruschke’s unique contributions was the way he used diagrams to depict his statistical models. Here’s an example from the text (Figure 8.2 on page 196):\nIn the figure’s caption, we read:\n Diagram of model with Bernoulli likelihood and beta prior. The pictures of the distributions are intended as stereotypical icons, and are not meant to indicate the exact forms of the distributions. Diagrams like this should be scanned from the bottom up, starting with the data \\(y_i\\) and working upward through the likelihood function and prior distribution. Every arrow in the diagram has a corresponding line of code in a JAGS model specification.\n Making diagrams like this is a bit of a challenge because even Kruchke, who is no R slouch, used other software to make his diagrams. In the comments section from his blog post, Graphical model diagrams in Doing Bayesian Data Analysis versus traditional convention, Kruschke remarked he made these “‘by hand’ in OpenOffice.” If you look over to the How to produce John Kruschke’s Bayesian model diagrams using TikZ or similar tools? thread in StackExchange, you’ll find a workflow to make plots like this with TikZ. In a related GitHub repo, the great Rasmus Bååth showed how to make diagrams like this with a combination of base R and Libre Office Draw.\nIt’d be nice, however, if one could make plots like this entirely within R, preferably with a tidyverse-style workflow. With help from the handy new patchwork package, I believe we can make it work. In this post, I’ll walk through a few attempts.\nMy assumptions. For the sake of this post, I’m presuming you’re familiar with R, aware of the tidyverse, and have fit a Bayesian model or two.\n Figure 8.2: Keep it simple. One way to conceptualize Figure 8.2, above, is to break it down into discrete parts. To my mind, there are five. Starting from the top and going down, we have\n a plot of a beta density, an annotated arrow, a bar plot of Bernoulli data, another annotated arrow, and some text.  If we make and save each component separately with ggplot2, we can then combine them with patchwork syntax. First we’ll laod the necessary packages.\nlibrary(tidyverse) library(patchwork) library(ggforce) We won’t need ggforce for this first diagram, but it’ll come in handy in the next section. Before we start making our subplots, we can use the ggplot2::theme_set() function to adjust the global theme.\ntheme_set(theme_grey() + theme_void() + theme(plot.margin = margin(0, 5.5, 0, 5.5))) Here we’ll make the 5 subplots, saving them as p1, p2, and so on. Since I’m presuming a working fluency with ggplot2 and tidyverse basics, I’m not going to explain the plot code in detail. If you’re new to plotting like this, execute the code for a given plot line by line to see how each layer builds on the last.\n# plot of a beta density p1 \u0026lt;- tibble(x = seq(from = .01, to = .99, by = .01), d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %\u0026gt;% ggplot(aes(x = x, ymin = 0, ymax = d)) + geom_ribbon(fill = \u0026quot;skyblue\u0026quot;, size = 0) + annotate(geom = \u0026quot;text\u0026quot;, x = .5, y = .2, label = \u0026quot;beta\u0026quot;, size = 7) + annotate(geom = \u0026quot;text\u0026quot;, x = .5, y = .6, label = \u0026quot;italic(A)*\u0026#39;, \u0026#39;*italic(B)\u0026quot;, size = 7, family = \u0026quot;Times\u0026quot;, parse = TRUE) + scale_x_continuous(expand = c(0, 0)) + theme(axis.line.x = element_line(size = 0.5)) ## an annotated arrow # save our custom arrow settings my_arrow \u0026lt;- arrow(angle = 20, length = unit(0.35, \u0026quot;cm\u0026quot;), type = \u0026quot;closed\u0026quot;) p2 \u0026lt;- tibble(x = .5, y = 1, xend = .5, yend = 0) %\u0026gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow) + annotate(geom = \u0026quot;text\u0026quot;, x = .375, y = 1/3, label = \u0026quot;\u0026#39;~\u0026#39;\u0026quot;, size = 10, family = \u0026quot;Times\u0026quot;, parse = T) + xlim(0, 1) # bar plot of Bernoulli data p3 \u0026lt;- tibble(x = 0:1, d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %\u0026gt;% ggplot(aes(x = x, y = d)) + geom_col(fill = \u0026quot;skyblue\u0026quot;, width = .4) + annotate(geom = \u0026quot;text\u0026quot;, x = .5, y = .2, label = \u0026quot;Bernoulli\u0026quot;, size = 7) + annotate(geom = \u0026quot;text\u0026quot;, x = .5, y = .94, label = \u0026quot;theta\u0026quot;, size = 7, family = \u0026quot;Times\u0026quot;, parse = T) + xlim(-.75, 1.75) + theme(axis.line.x = element_line(size = 0.5)) # another annotated arrow p4 \u0026lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(\u0026quot;\u0026#39;~\u0026#39;\u0026quot;, \u0026quot;italic(i)\u0026quot;)) %\u0026gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = \u0026quot;Times\u0026quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) # some text p5 \u0026lt;- tibble(x = 1, y = .5, label = \u0026quot;italic(y[i])\u0026quot;) %\u0026gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, parse = T, family = \u0026quot;Times\u0026quot;) + xlim(0, 2) Now we’ve saved each of the components as subplots, we can combine them with a little patchwork syntax.\nlayout \u0026lt;- c( area(t = 1, b = 2, l = 1, r = 1), area(t = 3, b = 3, l = 1, r = 1), area(t = 4, b = 5, l = 1, r = 1), area(t = 6, b = 6, l = 1, r = 1), area(t = 7, b = 7, l = 1, r = 1) ) (p1 + p2 + p3 + p4 + p5) + plot_layout(design = layout) \u0026amp; ylim(0, 1) For that plot, the settings in the R Markdown code chunk were fig.width = 2, fig.height = 3.5. An obvious difference between our plot and Kruschke’s is whereas he depicted the beta density with a line, we used geom_ribbon() to make the shape a solid blue. If you prefer Kruschke’s approach, just use something like geom_line() instead.\nWithin some of the annotate() and geom_text() functions, above, you may have noticed we set parse = T. Though it wasn’t always necessary, it helps streamline the workflow. I found this particularly helpful when setting the coordinates for the tildes (i.e., the \\(\\sim\\) signs).\nThe main thing to focus on is the patchwork syntax from that last code block. We combined the five subplots with the (p1 + p2 + p3 + p4 + p5) code. It was the plot_layout(design = layout) part and the associated code defining layout that helped us arrange the subplots in the right order and according to the desired size ratios. For each subplot, we used the t, b, l, and r parameters to define the four bounds (top, bottom, left, and right) in overall plot grid. You can learn more about how this works from Thomas Lin Pedersen’s Controlling Layouts and Specify a plotting area in a layout vignettes.\nNow we’ve covered the basics, it’s time to build.\n Figure 9.1: Add an offset formula and some curvy lines. For our next challenge, we’ll tackle Kruschke’s Figure 9.1:\nFrom a statistical perspective, this model is interesting in that it uses a hierarchical prior specification wherein the lower-level beta density is parameterized in terms \\(\\omega\\) (mode) and \\(K\\) (concentration). From a plotting perspective, adding more density and arrow subplots isn’t a big deal. But see how the \\(\\omega(K-2)+1, (1-\\omega)(K-2)+1\\) formula extends way out past the right bound of that second beta density? Also, check those wavy arrows right above. These require an amended workflow. Let’s go step by step. The top subplot is farily simple.\np1 \u0026lt;- tibble(x = seq(from = .01, to = .99, by = .01), d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %\u0026gt;% ggplot(aes(x = x, ymin = 0, ymax = d)) + geom_ribbon(fill = \u0026quot;skyblue\u0026quot;, size = 0) + annotate(geom = \u0026quot;text\u0026quot;, x = .5, y = .2, label = \u0026quot;beta\u0026quot;, size = 7) + annotate(geom = \u0026quot;text\u0026quot;, x = .5, y = .6, label = \u0026quot;italic(A[omega])*\u0026#39;, \u0026#39;*italic(B[omega])\u0026quot;, size = 7, family = \u0026quot;Times\u0026quot;, parse = TRUE) + scale_x_continuous(expand = c(0, 0)) + theme(axis.line.x = element_line(size = 0.5)) p1 Now things get wacky.\nWe are going to make the formula and the wavy lies in one subplot. We can define the basic coordinates for the wavy lines with the ggforce::geom_bspline() function (learn more here). For each line segment, we just need about 5 pairs of \\(x\\) and \\(y\\) coordinates. There’s no magic solution to these coordinates. I came to them by trial and error. As far as the formula goes, it isn’t much more complicated from what we’ve been doing. It’s all just a bunch of plotmath syntax. The main deal is to notice how we set the limits in the scale_x_continuous() function to (0, 2). In the other plots, those are restricted to 0, 1.\np2 \u0026lt;- tibble(x = c(.5, .475, .26, .08, .06, .5, .55, .85, 1.15, 1.2), y = c(1, .7, .6, .5, .2, 1, .7, .6, .5, .2), line = rep(letters[2:1], each = 5)) %\u0026gt;% ggplot(aes(x = x, y = y)) + geom_bspline(aes(color = line), size = 2/3, show.legend = F) + annotate(geom = \u0026quot;text\u0026quot;, x = 0, y = .125, label = \u0026quot;omega(italic(K)-2)+1*\u0026#39;, \u0026#39;*(1-omega)(italic(K)-2)+1\u0026quot;, size = 7, parse = T, family = \u0026quot;Times\u0026quot;, hjust = 0) + annotate(geom = \u0026quot;text\u0026quot;, x = 1/3, y = .7, label = \u0026quot;\u0026#39;~\u0026#39;\u0026quot;, size = 10, parse = T, family = \u0026quot;Times\u0026quot;) + scale_color_manual(values = c(\u0026quot;grey75\u0026quot;, \u0026quot;black\u0026quot;)) + scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) + ylim(0, 1) + theme_void() p2 You’ll see how this will works when we combine all the subplots, below. The rest of the subplots are similar or identical to the ones from the first section. Here we’ll make them in bulk.\n# another beta density p3 \u0026lt;- tibble(x = seq(from = .01, to = .99, by = .01), d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %\u0026gt;% ggplot(aes(x = x, ymin = 0, ymax = d)) + geom_ribbon(fill = \u0026quot;skyblue\u0026quot;, size = 0) + annotate(geom = \u0026quot;text\u0026quot;, x = .5, y = .2, label = \u0026quot;beta\u0026quot;, size = 7) + scale_x_continuous(expand = c(0, 0)) + theme(axis.line.x = element_line(size = 0.5)) # an annotated arrow p4 \u0026lt;- tibble(x = .5, y = 1, xend = .5, yend = 0) %\u0026gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(arrow = my_arrow) + annotate(geom = \u0026quot;text\u0026quot;, x = .375, y = 1/3, label = \u0026quot;\u0026#39;~\u0026#39;\u0026quot;, size = 10, family = \u0026quot;Times\u0026quot;, parse = T) + xlim(0, 1) # bar plot of Bernoulli data p5 \u0026lt;- tibble(x = 0:1, d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %\u0026gt;% ggplot(aes(x = x, y = d)) + geom_col(fill = \u0026quot;skyblue\u0026quot;, width = .4) + annotate(geom = \u0026quot;text\u0026quot;, x = .5, y = .2, label = \u0026quot;Bernoulli\u0026quot;, size = 7) + annotate(geom = \u0026quot;text\u0026quot;, x = .5, y = .94, label = \u0026quot;theta\u0026quot;, size = 7, family = \u0026quot;Times\u0026quot;, parse = T) + xlim(-.75, 1.75) + theme(axis.line.x = element_line(size = 0.5)) # another annotated arrow p6 \u0026lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(\u0026quot;\u0026#39;~\u0026#39;\u0026quot;, \u0026quot;italic(i)\u0026quot;)) %\u0026gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = \u0026quot;Times\u0026quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) # some text p7 \u0026lt;- tibble(x = .5, y = .5, label = \u0026quot;italic(y[i])\u0026quot;) %\u0026gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, parse = T, family = \u0026quot;Times\u0026quot;) + xlim(0, 1) Now combine the subplots with patchwork.\nlayout \u0026lt;- c( area(t = 1, b = 2, l = 1, r = 1), area(t = 4, b = 5, l = 1, r = 1), area(t = 3, b = 4, l = 1, r = 2), area(t = 6, b = 6, l = 1, r = 1), area(t = 7, b = 8, l = 1, r = 1), area(t = 9, b = 9, l = 1, r = 1), area(t = 10, b = 10, l = 1, r = 1) ) (p1 + p3 + p2 + p4 + p5 + p6 + p7) + plot_layout(design = layout) \u0026amp; ylim(0, 1) There are a few reasons why this worked. First, we superimposed the subplot with the formula and wavy lines (p2) atop of the second density (p3) by ordering the plots as (p1 + p3 + p2 + p4 + p5 + p6 + p7). Placing one plot atop another was made easy by our use of theme_void(), which made the backgrounds for all the subplots transparent. But also look at how we set the r argument to 2 within the area() function for our p2. That’s what bought us that extra space for the formula.\n Figure 9.7: Add more curvy lines and a second density to the top row. For our next challenge, we’ll tackle Kruschke’s Figure 9.7:\nThis is a mild extension of the previous one. From a plotting perspective, the noteworthy new features are we have two density plots on the top row and now we have to juggle two pairs of wiggly lines in the subplot with the formula. The two subplots in the top row are no big deal. To make the gamma density, just use the dgamma() function in place of dbeta().\n# a beta density p1 \u0026lt;- tibble(x = seq(from = .01, to = .99, by = .01), d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %\u0026gt;% ggplot(aes(x = x, ymin = 0, ymax = d)) + geom_ribbon(fill = \u0026quot;skyblue\u0026quot;, size = 0) + annotate(geom = \u0026quot;text\u0026quot;, x = .5, y = .2, label = \u0026quot;beta\u0026quot;, size = 7) + annotate(geom = \u0026quot;text\u0026quot;, x = .5, y = .6, label = \u0026quot;italic(A[omega])*\u0026#39;, \u0026#39;*italic(B[omega])\u0026quot;, size = 7, family = \u0026quot;Times\u0026quot;, parse = TRUE) + scale_x_continuous(expand = c(0, 0)) + theme(axis.line.x = element_line(size = 0.5)) # a gamma density p2 \u0026lt;- tibble(x = seq(from = 0, to = 5, by = .01), d = (dgamma(x, 1.75, .85) / max(dgamma(x, 1.75, .85)))) %\u0026gt;% ggplot(aes(x = x, ymin = 0, ymax = d)) + geom_ribbon(fill = \u0026quot;skyblue\u0026quot;, size = 0) + annotate(geom = \u0026quot;text\u0026quot;, x = 2.5, y = .2, label = \u0026quot;gamma\u0026quot;, size = 7) + annotate(geom = \u0026quot;text\u0026quot;, x = 2.5, y = .6, label = \u0026quot;list(italic(S)[kappa], italic(R)[kappa])\u0026quot;, size = 7, family = \u0026quot;Times\u0026quot;, parse = TRUE) + scale_x_continuous(expand = c(0, 0)) + theme(axis.line.x = element_line(size = 0.5)) The third subplot contains our offset formula and two sets of wiggly lines.\np3 \u0026lt;- tibble(x = c(.5, .475, .26, .08, .06, .5, .55, .85, 1.15, 1.175, 1.5, 1.4, 1, .25, .2, 1.5, 1.49, 1.445, 1.4, 1.39), y = c(1, .7, .6, .5, .2, 1, .7, .6, .5, .2, 1, .7, .6, .5, .2, 1, .75, .6, .45, .2), line = rep(letters[2:1], each = 5) %\u0026gt;% rep(., times = 2), plot = rep(1:2, each = 10)) %\u0026gt;% ggplot(aes(x = x, y = y, group = interaction(plot, line))) + geom_bspline(aes(color = line), size = 2/3, show.legend = F) + annotate(geom = \u0026quot;text\u0026quot;, x = 0, y = .1, label = \u0026quot;omega(kappa-2)+1*\u0026#39;, \u0026#39;*(1-omega)(kappa-2)+1\u0026quot;, size = 7, parse = T, family = \u0026quot;Times\u0026quot;, hjust = 0) + annotate(geom = \u0026quot;text\u0026quot;, x = c(1/3, 1.15), y = .7, label = \u0026quot;\u0026#39;~\u0026#39;\u0026quot;, size = 10, parse = T, family = \u0026quot;Times\u0026quot;) + scale_color_manual(values = c(\u0026quot;grey75\u0026quot;, \u0026quot;black\u0026quot;)) + scale_x_continuous(expand = c(0, 0), limits = c(0, 2)) + ylim(0, 1) p3 The rest of the subplots are similar or identical to the ones from the last section. Here we’ll make them in bulk.\n# another beta density p4 \u0026lt;- tibble(x = seq(from = .01, to = .99, by = .01), d = (dbeta(x, 2, 2)) / max(dbeta(x, 2, 2))) %\u0026gt;% ggplot(aes(x = x, ymin = 0, ymax = d)) + geom_ribbon(fill = \u0026quot;skyblue\u0026quot;, size = 0) + annotate(geom = \u0026quot;text\u0026quot;, x = .5, y = .2, label = \u0026quot;beta\u0026quot;, size = 7) + scale_x_continuous(expand = c(0, 0)) + theme(axis.line.x = element_line(size = 0.5)) # an annotated arrow p5 \u0026lt;- tibble(x = c(.375, .625), y = c(1/3, 1/3), label = c(\u0026quot;\u0026#39;~\u0026#39;\u0026quot;, \u0026quot;italic(s)\u0026quot;)) %\u0026gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = \u0026quot;Times\u0026quot;) + geom_segment(x = 0.5, xend = 0.5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) # bar plot of Bernoulli data p6 \u0026lt;- tibble(x = 0:1, d = (dbinom(x, size = 1, prob = .6)) / max(dbinom(x, size = 1, prob = .6))) %\u0026gt;% ggplot(aes(x = x, y = d)) + geom_col(fill = \u0026quot;skyblue\u0026quot;, width = .4) + annotate(geom = \u0026quot;text\u0026quot;, x = .5, y = .2, label = \u0026quot;Bernoulli\u0026quot;, size = 7) + annotate(geom = \u0026quot;text\u0026quot;, x = .5, y = .94, label = \u0026quot;theta\u0026quot;, size = 7, family = \u0026quot;Times\u0026quot;, parse = T) + xlim(-.75, 1.75) + theme(axis.line.x = element_line(size = 0.5)) # another annotated arrow p7 \u0026lt;- tibble(x = c(.35, .65), y = c(1/3, 1/3), label = c(\u0026quot;\u0026#39;~\u0026#39;\u0026quot;, \u0026quot;italic(i)*\u0026#39;|\u0026#39;*italic(s)\u0026quot;)) %\u0026gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = c(10, 7), parse = T, family = \u0026quot;Times\u0026quot;) + geom_segment(x = .5, xend = .5, y = 1, yend = 0, arrow = my_arrow) + xlim(0, 1) # some text p8 \u0026lt;- tibble(x = .5, y = .5, label = \u0026quot;italic(y[i])[\u0026#39;|\u0026#39;][italic(s)]\u0026quot;) %\u0026gt;% ggplot(aes(x = x, y = y, label = label)) + geom_text(size = 7, parse = T, family = \u0026quot;Times\u0026quot;) + xlim(0, 1) Now combine the subplots with patchwork.\nlayout \u0026lt;- c( area(t = 1, b = 2, l = 1, r = 1), area(t = 1, b = 2, l = 2, r = 2), area(t = 4, b = 5, l = 1, r = 1), area(t = 3, b = 4, l = 1, r = 2), area(t = 6, b = 6, l = 1, r = 1), area(t = 7, b = 8, l = 1, r = 1), area(t = 9, b = 9, l = 1, r = 1), area(t = 10, b = 10, l = 1, r = 1) ) (p1 + p2 + p4 + p3 + p5 + p6 + p7 + p8) + plot_layout(design = layout) \u0026amp; ylim(0, 1) Boom; we did it!\n  Limitations Though I’m overall pleased with this workflow, it’s not without limitations. To keep the values in our area() functions simple, we scaled the density plots to be twice the size of the arrow plots. With simple ratios like 1/2, this works well but it can be a bit of a pain with more exotic ratios. The size and proportions of the fonts are quite sensitive to the overall height and width values for the final plot. You’ll find similar issues with the coordinates for the wiggly geom_bspline() lines. Getting these right will likely take a few iterations. Speaking of geom_bspline(), I’m also not happy that there doesn’t appear to be an easy way to have them end with arrow heads. Perhaps you could hack some in with another layer of geom_segment().\nLimitations aside, I hope this helps makes it one step easier for applied researchers to create their own Kruschke-stype model diagrams. Happy plotting!\n Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggforce_0.3.1 patchwork_1.0.0 forcats_0.4.0 stringr_1.4.0 ## [5] dplyr_0.8.4 purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 ## [9] tibble_2.1.3 ggplot2_3.2.1 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.0.0 xfun_0.12 haven_2.2.0 lattice_0.20-38 ## [5] colorspace_1.4-1 vctrs_0.2.2 generics_0.0.2 htmltools_0.4.0 ## [9] yaml_2.2.1 rlang_0.4.5 pillar_1.4.3 withr_2.1.2 ## [13] glue_1.3.1 DBI_1.1.0 tweenr_1.0.1 dbplyr_1.4.2 ## [17] modelr_0.1.5 readxl_1.3.1 lifecycle_0.1.0 munsell_0.5.0 ## [21] blogdown_0.17 gtable_0.3.0 cellranger_1.1.0 rvest_0.3.5 ## [25] evaluate_0.14 labeling_0.3 knitr_1.26 fansi_0.4.1 ## [29] broom_0.5.3 Rcpp_1.0.3 backports_1.1.5 scales_1.1.0 ## [33] jsonlite_1.6.1 farver_2.0.3 fs_1.3.1 hms_0.5.3 ## [37] digest_0.6.23 stringi_1.4.6 bookdown_0.17 polyclip_1.10-0 ## [41] grid_3.6.2 cli_2.0.1 tools_3.6.2 magrittr_1.5 ## [45] lazyeval_0.2.2 crayon_1.3.4 pkgconfig_2.0.3 MASS_7.3-51.4 ## [49] xml2_1.2.2 reprex_0.3.0 lubridate_1.7.4 assertthat_0.2.1 ## [53] rmarkdown_2.0 httr_1.4.1 rstudioapi_0.10 R6_2.4.1 ## [57] nlme_3.1-142 compiler_3.6.2  ","date":1583712000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583712000,"objectID":"d8a035dabbd036f79851d629e74c3cd9","permalink":"/post/make-model-diagrams-kruschke-style/","publishdate":"2020-03-09T00:00:00Z","relpermalink":"/post/make-model-diagrams-kruschke-style/","section":"post","summary":"tl;dr You too can make model diagrams with the tidyverse and patchwork packages. Here’s how.\n Diagrams can help us understand statistical models. I’ve been working through John Kruschke’s Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan and translating it into brms and tidyverse-style workflow. At this point, the bulk of the work is done and you can check it out at https://bookdown.org/content/3686/. One of Kruschke’s unique contributions was the way he used diagrams to depict his statistical models.","tags":["Bayesian","Kruschke","plot","R","tidyverse","tutorial"],"title":"Make model diagrams, Kruschke style","type":"post"},{"authors":[],"categories":[],"content":" tl;dr When you have a time-varying covariate you’d like to add to a multilevel growth model, it’s important to break that variable into two. One part of the variable will account for within-person variation. The other part will account for between person variation. Keep reading to learn how you might do so when your time-varying covariate is binary.\n I assume things. For this post, I’m presuming you are familiar with longitudinal multilevel models and vaguely familiar with the basic differences between frequentist and Bayesian statistics. All code in is R, with a heavy use of the tidyverse–which you might learn a lot about here, especially chapter 5–, and the brms package for Bayesian regression.\n Context In my applied work, one of my collaborators collects longitudinal behavioral data. They are in the habit of analyzing their focal dependent variables (DVs) with variants of the longitudinal multilevel model, which is great. Though they often collect their primary independent variables (IVs) at all time points, they typically default to only using the baseline values for their IVs to predict the random intercepts and slopes of the focal DVs.\nIt seems like we’re making inefficient use of the data. At first I figured we’d just use the IVs at all time points, which would be treating them as time-varying covariates. But time varying covariates don’t allow one to predict variation in the random intercepts and slopes, which I and my collaborator would like to do. So while using the IVs at all time points as time-varying covariates makes use of more of the available data, it requires us to trade one substantive focus for another, which seems frustrating.\nAfter low-key chewing on this for a while, I recalled that it’s possible to decompose time-varying covariates into measures of traits and states. Consider the simple case where your time-varying covariate, \\(x_{ij}\\) is continuous. In this notation, the \\(x\\) values vary across persons \\(i\\) and time points \\(j\\). If we compute the person level mean, \\(\\overline x_i\\), that would be a time-invariant covariate and would, conceptually, be a measure of a person’s trait level for \\(x\\). Even if you do this, it’s still okay to include both \\(\\overline x_i\\) and \\(x_{ij}\\) in the model equation. The former would be the time-invariant covariate that might predict the variation in the random intercepts and slopes. The latter would still serve as a time-varying covariate that might account for the within-person variation in the DV over time.\nThere, of course, are technicalities about how one might center \\(\\overline x_i\\) and \\(x_{ij}\\) that one should carefully consider for these kinds of models. Enders \u0026amp; Tofighi (2007) covered the issue from a cross-sectional perspective. Hoffman (2015) covered it from a longitudinal perspective. But in the grand scheme of things, those are small potatoes. The main deal is that I can use our IVs as both time-varying and time-invariant predictors.\nI was pretty excited once I remembered all this.\nBut then I realized that some of my collaborator’s IVs are binary, which initially seemed baffling, to me. Would it be sensible to compute \\(\\overline x_i\\) for a binary time-varying covariate? What would that mean for the time-varying version of the variable? So I did what any responsible postdoctoral researcher would do. I posed the issue on Twitter.\nContext: multilevel growth models.\nWhen you have a continuous time-varying covariate, you can decompose it into two variables, an id-level grand mean and the time-specific deviations from that mean. Is anyone aware of a complimentary approach for binary time-varying covariates?\n\u0026mdash; Solomon Kurz (@SolomonKurz) October 26, 2019  My initial thoughts on the topic were a little confused. I wasn’t differentiating well between issues about the variance decomposition and centering and I’m a little embarrassed over that gaff. But I’m still glad I posed the question to Twitter. My virtual colleagues came through in spades! In particular, I’d like to give major shout outs to Andrea Howard (@DrAndreaHoward), Mattan Ben-Shachar (@mattansb), and Aidan Wright (@aidangcw), who collectively pointed me to the solution. It was detailed in the references I listed, above: Enders \u0026amp; Tofighi (2007) and Hoffman (2015). Thank you, all!\nHere’s the deal: Yes, you simply take the person-level means for the binary covariate \\(x\\). That will create a vector of time-invariant IVs ranging continuously from 0 to 1. They’ll be in a probability metric and they conceptually index a person’s probability of endorsing 1 over time. It’s basically the same as a batting average in baseball. You are at liberty to leave the time-invariant covariate in this metric, or you could center it by standardizing or some other sensible transformation. As for the state version of the IV, \\(x_{ij}\\), you’d just leave it exactly as it is. [There are other ways to code binary data, such as effects coding. I’m not a fan and will not be covering that in detail, here. But yes, you could recode your time-varying binary covariate that way, too.]\n Break out the data We should practice this with some data. I’ve been chipping away at working through Singer and Willett’s classic (2003) text, Applied longitudinal data analysis: Modeling change and event occurrence with brms and tidyverse code. You can find the working files in this GitHub repository. In chapter 5, Singer and Willett worked through a series of examples with a data set with a continuous DV and a binary IV. Here are those data.\nlibrary(tidyverse) d \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/ASKurz/Applied-Longitudinal-Data-Analysis-with-brms-and-the-tidyverse/master/data/unemployment_pp.csv\u0026quot;) glimpse(d) ## Observations: 674 ## Variables: 4 ## $ id \u0026lt;dbl\u0026gt; 103, 103, 103, 641, 641, 641, 741, 846, 846, 846, 937, 937, 11… ## $ months \u0026lt;dbl\u0026gt; 1.149897, 5.946612, 12.911704, 0.788501, 4.862423, 11.827515, … ## $ cesd \u0026lt;dbl\u0026gt; 25, 16, 33, 27, 7, 25, 40, 2, 22, 0, 3, 8, 3, 0, 5, 7, 18, 26,… ## $ unemp \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,… Set the stage with descriptive plots. The focal DV is cesd, a continuous variable measuring depression. Singer and Willett (2003):\n Each time participants completed the Center for Epidemiologic Studies’ Depression (CES-D) scale (Radloff, 1977), which asks them to rate, on a four-point scale, the frequency with which they experience each of the 20 depressive symptoms. The CES-D scores can vary from a low or 0 for someone with no symptoms to a high of 80 for someone in serious distress. (p. 161)\n Here’s what the cesd scores look like, collapsing over time.\ntheme_set(theme_gray() + theme(panel.grid = element_blank())) d %\u0026gt;% ggplot(aes(x = cesd)) + geom_histogram(fill = \u0026quot;grey50\u0026quot;, binwidth = 1) + scale_y_continuous(NULL, breaks = NULL) Since these are longutdnial data, our fundamental IV is a measure of time. That’s captured in the months column. Most participants have data on just three occasions and the months values range from about 0 to 15.\nd %\u0026gt;% ggplot(aes(x = months)) + geom_histogram(fill = \u0026quot;grey50\u0026quot;, binwidth = 1) + scale_y_continuous(NULL, breaks = NULL) The main research question we’ll be addressing is: What do participants’ cesd scores look like over time and to what extent does their employment/unemployment status help explain their depression? So our substantive IV of interest is unemp, which is coded 0 = employed and 1 = unemployed. Since participants were recruited from local unemployment offices, everyone started off as unemp == 1. The values varied after that. Here’s a look at the data from a random sample of 25 of the participants.\n# this makes `sample_n()` reproducible set.seed(5) # wrangle the data a little d %\u0026gt;% nest(data = c(months, cesd, unemp)) %\u0026gt;% sample_n(size = 25) %\u0026gt;% unnest(data) %\u0026gt;% mutate(id = str_c(\u0026quot;id: \u0026quot;, id), e = if_else(unemp == 0, \u0026quot;employed\u0026quot;, \u0026quot;unemployed\u0026quot;)) %\u0026gt;% # plot ggplot(aes(x = months, y = cesd)) + geom_line(aes(group = id), size = 1/4) + geom_point(aes(color = e), size = 7/4) + scale_color_manual(NULL, values = c(\u0026quot;blue3\u0026quot;, \u0026quot;red3\u0026quot;)) + theme(panel.grid = element_blank(), legend.position = \u0026quot;top\u0026quot;) + facet_wrap(~id, nrow = 5)  Embrace the hate. To be honest, I kinda hate these data. There are too few measurement occasions within participants for my liking and the assessment schedule just seems bazar. As we’ll see in a bit, these data are also unideal to address exactly the kinds of models this blog is centered on.\nYet it’s for just these reasons I love these data. Real-world data analysis is ugly. The data are never what you want or expected them to be. So it seems the data we use in our educational materials should be equally terrible.\nMuch like we do for our most meaningful relationships, let’s embrace our hate/love ambivalence for our data with wide-open eyes and tender hearts. 🖤\n Time to model. Following Singer and Willett, we can define our first model using a level-1/level-2 specification. The level-1 model would be\n\\[ \\text{cesd}_{ij} = \\pi_{0i} + \\pi_{1i} \\text{months}_{ij} + \\pi_{2i} \\text{unemp}_{ij} + \\epsilon_{ij}, \\] where \\(\\pi_{0i}\\) is the intercept, \\(\\pi_{1i}\\) is the effect of months on cesd, and \\(\\pi_{2i}\\) is the effect of unemp on cesd. The final term, \\(\\epsilon_{ij}\\), is the within-person variation not accounted for by the model–sometimes called error or residual variance. Our \\(\\epsilon_{ij}\\) term follows the usual distribution of\n\\[ \\epsilon_{ij} \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon), \\]\nwhich, in words, means that the within-person variance estimates are normally distributed with a mean of zero and a standard deviation that’s estimated from the data. The corresponding level-2 model follows the form\n\\[\\begin{align*} \\pi_{0i} \u0026amp; = \\gamma_{00} + \\zeta_{0i} \\\\ \\pi_{1i} \u0026amp; = \\gamma_{10} + \\zeta_{1i} \\\\ \\pi_{2i} \u0026amp; = \\gamma_{20}, \\end{align*}\\]\nwhere \\(\\gamma_{00}\\) is the grand mean for the intercept, which varies by person, as captured by the level-2 variance term \\(\\zeta_{0i}\\). Similarly, \\(\\gamma_{10}\\) is the grand mean for the effect of months, which varies by person, as captured by the second level-2 variance term \\(\\zeta_{1i}\\). With this parameterization, it turns out \\(\\pi_{2i}\\) does not vary by person and so its \\(\\gamma_{20}\\) terms does not get a corresponding level-2 variance coefficient. If we wanted the effects of the time-varying covariate unemp to vary across individuals, we’d expand the definition of \\(\\pi_{2i}\\) to be\n\\[ \\pi_{2i} = \\gamma_{20} + \\zeta_{2i}. \\]\nWithin our brms paradigm, the two level-2 variance parameters follow the form\n\\[\\begin{align*} \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\\\ \\end{bmatrix} \u0026amp; \\sim \\operatorname{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf{D} \\mathbf{\\Omega} \\mathbf{D}\u0026#39; \\Bigg ), \\text{where} \\\\ \\mathbf{D} \u0026amp; = \\begin{bmatrix} \\sigma_0 \u0026amp; 0 \\\\ 0 \u0026amp; \\sigma_1 \\end{bmatrix} \\text{and} \\\\ \\mathbf{\\Omega} \u0026amp; = \\begin{bmatrix} 1 \u0026amp; \\rho_{01} \\\\ \\rho_{01} \u0026amp; 1 \\end{bmatrix}. \\end{align*}\\]\nI’ll be using a weakly-regularizing approach for the model priors in this post. I detail how I came to these in the Chapter 5 file from my GitHub repo. If you check that file, you’ll see this model is a simplified version of fit10. Here are our priors:\n\\[\\begin{align*} \\gamma_{00} \u0026amp; \\sim \\operatorname{Normal}(14.5, 20) \\\\ \\gamma_{10} \\text{ and } \\gamma_{20} \u0026amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma_\\epsilon, \\sigma_0, \\text{ and } \\sigma_1 \u0026amp; \\sim \\operatorname{Student-t} (3, 0, 10) \\\\ \\Omega \u0026amp; \\sim \\operatorname{LKJ} (4). \\end{align*}\\]\nFeel free to explore different priors on your own. But now we’re done spelling our our first model, it’s time to fire up our main statistical package, brms.\nlibrary(brms) We can fit the model with brms::brm(), like so.\nfit1 \u0026lt;- brm(data = d, family = gaussian, cesd ~ 0 + intercept + months + unemp + (1 + months | id), prior = c(prior(normal(14.5, 20), class = b, coef = \u0026quot;intercept\u0026quot;), prior(normal(0, 10), class = b), prior(student_t(3, 0, 10), class = sd), prior(student_t(3, 0, 10), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = .95), seed = 5) Before we explore the results from this model, we should point out that we only included unemp as a level-1 time-varying predictor. As Hoffman pointed out in her (2015) text, the flaw in this approach is that\n time-varying predictors contain both between-person and within-person information…\n[Thus,] time-varying predictors will need to be represented by two separate predictors that distinguish their between-person and within-person sources of variance in order to properly distinguish their potential between-person and within-person effects on a longitudinal outcome. (pp. 329, 333, emphasis in the original)\n The simplest way to separate the between-person variance in unemp from the pure within-person variation is to compute a new variable capturing \\(\\overline{\\text{unemp}}_i\\), the person-level means for their unemployment status. Here we compute that variable, which we’ll call unemp_id_mu.\nd \u0026lt;- d %\u0026gt;% group_by(id) %\u0026gt;% mutate(unemp_id_mu = mean(unemp)) %\u0026gt;% ungroup() head(d) ## # A tibble: 6 x 5 ## id months cesd unemp unemp_id_mu ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 103 1.15 25 1 1 ## 2 103 5.95 16 1 1 ## 3 103 12.9 33 1 1 ## 4 641 0.789 27 1 0.333 ## 5 641 4.86 7 0 0.333 ## 6 641 11.8 25 0 0.333 Because umemp is binary, \\(\\overline{\\text{unemp}}_i\\) can only take on values ranging from 0 to 1. Here are the unique values we have for unemp_id_mu.\nd %\u0026gt;% distinct(unemp_id_mu) ## # A tibble: 4 x 1 ## unemp_id_mu ## \u0026lt;dbl\u0026gt; ## 1 1 ## 2 0.333 ## 3 0.667 ## 4 0.5 Because each participant’s \\(\\overline{\\text{unemp}}_i\\) was based on 3 or fewer measurement occasions, basic algebra limited the variability in our unemp_id_mu values. You’ll also note that there were no 0s. This, recall, is because participants were recruited at local unemployment offices, leaving all participants with at least one starting value of unemp == 1.\nWe should rehearse how we might interpret the unemp_id_mu values. First recall they are considered level-2 variables; they are between-participant variables. Since they are averages of binary data, they are in a probability metric. In this instance, they are each participants overall probability of being unemployed–their trait-level propensity toward unemployment. No doubt these values would be more reliable if they were computed from data on a greater number of assessment occasions. But with three measurement occasions, we at least have a sense of stability.\nSince our new \\(\\overline{\\text{unemp}}_i\\) variable is a level-2 predictor, the level-1 equation for our next model is the same as before:\n\\[ \\text{cesd}_{ij} = \\pi_{0i} + \\pi_{1i} \\text{months}_{ij} + \\pi_{2i} \\text{unemp}_{ij} + \\epsilon_{ij}. \\]\nHowever, there are two new terms in our level-2 model,\n\\[\\begin{align*} \\pi_{0i} \u0026amp; = \\gamma_{00} + \\gamma_{01} (\\overline{\\text{unemp}}_i) + \\zeta_{0i} \\\\ \\pi_{1i} \u0026amp; = \\gamma_{10} + \\gamma_{11} (\\overline{\\text{unemp}}_i) + \\zeta_{1i} \\\\ \\pi_{2i} \u0026amp; = \\gamma_{20}, \\end{align*}\\]\nwhich is meant to convey that \\(\\overline{\\text{unemp}}_i\\) is allowed to explain variability in both initial status on CES-D scores (i.e., the random intercepts) and change in CES-D scores over time (i.e., the random months slopes). Our variance parameters are all the same:\n\\[\\begin{align*} \\epsilon_{ij} \u0026amp; \\sim \\operatorname{Normal} (0, \\sigma_\\epsilon) \\text{ and} \\\\ \\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\\\ \\end{bmatrix} \u0026amp; \\sim \\operatorname{Normal} \\Bigg ( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf{D} \\mathbf{\\Omega} \\mathbf{D}\u0026#39; \\Bigg ), \\text{where} \\\\ \\mathbf{D} \u0026amp; = \\begin{bmatrix} \\sigma_0 \u0026amp; 0 \\\\ 0 \u0026amp; \\sigma_1 \\end{bmatrix} \\text{and} \\\\ \\mathbf{\\Omega} \u0026amp; = \\begin{bmatrix} 1 \u0026amp; \\rho_{01} \\\\ \\rho_{01} \u0026amp; 1 \\end{bmatrix}. \\end{align*}\\]\nOur priors also follow the same basic specification as before:\n\\[\\begin{align*} \\gamma_{00} \u0026amp; \\sim \\operatorname{Normal}(14.5, 20) \\\\ \\gamma_{01}, \\gamma_{10}, \\gamma_{11}, \\text{ and } \\gamma_{20} \u0026amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma_\\epsilon, \\sigma_0, \\text{ and } \\sigma_1 \u0026amp; \\sim \\operatorname{Student-t} (3, 0, 10) \\\\ \\Omega \u0026amp; \\sim \\operatorname{LKJ} (4). \\end{align*}\\]\nNote, however, that the inclusion of our new level-2 predictor, \\((\\overline{\\text{unemp}}_i)\\), changes the meaning of the intercept, \\(\\gamma_{00}\\). The intercept is now the expected value for a person for whom unemp_id_mu == 0 at the start of the study (i.e., months == 0). I still think our intercept prior from the first model is fine for this example. But do think carefully about the priors you use in your real-world data analyses.\nHere’s how to fit the udpdate model with brms.\nfit2 \u0026lt;- brm(data = d, family = gaussian, cesd ~ 0 + intercept + months + unemp + unemp_id_mu + unemp_id_mu:months + (1 + months | id), prior = c(prior(normal(14.5, 20), class = b, coef = \u0026quot;intercept\u0026quot;), prior(normal(0, 10), class = b), prior(student_t(3, 0, 10), class = sd), prior(student_t(3, 0, 10), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5) We should fit one more model before we look at the parameters. If you were paying close attention, above, you may have noticed how it’s odd that we kept unemp_id_mu in it’s natural metric. Sure, it’s fine in principle–sensible even–to use a variable in a probability metric. But in this particular study, none of the participants had a value of unemp_id_mu == 0 because all of them were unemployed at the first time point. Though it is mathematically kosher to fit a model with an intercept based on unemp_id_mu == 0, it’s awkward to interpret. So in this case, it makes sense to transform the metric of our level-2 predictor. Perhaps the simplest way is to standardize the variable. That would then give an intercept based on the average unemp_id_mu value and a \\(\\gamma_{01}\\) coefficient that was the expected change in intercept based on a one-standard-deviation higher value in unemp_id_mu. Let’s compute that new standardized variable, which we’ll call unemp_id_mu_s.\nd \u0026lt;- d %\u0026gt;% nest(data = c(months:unemp)) %\u0026gt;% mutate(unemp_id_mu_s = (unemp_id_mu - mean(unemp_id_mu)) / sd(unemp_id_mu)) %\u0026gt;% unnest(data) head(d) ## # A tibble: 6 x 6 ## id unemp_id_mu months cesd unemp unemp_id_mu_s ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 103 1 1.15 25 1 0.873 ## 2 103 1 5.95 16 1 0.873 ## 3 103 1 12.9 33 1 0.873 ## 4 641 0.333 0.789 27 1 -1.58 ## 5 641 0.333 4.86 7 0 -1.58 ## 6 641 0.333 11.8 25 0 -1.58 The model formula is the same as before with the exception that we replace unemp_id_mu with unemp_id_mu_s. For simplicity, I’m leaving the priors the way they were.\nfit3 \u0026lt;- brm(data = d, family = gaussian, cesd ~ 0 + intercept + months + unemp + unemp_id_mu_s + unemp_id_mu_s:months + (1 + months | id), prior = c(prior(normal(14.5, 20), class = b, coef = \u0026quot;intercept\u0026quot;), prior(normal(0, 10), class = b), prior(student_t(3, 0, 10), class = sd), prior(student_t(3, 0, 10), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = .9), seed = 5) Instead of examining each of the model summaries one by one, we’ll condense the information into a series of coefficient plots. For simplicity, we’ll restrict our focus to the \\(\\gamma\\) parameters.\n# extract the `fit1` summaries fixef(fit1) %\u0026gt;% data.frame() %\u0026gt;% rownames_to_column(\u0026quot;par\u0026quot;) %\u0026gt;% mutate(fit = \u0026quot;fit1\u0026quot;) %\u0026gt;% bind_rows( # add the `fit2` summaries fixef(fit2) %\u0026gt;% data.frame() %\u0026gt;% rownames_to_column(\u0026quot;par\u0026quot;) %\u0026gt;% mutate(fit = \u0026quot;fit2\u0026quot;), # add the `fit2` summaries fixef(fit3) %\u0026gt;% data.frame() %\u0026gt;% rownames_to_column(\u0026quot;par\u0026quot;) %\u0026gt;% mutate(fit = \u0026quot;fit3\u0026quot;) ) %\u0026gt;% # rename the parameters mutate(gamma = case_when( par == \u0026quot;intercept\u0026quot; ~ \u0026quot;gamma[0][0]\u0026quot;, par == \u0026quot;months\u0026quot; ~ \u0026quot;gamma[1][0]\u0026quot;, par == \u0026quot;unemp\u0026quot; ~ \u0026quot;gamma[2][0]\u0026quot;, str_detect(par, \u0026quot;:\u0026quot;) ~ \u0026quot;gamma[1][1]\u0026quot;, par == \u0026quot;unemp_id_mu\u0026quot; ~ \u0026quot;gamma[0][1]\u0026quot;, par == \u0026quot;unemp_id_mu_s\u0026quot; ~ \u0026quot;gamma[0][1]\u0026quot; )) %\u0026gt;% # plot! ggplot(aes(x = fit, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_hline(yintercept = 0, color = \u0026quot;white\u0026quot;) + geom_pointrange(fatten = 3) + xlab(NULL) + coord_flip() + facet_wrap(~gamma, nrow = 3, scale = \u0026quot;free_x\u0026quot;, labeller = label_parsed) In case you’re not familiar with the output from the brms::fixef() function, each of the parameter estimates are summarized by their posterior means (i.e,. the dots) and percentile-based 95% intervals (i.e., the horizontal lines).\nRecall how earlier I complained that these data weren’t particularly good for demonstrating this method? Well, here you finally get to see why. Regardless of the model, the estimates didn’t change much. In these data, the predictive utility of our between-level variable, unemp_id_mu–standardized or not–, was just about zilch. This is summarized by the \\(\\gamma_{01}\\) and \\(\\gamma_{11}\\) parameters. Both are centered around zero for both models containing them. Thus adding in an inconsequential level-2 predictor had little effect on its level-1 companion, unemp, which was expressed by \\(\\gamma_{20}\\).\nDepressing as these results are, the practice was still worthwhile. Had we not decomposed our time-varying unemp variable into its within- and between-level components, we would never had known that the trait levels of umemp were inconsequential for these analyses. Now we know. For these models, all the action for unemp was at the within-person level.\nThis is also the explanation for why we focused on the \\(\\gamma\\)s to the neglect of the variance parameters. Because our unemp_id_mu variables were poor predictors of the random effects, there was no reason to expect they’d differ meaningfully across models. And because unemp_id_mu is only a level-2 predictor, it never had any hope for changing the estimates for \\(\\sigma_\\epsilon\\).\n What about centering umemp? If you look through our primary two references for this post, Enders \u0026amp; Tofighi (2007) and Hoffman (2015), you’ll see both works spend a lot of time on discussing how one might center the level-1 versions of the time-varying covariates. If unemp was a continuous variable, we would have had to contend with that issue, too. But this just isn’t necessary with binary variables. They have a sensible interpretation when left in the typical 0/1 format. So my recommendation is when you’re decomposing your binary time-varying covariates, put your focus on meaningfully centering the level-2 version of the variable. Leave the level-1 version alone. However, if you’re really interested in playing around with alternatives like effects coding, Enders and Tofighi provided several recommendations.\n  Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.12.0 Rcpp_1.0.3 forcats_0.4.0 stringr_1.4.0 ## [5] dplyr_0.8.4 purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 ## [9] tibble_2.1.3 ggplot2_3.2.1 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.2 rsconnect_0.8.16 ## [4] markdown_1.1 base64enc_0.1-3 fs_1.3.1 ## [7] rstudioapi_0.10 farver_2.0.3 rstan_2.19.2 ## [10] DT_0.11 fansi_0.4.1 mvtnorm_1.0-12 ## [13] lubridate_1.7.4 xml2_1.2.2 bridgesampling_0.8-1 ## [16] knitr_1.26 shinythemes_1.1.2 bayesplot_1.7.1 ## [19] jsonlite_1.6.1 broom_0.5.3 dbplyr_1.4.2 ## [22] shiny_1.4.0 compiler_3.6.2 httr_1.4.1 ## [25] backports_1.1.5 assertthat_0.2.1 Matrix_1.2-18 ## [28] fastmap_1.0.1 lazyeval_0.2.2 cli_2.0.1 ## [31] later_1.0.0 prettyunits_1.1.1 htmltools_0.4.0 ## [34] tools_3.6.2 igraph_1.2.4.2 coda_0.19-3 ## [37] gtable_0.3.0 glue_1.3.1 reshape2_1.4.3 ## [40] cellranger_1.1.0 vctrs_0.2.2 nlme_3.1-142 ## [43] blogdown_0.17 crosstalk_1.0.0 xfun_0.12 ## [46] ps_1.3.0 rvest_0.3.5 mime_0.8 ## [49] miniUI_0.1.1.1 lifecycle_0.1.0 gtools_3.8.1 ## [52] zoo_1.8-7 scales_1.1.0 colourpicker_1.0 ## [55] hms_0.5.3 promises_1.1.0 Brobdingnag_1.2-6 ## [58] parallel_3.6.2 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.1 curl_4.3 gridExtra_2.3 ## [64] StanHeaders_2.19.0 loo_2.2.0 stringi_1.4.6 ## [67] dygraphs_1.1.1.6 pkgbuild_1.0.6 rlang_0.4.5 ## [70] pkgconfig_2.0.3 matrixStats_0.55.0 evaluate_0.14 ## [73] lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 ## [76] labeling_0.3 tidyselect_1.0.0 processx_3.4.1 ## [79] plyr_1.8.5 magrittr_1.5 bookdown_0.17 ## [82] R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [85] pillar_1.4.3 haven_2.2.0 withr_2.1.2 ## [88] xts_0.12-0 abind_1.4-5 modelr_0.1.5 ## [91] crayon_1.3.4 utf8_1.1.4 rmarkdown_2.0 ## [94] emo_0.0.0.9000 grid_3.6.2 readxl_1.3.1 ## [97] callr_3.4.1 threejs_0.3.3 reprex_0.3.0 ## [100] digest_0.6.23 xtable_1.8-4 httpuv_1.5.2 ## [103] stats4_3.6.2 munsell_0.5.0 shinyjs_1.1 d \u0026lt;- d %\u0026gt;% mutate(unemp_cwc = unemp - unemp_id_mu) head(d) ## # A tibble: 6 x 7 ## id unemp_id_mu months cesd unemp unemp_id_mu_s unemp_cwc ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 103 1 1.15 25 1 0.873 0 ## 2 103 1 5.95 16 1 0.873 0 ## 3 103 1 12.9 33 1 0.873 0 ## 4 641 0.333 0.789 27 1 -1.58 0.667 ## 5 641 0.333 4.86 7 0 -1.58 -0.333 ## 6 641 0.333 11.8 25 0 -1.58 -0.333 d %\u0026gt;% distinct(unemp_cwc) %\u0026gt;% arrange(unemp_cwc) ## # A tibble: 7 x 1 ## unemp_cwc ## \u0026lt;dbl\u0026gt; ## 1 -0.667 ## 2 -0.5 ## 3 -0.333 ## 4 0 ## 5 0.333 ## 6 0.5 ## 7 0.667 d \u0026lt;- d %\u0026gt;% mutate(unemp_cwc = unemp - unemp_id_mu) fit4 \u0026lt;- brm(data = d, family = gaussian, cesd ~ 0 + intercept + months + unemp_cwc + unemp_id_mu + unemp_id_mu:months + (1 + months | id), prior = c(prior(normal(14.5, 20), class = b, coef = \u0026quot;intercept\u0026quot;), prior(normal(0, 10), class = b), prior(student_t(3, 0, 10), class = sd), prior(student_t(3, 0, 10), class = sigma), prior(lkj(4), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5) posterior_summary(fit2)[1:5, ] %\u0026gt;% round(2) ## Estimate Est.Error Q2.5 Q97.5 ## b_intercept 12.48 2.42 7.71 17.23 ## b_months -0.15 0.29 -0.73 0.39 ## b_unemp 5.13 1.30 2.62 7.68 ## b_unemp_id_mu 0.24 2.70 -5.21 5.44 ## b_months:unemp_id_mu -0.07 0.34 -0.72 0.61 posterior_summary(fit4)[1:5, ] %\u0026gt;% round(2) ## Estimate Est.Error Q2.5 Q97.5 ## b_intercept 12.84 2.31 8.41 17.45 ## b_months -0.19 0.28 -0.73 0.36 ## b_unemp_cwc 5.00 1.31 2.42 7.54 ## b_unemp_id_mu 4.93 2.77 -0.58 10.25 ## b_months:unemp_id_mu -0.02 0.33 -0.67 0.62 d %\u0026gt;% group_by(id) %\u0026gt;% summarise(min = min(unemp_cwc), max = max(unemp_cwc)) %\u0026gt;% mutate(dif = max - min) %\u0026gt;% count(dif) ## # A tibble: 2 x 2 ## dif n ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 0 132 ## 2 1 122 w\n ","date":1572480000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572480000,"objectID":"08a2a9672889b7cae37657afd2878d57","permalink":"/post/time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/","publishdate":"2019-10-31T00:00:00Z","relpermalink":"/post/time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/","section":"post","summary":"tl;dr When you have a time-varying covariate you’d like to add to a multilevel growth model, it’s important to break that variable into two. One part of the variable will account for within-person variation. The other part will account for between person variation. Keep reading to learn how you might do so when your time-varying covariate is binary.\n I assume things. For this post, I’m presuming you are familiar with longitudinal multilevel models and vaguely familiar with the basic differences between frequentist and Bayesian statistics.","tags":["Bayesian","brms","multilevel","R","tutorial"],"title":"Time-varying covariates in longitudinal multilevel models contain state- and trait-level information: This includes binary variables, too","type":"post"},{"authors":[],"categories":[],"content":" tl;dr When people conclude results from group-level data will tell you about individual-level processes, they commit the ecological fallacy. This is true even of the individuals whose data contributed to those group-level results. This phenomenon can seem odd and counterintuitive. Keep reading to improve your intuition.\n We need history. The ecological fallacy is closely related to Simpson’s paradox 1. It is often attributed to sociologist William S. Robinson’s (1950) paper Ecological Correlations and the Behavior of Individuals. My fellow psychologists might be happy to learn the idea goes back at least as far as E. L. Thorndike’s (1939) paper, On the fallacy of imputing the correlations found for groups to the individuals or smaller groups composing them. Yet as far as I can tell, the term “ecological fallacy” itself first appeared in sociologist Hanan C. Selvin’s (1958) paper, Durkheim’s suicide and problems of empirical research.\nThe central point of Robinson’s seminal paper can be summed up in this quote: “There need be no correspondence between the individual correlation and the ecological correlation” (p. 339, emphasis added) 2. Both Robinson and Thorndike framed their arguments in terms of correlations. In more general and contemporary terms, their insight was that results from between-person analyses will not necessarily match up with results from within-person analyses. When we assume the results from a between-person analysis will tell us about within-person processes, we commit the ecological fallacy.\n An example might help. Though Thorndike, Robinson, and Selvin all worked through examples of the ecological fallacy, I’m a fan of contemporary methodologist Ellen L. Hamaker’s way of explaining it. We’ll quote from her (2012) chapter, Why researchers should think “within-person”: A paradigmatic rationale, in bulk:\n Suppose we are interested in the relationship between typing speed (i.e., number of words typed per minute) and the percentage of typos that are made. If we look at the cross-sectional relationship (i.e., the population level), we may well find a negative relationship, in that people who type faster make fewer mistakes (this may be reflective of the fact that people with better developed typing skills and more experience both type faster and make fewer mistakes). (p. 44)\n Based on a figure in Hamaker’s chapter, that cross-sectional relationship might look something like this.\nlibrary(tidyverse) d %\u0026gt;% filter(j == 1) %\u0026gt;% ggplot(aes(x = x, y = y)) + geom_point(alpha = 2/3) + stat_ellipse(size = 1/4) + scale_x_continuous(\u0026quot;typing speed\u0026quot;, breaks = NULL, limits = c(-3, 3)) + scale_y_continuous(\u0026quot;number of typos\u0026quot;, breaks = NULL, limits = c(-3, 3)) + coord_equal() + theme(panel.grid = element_blank()) Since these data were from one time point, they only allow us to make a between-person analysis.\n[I’m not going to show you how I made these data just yet. It’d break up the flow. For the curious, the statistical formula and code are at the end of the post. I should point out, though, that the scale is arbitrary.]\nAnyway, here’s that cross-sectional correlation.\nd %\u0026gt;% filter(j == 1) %\u0026gt;% summarise(correlation_between_participants = cor(x, y)) ## # A tibble: 1 x 1 ## correlation_between_participants ## \u0026lt;dbl\u0026gt; ## 1 -0.696 Like Hamaker suggested, it’s large and negative. Hamaker continued:\n If we were to generalize this result to the within-person level, we would conclude that if a particular person types faster, he or she will make fewer mistakes. Clearly, this is not what we expect: In fact, we are fairly certain that for any particular individual, the number of typos will increase if he or she tries to type faster. This implies a positive–rather than a negative–relationship at the within-person level. (p. 44)\n The simulated data I just showed were from 50 participants at one measurement occasion. However, the full data set contains 50 measurement occasions for each of the 50 participants. In the next plot, we’ll show all 50 measurement occasions for just 5 participants. The \\(n\\) is reduced in the plot to avoid cluttering.\nd %\u0026gt;% filter(i %in% c(1, 10, 25, 47, 50)) %\u0026gt;% mutate(i = factor(i)) %\u0026gt;% ggplot() + geom_point(aes(x = x, y = y, color = i), size = 1/3) + stat_ellipse(aes(x = x, y = y, color = i), size = 1/5) + geom_point(data = d %\u0026gt;% filter(j == 1 \u0026amp; i %in% c(1, 10, 25, 47, 50)), aes(x = x0, y = y0), size = 2, color = \u0026quot;grey50\u0026quot;) + scale_color_viridis_d(option = \u0026quot;B\u0026quot;, begin = .25, end = .85) + scale_x_continuous(\u0026quot;typing speed\u0026quot;, breaks = NULL, limits = c(-3, 3)) + scale_y_continuous(\u0026quot;number of typos\u0026quot;, breaks = NULL, limits = c(-3, 3)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = \u0026quot;none\u0026quot;) The points are colored by participant. The gray points in each data cloud are the participant-level means. Although we still see a clear negative relationship between participants, we now also see a mild positive relationship within participants. If we compute the correlation separately for each of our 50 participants, we can summarize the results in a histogram.\nd %\u0026gt;% group_by(i) %\u0026gt;% summarise(r = cor(x, y) %\u0026gt;% round(digits = 2)) %\u0026gt;% ggplot(aes(x = r)) + geom_histogram(binwidth = .1) + xlab(\u0026quot;correlations within participants\u0026quot;) + theme(panel.grid = element_blank()) In this case, the within-person correlations all clustered together around .3. That won’t necessarily be the case in other contexts.\nHopefully this gives you a sense of how meaningless a question like What is the correlation between typing speed and typo rates? is. The question is poorly specified because it makes no distinction between the between- and within-person frameworks. As it turns out, the answer could well be different depending on which one you care about and which one you end up studying. I suspect poorly-specified questions of this kind are scattered throughout the literature. For example, I’m a clinical psychologist. Have you ever heard a clinical psychologist talk about how highly anxiety is correlated with depression? And yet much of that evidence is from cross-sectional data (e.g., correlations between the anxiety and depression subscales of the DASS; Lovibond \u0026amp; Lovibond, 1995). But what about within specific people? Do you really believe anxiety and depression are highly-correlated in all people? Maybe. But more cross-sectional analyses will not answer that question.\n Robinson finished with a bang. Our typing data are just one example of how between- and within-person analyses can differ. To see Hamaker walk out the example herself, check out her talk on the subject from a few years ago. I think you’ll find her an engaging speaker. But to sum this topic up, it’s worth considering the Conclusion section from Robinson’s (1950) paper in full:\n The relation between ecological and individual correlations which is discussed in this paper provides a definite answer as to whether ecological correlations can validly be used as substitutes for individual correlations. They cannot. While it is theoretically possible for the two to be equal, the conditions under which this can happen are far removed from those ordinarily encountered in data. From a practical standpoint, therefore, the only reasonable assumption is that an ecological correlation is almost certainly not equal to its corresponding individual correlation.\nI am aware that this conclusion has serious consequences, and that its effect appears wholly negative because it throws serious doubt upon the validity of a number of important studies made in recent years. The purpose of this paper will have been accomplished, however, if it prevents the future computation of meaningless correlations and stimulates the study of similar problems with the use of meaningful correlations between the properties of individuals. (pp. 340–341)\n  Regroup and look ahead. Let’s review what we’ve covered so far. With Simpson’s paradox, we learned that the apparent association between two variables can be attenuated after conditioning on a relevant third variable. In the literature, that third variable is often a grouping variable like gender or college department.\nThe ecological fallacy demonstrated something similar, but from a different angle. That literature showed us that the results from a between-person analysis will not necessarily inform us of within-person processes. The converse is true, too. Indeed, the ecological fallacy is something of a special case of Simpson’s paradox. With the ecological fallacy, the grouping variable is participant id which, when accounted for, yields a different level of analysis 3.\nWith both validity threats, the results of a given analysis can attenuate, go to zero, or even switch sign. With the Berkeley example in the previous post, the relation went to zero. With our simulated data inspired by Hamaker’s example, the effect size went from a large negative cross-sectional correlation to a bundle of small/medium positive within-person correlations. These are non-trivial changes.\nIf at this point you find yourself fatigued and your head hurts a little, don’t worry. You’re probably normal. In a series of experiments, Fiedler, Walther, Freytag, and Nickel (2003) showed it’s quite normal to struggle with these concepts. For more practice, check out Kievit, Frankenhuis, Waldorp, and Borsboom’s nice (2013) paper, Simpson’s paradox in psychological science: a practical guide. Kuppens and Pollet (2014) covered more examples of the ecological fallacy in their Mind the level: problems with two recent nation-level analyses in psychology. I’ve also worked out and posted the example of the ecological fallacy from Thorndike’s (1939) paper, which you can find here.\nIn the next post, we’ll continue developing this material with a discussion of traits versus states.\n Afterward: How might one simulate those typing speed data? In those simulated data, we generically named the “typing speed” variable x and the “error count” variable y. If you let the index \\(i\\) stand for the \\(i^\\text{th}\\) case and \\(j\\) stand for the \\(j^\\text{th}\\) measurement occasion, the data-generating formula is as follows:\n\\[\\begin{align*} \\text x_{ij} \u0026amp; = \\gamma_{00}^\\text x + \\zeta_{0i}^\\text x + \\epsilon_{ij}^\\text x \\\\ \\text y_{ij} \u0026amp; = \\gamma_{00}^\\text y + \\zeta_{0i}^\\text y + \\epsilon_{ij}^\\text y, \\text{where} \\\\ \\begin{bmatrix} \\zeta_{0_i}^\\text x \\\\ \\zeta_{0_i}^\\text y \\end{bmatrix} \u0026amp; \\sim \\text{MVNormal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 \u0026amp; -0.8 \\\\ -0.8 \u0026amp; 1 \\end{bmatrix} \\end{pmatrix} \\text{and} \\\\ \\begin{bmatrix} \\epsilon_{ij}^\\text x \\\\ \\epsilon_{ij}^\\text y \\end{bmatrix} \u0026amp; \\sim \\text{MVNormal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0.625 \u0026amp; 0.019 \\\\ 0.019 \u0026amp; 0.625 \\end{bmatrix} \\end{pmatrix}. \\end{align*}\\]\nBecause the variances for the \\(\\zeta\\)s were 1, that put them in a standardized metric. As such \\(-0.8\\) is both a covariance and a correlation. I came to the values in the variance/covariance matrix for the \\(\\epsilon\\)s by trial and error, which I cover in more detail, below. In words, this model is a bivariate intercepts-only multilevel model. For an introduction to these kinds of models, check out Baldwin, Imel, Braithwaite, and Atkins (2014).\nThe approach I used to generate the data is an extension of the one I used in chapter 13 of my project recoding McElreath’s (2015) text. You can find that code, here. There are two big changes to the original. First, setting the random effects for the mean structure to a \\(z\\)-score metric simplified that part of the code quite a bit. Second, I defined the residuals, which were correlated, in a separate data object from the one containing the mean structure. In the final step, we combined the two and simulated the x and y values.\nFirst, define the mean structure.\nn \u0026lt;- 50 # choose the n x0 \u0026lt;- 0 # population mean for x y0 \u0026lt;- 0 # population mean for x v_x0 \u0026lt;- 1 # variance around x v_y0 \u0026lt;- 1 # variance around y cov \u0026lt;- -.8 # covariance for the variances # the next three lines of code simply combine the terms, above mu \u0026lt;- c(x0, y0) sigma \u0026lt;- matrix(c(v_x0, cov, cov, v_y0), ncol = 2) set.seed(1) m \u0026lt;- MASS::mvrnorm(n, mu, sigma) %\u0026gt;% data.frame() %\u0026gt;% set_names(\u0026quot;x0\u0026quot;, \u0026quot;y0\u0026quot;) %\u0026gt;% arrange(x0) %\u0026gt;% mutate(i = 1:n) %\u0026gt;% expand(nesting(i, x0, y0), j = 1:n) Second, define the residual structure.\n# note how these values initially place the epsilons in a standardized metric sigma \u0026lt;- matrix(c(v_x0, .3, .3, v_y0), ncol = 2) set.seed(1) r \u0026lt;- MASS::mvrnorm(n * n, mu, sigma) %\u0026gt;% data.frame() %\u0026gt;% set_names(\u0026quot;e_x\u0026quot;, \u0026quot;e_y\u0026quot;) %\u0026gt;% # you do not need this step. # it\u0026#39;s something I experimented with to rescale # the residual variances to a workable level for the plots. mutate_all(.funs = ~. * .25) Combine the two data structures and save.\nd \u0026lt;- bind_cols(m, r) %\u0026gt;% mutate(x = x0 + e_x, y = y0 + e_y)  Because we used the set.seed() function before each simulation step, you will be able to reproduce the results exactly.\n Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 purrr_0.3.3 ## [5] readr_1.3.1 tidyr_1.0.2 tibble_2.1.3 ggplot2_3.2.1 ## [9] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.0.0 xfun_0.12 haven_2.2.0 lattice_0.20-38 ## [5] colorspace_1.4-1 vctrs_0.2.2 generics_0.0.2 viridisLite_0.3.0 ## [9] htmltools_0.4.0 yaml_2.2.1 utf8_1.1.4 rlang_0.4.5 ## [13] pillar_1.4.3 withr_2.1.2 glue_1.3.1 DBI_1.1.0 ## [17] dbplyr_1.4.2 modelr_0.1.5 readxl_1.3.1 lifecycle_0.1.0 ## [21] munsell_0.5.0 blogdown_0.17 gtable_0.3.0 cellranger_1.1.0 ## [25] rvest_0.3.5 evaluate_0.14 labeling_0.3 knitr_1.26 ## [29] fansi_0.4.1 broom_0.5.3 Rcpp_1.0.3 backports_1.1.5 ## [33] scales_1.1.0 jsonlite_1.6.1 farver_2.0.3 fs_1.3.1 ## [37] hms_0.5.3 digest_0.6.23 stringi_1.4.6 bookdown_0.17 ## [41] grid_3.6.2 cli_2.0.1 tools_3.6.2 magrittr_1.5 ## [45] lazyeval_0.2.2 crayon_1.3.4 pkgconfig_2.0.3 MASS_7.3-51.4 ## [49] xml2_1.2.2 reprex_0.3.0 lubridate_1.7.4 assertthat_0.2.1 ## [53] rmarkdown_2.0 httr_1.4.1 rstudioapi_0.10 R6_2.4.1 ## [57] nlme_3.1-142 compiler_3.6.2  Footnotes   Do you need a refresher on Simpson’s paradox? Click here.↩\n The page numbers in this section might could use some clarifications. It’s a bit of a pain to locate a PDF of Robinson’s original 1950 paper. If you do a casual online search, it’s more likely you’ll come across this 2009 reprint of the paper. To the best of my knowledge, the reprint is faithful. For all the Robinson quotes in this post, the page numbers are based on the 2009 reprint.↩\n Some readers may see the multilevel model lurking in the shadows, here. You’re right. When we think of the relationship between Simpson’s paradox and the ecological fallacy, I find it particularly instructive to recall how the multilevel model can be thought of as a high-rent interaction model. I’m not making that point directly in the prose, yet, for the sake of keeping the content more general and conceptual. But yes, the multilevel model will move out of the shadows into the light of day as we press forward in this series.↩\n   ","date":1571011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571011200,"objectID":"7a902cb38f93b9503682a67f6f83c55f","permalink":"/post/individuals-are-not-small-groups-ii-the-ecological-fallacy/","publishdate":"2019-10-14T00:00:00Z","relpermalink":"/post/individuals-are-not-small-groups-ii-the-ecological-fallacy/","section":"post","summary":"tl;dr When people conclude results from group-level data will tell you about individual-level processes, they commit the ecological fallacy. This is true even of the individuals whose data contributed to those group-level results. This phenomenon can seem odd and counterintuitive. Keep reading to improve your intuition.\n We need history. The ecological fallacy is closely related to Simpson’s paradox 1. It is often attributed to sociologist William S. Robinson’s (1950) paper Ecological Correlations and the Behavior of Individuals.","tags":["dynamic p-technique","idiographic","R","single-case","tutorial"],"title":"Individuals are not small groups, II: The ecological fallacy","type":"post"},{"authors":[],"categories":[],"content":" tl;dr If you are under the impression group-level data and group-based data analysis will inform you about within-person processes, you would be wrong. Stick around to learn why.\n This is gonna be a long car ride. Earlier this year I published a tutorial 1 on a statistical technique that will allow you to analyze the multivariate time series data of a single individual. It’s called the dynamic p-technique. The method has been around since at least the 80s (Molenaar, 1985) and its precursors date back to at least the 40s (Cattell, Cattell, \u0026amp; Rhymer, 1947). In the age where it’s increasingly cheap and easy to collect data from large groups, on both one measurement occasion or over many, you might wonder why you should learn about a single-case statistical technique. Isn’t such a thing unneeded?\nNo. It is indeed needed. Unfortunately for me, the reasons we need it aren’t intuitive or well understood. Luckily for us all, I’m a patient man. We’ll be covering the reasons step by step. Once we’re done covering reasons, we’ll switch into full-blown tutorial mode. In this first blog on the topic, we’ll cover reason #1: Simpson’s paradox is a thing and it’ll bite you hard it you’re not looking for it.\n Simpson’s paradox Simpson’s paradox officially made its way into the literature in this 1951 paper by Simpson. Rather than define the paradox outright, I’m going to demonstrate it with a classic example. The data come from the 1973 University of California, Berkeley, graduate admissions. Based on a simple breakdown of the admission rates, 44% of the men who applied were admitted. In contrast, only 35% of the women who applied were admitted. The university was accused of sexism and the issue made its way into the courts.\nHowever, when statisticians looked more closely at the data, it became apparent those data were not the compelling evidence of sexism they were initially made out to be. To see why, we’ll want to get into the data, ourselves. The admissions rates for the six largest departments have made their way into the peer-reviewed literature (Bickel, Hammel, \u0026amp; O’Connell, 1975), into many textbooks (e.g., Danielle Navarro’s Learning statistics with R), and are available in R as the built-in data set UCBAdmissions. Here we’ll call them, convert the data into a tidy format 2, and add a variable.\nlibrary(tidyverse) d \u0026lt;- UCBAdmissions %\u0026gt;% as_tibble() %\u0026gt;% pivot_wider(id_cols = c(Dept, Gender), names_from = Admit, values_from = n) %\u0026gt;% mutate(total = Admitted + Rejected) head(d) ## # A tibble: 6 x 5 ## Dept Gender Admitted Rejected total ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 A Male 512 313 825 ## 2 A Female 89 19 108 ## 3 B Male 353 207 560 ## 4 B Female 17 8 25 ## 5 C Male 120 205 325 ## 6 C Female 202 391 593 The identities of the departments have been anonymized, so we’re stuck with referring to them as A through F. Much like with the overall rates for graduate admissions, it appears that the admission rates for the six anonymized departments in the UCBAdmissions data show higher a admission rate for men.\nd %\u0026gt;% group_by(Gender) %\u0026gt;% summarise(percent_admitted = (100 * sum(Admitted) / sum(total)) %\u0026gt;% round(digits = 1)) ## # A tibble: 2 x 2 ## Gender percent_admitted ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Female 30.4 ## 2 Male 44.5 A 14% difference seems large enough to justify a stink. However, the plot thickens when we break the data down by department. For that, we’ll make a visual.\nd %\u0026gt;% mutate(dept = str_c(\u0026quot;department \u0026quot;, Dept)) %\u0026gt;% pivot_longer(cols = Admitted:Rejected, names_to = \u0026quot;admit\u0026quot;, values_to = \u0026quot;n\u0026quot;) %\u0026gt;% ggplot(aes(x = Gender, y = n, fill = admit)) + geom_col(position = \u0026quot;dodge\u0026quot;) + scale_fill_viridis_d(NULL, option = \u0026quot;A\u0026quot;, end = .6) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~dept) The problem with our initial analysis is it didn’t take into account how different departments might admit men/women at different rates. We also failed to consider whether men and women applied to those different departments at different rates. Take departments A and B. Both admitted the majority of applicants, regardless of gender. Now look at departments E and F. The supermajorities of applicants were rejected, both for men and women Also notice that whereas the departments where the supermajority of applicants were men (i.e., departments A and B) had generous admission rates, the departments with the largest proportion of women applicants (i.e., departments C and E) had rather high rejection rates.\nIt can be hard to juggle all this in your head at once, even with the aid of our figure. Let’s look at the data in a different way. This time we’ll summarize the admission rates in a probability metric where the probability of admission is n / total (i.e., the number of successes divided by the total number of trials). We’ll compute those probabilities while grouping by Gender and Dept.\nd %\u0026gt;% mutate(p = Admitted / total) %\u0026gt;% ggplot(aes(x = Dept, y = p)) + geom_hline(yintercept = .5, color = \u0026quot;white\u0026quot;) + geom_point(aes(color = Gender, size = total), position = position_dodge(width = 0.3)) + scale_color_manual(NULL, values = c(\u0026quot;red3\u0026quot;, \u0026quot;blue3\u0026quot;)) + scale_y_continuous(\u0026quot;admission probability\u0026quot;, limits = 0:1) + xlab(\u0026quot;department\u0026quot;) + theme(panel.grid = element_blank()) Several things pop out. For \\(5/6\\) of the departments (i.e., all but A), the admission probabilities were very similar for men and women–sometimes slightly higher for women, sometimes slightly higher for men. We also see a broad range overall admission rates across departments. Note how the dots are sized based on the total number of applications, by Gender and Dept. Hopefully those sizes help show how women disproportionately applied to departments with low overall admission probabilities. Interestingly, the department with the largest gender bias was A, which showed a bias towards admitting women at higher rates than men.\nLet’s get formal. The paradox Simpson wrote about is that the simple association between two variables can disappear or even change sign when it is conditioned on a relevant third variable. The relevant third variable is typically a grouping variable. In the Berkeley admissions example, the seemingly alarming association between graduate admissions and gender disappeared when conditioned on department. If you’re still jarred by this, Navarro covered this in the opening chapter of her text. Richard McElreath covered it more extensively in chapters 10 and 13 of his (2015) text, Statistical Rethinking. I’ve also worked through a similar example of Simpson’s paradox from the more recent literature, here. Kievit, Frankenhuis, Waldorp, and Borsboom (2013) wrote a fine primer on the topic, too.\n Wait. What? At this point you might be wondering what this has to do with the difference between groups and individuals. We’re slowly building a case step by step, remember? For this first installment, just notice how a simple bivariate analysis fell apart once we took an important third variable into account. In this case and in many others, it so happened that third variable was a grouping variable.\nStay tuned for the next post where well build on this with a related phenomenon: the ecological fallacy.\n Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 purrr_0.3.3 ## [5] readr_1.3.1 tidyr_1.0.2 tibble_2.1.3 ggplot2_3.2.1 ## [9] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.0.0 xfun_0.12 haven_2.2.0 lattice_0.20-38 ## [5] colorspace_1.4-1 vctrs_0.2.2 generics_0.0.2 viridisLite_0.3.0 ## [9] htmltools_0.4.0 yaml_2.2.1 utf8_1.1.4 rlang_0.4.5 ## [13] pillar_1.4.3 withr_2.1.2 glue_1.3.1 DBI_1.1.0 ## [17] dbplyr_1.4.2 modelr_0.1.5 readxl_1.3.1 lifecycle_0.1.0 ## [21] munsell_0.5.0 blogdown_0.17 gtable_0.3.0 cellranger_1.1.0 ## [25] rvest_0.3.5 evaluate_0.14 labeling_0.3 knitr_1.26 ## [29] fansi_0.4.1 broom_0.5.3 Rcpp_1.0.3 backports_1.1.5 ## [33] scales_1.1.0 jsonlite_1.6.1 farver_2.0.3 fs_1.3.1 ## [37] hms_0.5.3 digest_0.6.23 stringi_1.4.6 bookdown_0.17 ## [41] grid_3.6.2 cli_2.0.1 tools_3.6.2 magrittr_1.5 ## [45] lazyeval_0.2.2 crayon_1.3.4 pkgconfig_2.0.3 xml2_1.2.2 ## [49] reprex_0.3.0 lubridate_1.7.4 assertthat_0.2.1 rmarkdown_2.0 ## [53] httr_1.4.1 rstudioapi_0.10 R6_2.4.1 nlme_3.1-142 ## [57] compiler_3.6.2  Footnotes   You can find the preprint and supporting documents, including the data and code, here.↩\n Walking out the definition of tidy data is beyond the scope of this post. It’s connected to the work of data scientist Hadley Wickham, in particular, and the ethos behind the collection of R packages called the tidyverse, more generally. My R code tends to follow the tidyverse style. If you’re new these ideas, it’ll help if you familiarize yourself with them a bit. For an introduction to the notion of tidy data, Wickham’s recent talk, Data visualization and data science, is a fine place to start.↩\n   ","date":1570579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570579200,"objectID":"20478d9b6979cf1a34bcdddd459e9d04","permalink":"/post/individuals-are-not-small-groups-i-simpson-s-paradox/","publishdate":"2019-10-09T00:00:00Z","relpermalink":"/post/individuals-are-not-small-groups-i-simpson-s-paradox/","section":"post","summary":"tl;dr If you are under the impression group-level data and group-based data analysis will inform you about within-person processes, you would be wrong. Stick around to learn why.\n This is gonna be a long car ride. Earlier this year I published a tutorial 1 on a statistical technique that will allow you to analyze the multivariate time series data of a single individual. It’s called the dynamic p-technique. The method has been around since at least the 80s (Molenaar, 1985) and its precursors date back to at least the 40s (Cattell, Cattell, \u0026amp; Rhymer, 1947).","tags":["dynamic p-technique","idiographic","R","single-case","tutorial"],"title":"Individuals are not small groups, I: Simpson's paradox","type":"post"},{"authors":[],"categories":[],"content":" Hit the pause I started this series intending to add semiregular installments. Perhaps I’d add one every other month or so. It turns out I’m just not up to it, right now.\nSomeone important to me died during this plague. Reading and blogging on this book was and has been a way to connect with them—to honor their life. The experience has been rich and raw and meaningful. I hope that not too far off in the future, I’ll be in a better place to take up this task. Until then, take care of you and, to the extent you can bear, look upon those around you with kindness in your heart.\n ","date":1569801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569801600,"objectID":"89c9681631d4e37ff40263abb6fdcc89","permalink":"/post/how-to-survive-a-plague-part-2-n-of-a-premature-book-report/","publishdate":"2019-09-30T00:00:00Z","relpermalink":"/post/how-to-survive-a-plague-part-2-n-of-a-premature-book-report/","section":"post","summary":"Hit the pause I started this series intending to add semiregular installments. Perhaps I’d add one every other month or so. It turns out I’m just not up to it, right now.\nSomeone important to me died during this plague. Reading and blogging on this book was and has been a way to connect with them—to honor their life. The experience has been rich and raw and meaningful. I hope that not too far off in the future, I’ll be in a better place to take up this task.","tags":["HIV/AIDS"],"title":"\"How to Survive a Plague\": Part 2/$n$ of a premature book report","type":"post"},{"authors":[],"categories":[],"content":" Version 1.0.0 In the last post, we covered how the Poisson distribution is handy for modeling count data. Binary data are even weirder than counts. They typically only take on two values: 0 and 1. Sometimes 0 is a stand-in for “no” and 1 for “yes” (e.g., Are you an expert in Bayesian power analysis? For me that would be 0). You can also have data of this kind if you asked people whether they’d like to choose option A or B. With those kinds of data, you might arbitrarily code A as 0 and B as 1. Binary data also often stand in for trials where 0 = “fail” and 1 = “success.” For example, if you answered “Yes” to the question Are all data normally distributed? we’d mark your answer down as a 0.\nThough 0s and 1s are popular, sometimes binary data appear in their aggregated form. Let’s say I gave you 10 algebra questions and you got 7 of them right. Here’s one way to encode those data.\nn \u0026lt;- 10 z \u0026lt;- 7 rep(0:1, times = c(n - z, z)) ## [1] 0 0 0 1 1 1 1 1 1 1 In that example, n stood for the total number of trials and z was the number you got correct (i.e., the number of times we encoded your response as a 1). A more compact way to encode that data is with two columns, one for z and the other for n.\nlibrary(tidyverse) tibble(z = z, n = n) ## # A tibble: 1 x 2 ## z n ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 7 10 So then if you gave those same 10 questions to four of your friends, we could encode the results like this.\nset.seed(3) tibble(id = letters[1:5], z = rpois(n = 5, lambda = 5), n = n) ## # A tibble: 5 x 3 ## id z n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 a 3 10 ## 2 b 7 10 ## 3 c 4 10 ## 4 d 4 10 ## 5 e 5 10 If you were b, you’d be the smart one in the group.\nAnyway, whether working with binary or aggregated binary data, we’re interested in the probability a given trial will be 1.\n Logistic regression with unaggregated binary data Taking unaggregated binary data as a starting point, given \\(d\\) data that includes a variable \\(y\\) where the value in the \\(i^\\text{th}\\) row is a 0 or a 1, we’d like to know the probability a given trial would be 1, given \\(d\\) [i.e., \\(p(y_i = 1 | d)\\)]. The binomial distribution will help us get that estimate for \\(p\\). We’ll do so within the context of a logistic regression model following the form\n\\[ \\begin{align*} y_i \u0026amp; \\sim \\text{Binomial} (n = 1, p_i) \\\\ \\operatorname{logit} (p_i) \u0026amp; = \\beta_0, \\end{align*} \\]\nwere the logit function is defined as the log odds\n\\[ \\operatorname{logit} (p_i) = \\operatorname{log} \\bigg (\\frac{p_i}{1 - p_i} \\bigg), \\]\nwhich also means that\n\\[ \\operatorname{log} \\bigg (\\frac{p_i}{1 - p_i} \\bigg) = \\beta_0. \\]\nIn those formulas, \\(\\beta_0\\) is the intercept. In a binomial model with no predictors 1, the intercept \\(\\beta_0\\) is just the estimate for \\(p\\), but in the log-odds metric. So yes, similar to the Poisson models from the last post, we typically use a link function with our binomial models. Instead of the log link, we use the logit because it constrains the posterior for \\(p\\) to values between 0 and 1. Just as the null value for a probability is .5, the null value for the parameters within a logistic regression model is typically 0.\nAs with the Poisson, I’m not going to go into a full-blown tutorial on the binomial distribution or on logistic regression. For more thorough introductions, check out chapters 9 through 10 in McElreath’s Statistical Rethinking or Agresti’s Foundations of Linear and Generalized Linear Models.\nWe need data. Time to simulate some data. Let’s say we’d like to estimate the probability someone will hit a ball in a baseball game. Nowadays, batting averages for professional baseball players tend around .25 (see here). So if we wanted to simulate 50 at-bats, we might do so like this.\nset.seed(3) d \u0026lt;- tibble(y = rbinom(n = 50, size = 1, prob = .25)) str(d) ## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 50 obs. of 1 variable: ## $ y: int 0 1 0 0 0 0 0 0 0 0 ... Here are what those data look like in a bar plot.\ntheme_set(theme_gray() + theme(panel.grid = element_blank())) d %\u0026gt;% mutate(y = factor(y)) %\u0026gt;% ggplot(aes(x = y)) + geom_bar()  Time to model. To practice modeling those data, we’ll want to fire up the brms package.\nlibrary(brms) We can use the get_prior() function to get the brms default for our intercept-only logistic regression model.\nget_prior(data = d, family = binomial, y | trials(1) ~ 1) ## Intercept ~ student_t(3, 0, 10) As it turns out, that’s a really liberal prior. We might step up a bit and put a more skeptical normal(0, 2) prior on that intercept. With the context of our logit link, that still puts a 95% probability that the \\(p\\) is between .02 and .98, which is almost the entire parameter space. Here’s how to fit the model with the brm() function.\nfit1 \u0026lt;- brm(data = d, family = binomial, y | trials(1) ~ 1, prior(normal(0, 2), class = Intercept), seed = 3) In the brm() formula syntax, including a | bar on the left side of a formula indicates we have extra supplementary information about our criterion variable. In this case, that information is that each y value corresponds to a single trial [i.e., trials(1)], which itself corresponds to the \\(n = 1\\) portion of the statistical formula, above. Here are the results.\nprint(fit1) ## Family: binomial ## Links: mu = logit ## Formula: y | trials(1) ~ 1 ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -1.39 0.36 -2.12 -0.73 1.00 1527 1358 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Remember that that intercept is on the scale of the logit link, the log odds. We can transform it with the brms::inv_logit_scaled() function.\nfixef(fit1)[\u0026quot;Intercept\u0026quot;, 1] %\u0026gt;% inv_logit_scaled() ## [1] 0.1991036 If we’d like to view the full posterior distribution, we’ll need to work with the posterior draws themselves. Then we’ll plot.\n# extract the posterior draws posterior_samples(fit1) %\u0026gt;% # transform from the log-odds to a probability metric transmute(p = inv_logit_scaled(b_Intercept)) %\u0026gt;% # plot! ggplot(aes(x = p)) + geom_density(fill = \u0026quot;grey25\u0026quot;, size = 0) + scale_x_continuous(\u0026quot;probability of a hit\u0026quot;, limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL) Looks like the null hypothesis of \\(p = .5\\) is not credible for this simulation. If we’d like the posterior median and percentile-based 95% intervals, we might use the median_qi() function from the handy tidybayes package.\nlibrary(tidybayes) posterior_samples(fit1) %\u0026gt;% transmute(p = inv_logit_scaled(b_Intercept)) %\u0026gt;% median_qi() ## p .lower .upper .width .point .interval ## 1 0.2011897 0.1076116 0.3252849 0.95 median qi Yep, .5 was not within those intervals.\n But what about power? That’s enough preliminary work. Let’s see what happens when we do a mini power analysis with 100 iterations. First we set up our simulation function using the same methods we introduced in earlier blog posts.\nsim_data_fit \u0026lt;- function(seed, n_player) { n_trials \u0026lt;- 1 prob_hit \u0026lt;- .25 set.seed(seed) d \u0026lt;- tibble(y = rbinom(n = n_player, size = n_trials, prob = prob_hit)) update(fit1, newdata = d, seed = seed) %\u0026gt;% posterior_samples() %\u0026gt;% transmute(p = inv_logit_scaled(b_Intercept)) %\u0026gt;% median_qi() %\u0026gt;% select(.lower:.upper) } Simulate.\nsim1 \u0026lt;- tibble(seed = 1:100) %\u0026gt;% mutate(ci = map(seed, sim_data_fit, n_player = 50)) %\u0026gt;% unnest() You might plot the intervals.\nsim1 %\u0026gt;% ggplot(aes(x = seed, ymin = .lower, ymax = .upper)) + geom_hline(yintercept = c(.25, .5), color = \u0026quot;white\u0026quot;) + geom_linerange() + xlab(\u0026quot;seed (i.e., simulation index)\u0026quot;) + scale_y_continuous(\u0026quot;probability of hitting the ball\u0026quot;, limits = c(0, 1)) Like one of my old coworkers used to say: Purtier ’n a hog! Here we’ll summarize the results both in terms of their conventional power, their mean width, and the proportion of widths more narrow than .25. Why .25? I don’t know. Without a substantively-informed alternative, it’s as good a criterion as any.\nsim1 %\u0026gt;% mutate(width = .upper - .lower) %\u0026gt;% summarise(`conventional power` = mean(.upper \u0026lt; .5), `mean width` = mean(width), `width below .25` = mean(width \u0026lt; .25)) ## # A tibble: 1 x 3 ## `conventional power` `mean width` `width below .25` ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.95 0.231 0.8 Depending on your study needs, you’d adjust your sample size accordingly, do a mini simulation or two first, and then follow up with a proper power simulation with 1000+ iterations.\nI should point out that whereas in the last post we evaluated the power of the Poisson model with the parameters on the scale of the link function, here we evaluated the power for our logistic regression model after transforming the intercept back into the probability metric. Both methods are fine. I recommend you run your power simulation based on how you want to interpret and report your results.\nWe should also acknowledge that this was our first example of a power simulation that wasn’t based on some group comparison. Comparing groups is fine and normal and important. And it’s also the case that we can care about power and/or parameter precision for more than group-based analyses. Our simulation-based approach is fine for both.\n  Aggregated binomial regression It’s no more difficult to simulate and work with aggregated binomial data. But since the mechanics for brms::brm() and thus the down-the-road simulation setup are a little different, we should practice. With our new setup, we’ll consider a new example. Since .25 is the typical batting average, it might better sense to define the null hypothesis like this:\n\\[H_0 \\text{: } p = .25.\\]\nConsider a case where we had some intervention where we expected a new batting average of .35. How many trials would we need, then, to either reject \\(H_0\\) or perhaps estimate \\(p\\) with a satisfactory degree of precision? Here’s what the statistical formula for the implied aggregated binomial model might look like:\n\\[ \\begin{align*} y_i \u0026amp; \\sim \\text{Binomial} (n, p_i) \\\\ \\operatorname{logit} (p_i) \u0026amp; = \\beta_0. \\end{align*} \\]\nThe big change is we no longer defined \\(n\\) as 1. Let’s say we wanted our aggregated binomial data set to contain the summary statistics for \\(n = 100\\) trials. Here’s what that might look like.\nn_trials \u0026lt;- 100 prob_hit \u0026lt;- .35 set.seed(3) d \u0026lt;- tibble(n_trials = n_trials, y = rbinom(n = 1, size = n_trials, prob = prob_hit)) d ## # A tibble: 1 x 2 ## n_trials y ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 100 32 Now we have two columns. The first, n_trials, indicates how many cases or trials we’re summarizing. The second, y, indicates how many successes/1s/hits we might expect given \\(p = .35\\). This is the aggregated binomial equivalent of if we had a 100 row vector composed of 32 1s and 68 0s.\nNow, before we discuss fitting the model with brms, let’s talk priors. Since we’ve updated our definition of \\(H_0\\), it might make sense to update the prior for \\(\\beta_0\\). As it turns out, setting that prior to normal(-1, 0.5) puts the posterior mode at about .25 on the probability space, but with fairly wide 95% intervals ranging from about .12 to .5. Though centered on our updated null value, this prior is still quite permissive given our hypothesized \\(p = .35\\). Let’s give it a whirl.\nTo fit an aggregated binomial model with the brm() function, we augment the \u0026lt;criterion\u0026gt; | trials() syntax where the value that goes in trials() is either a fixed number or variable in the data indexing \\(n\\). Our approach will be the latter.\nfit2 \u0026lt;- brm(data = d, family = binomial, y | trials(n_trials) ~ 1, prior(normal(-1, 0.5), class = Intercept), seed = 3) Inspect the summary.\nprint(fit2) ## Family: binomial ## Links: mu = logit ## Formula: y | trials(n_trials) ~ 1 ## Data: d (Number of observations: 1) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.81 0.20 -1.19 -0.42 1.00 1398 1843 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). After a transformation, here’s what that looks like in a plot.\nposterior_samples(fit2) %\u0026gt;% transmute(p = inv_logit_scaled(b_Intercept)) %\u0026gt;% ggplot(aes(x = p, y = 0)) + geom_halfeyeh(.width = c(.5, .95)) + scale_x_continuous(\u0026quot;probability of a hit\u0026quot;, limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL) Based on a single simulation, it looks like \\(n = 100\\) won’t quite be enough to reject \\(H_0 \\text{: } p = .25\\) with a conventional 2-sided 95% interval. But it does look like we’re in the ballpark and that our basic data + model setup will work for a larger-scale simulation. Here’s an example of how you might update our custom simulation function.\nsim_data_fit \u0026lt;- function(seed, n_trials) { prob_hit \u0026lt;- .35 set.seed(seed) d \u0026lt;- tibble(y = rbinom(n = 1, size = n_trials, prob = prob_hit), n_trials = n_trials) update(fit2, newdata = d, seed = seed) %\u0026gt;% posterior_samples() %\u0026gt;% transmute(p = inv_logit_scaled(b_Intercept)) %\u0026gt;% median_qi() %\u0026gt;% select(.lower:.upper) } Simulate, this time trying out \\(n = 120\\).\nsim2 \u0026lt;- tibble(seed = 1:100) %\u0026gt;% mutate(ci = map(seed, sim_data_fit, n_trials = 120)) %\u0026gt;% unnest() Plot the intervals.\nsim2 %\u0026gt;% ggplot(aes(x = seed, ymin = .lower, ymax = .upper)) + geom_hline(yintercept = c(.25, .35), color = \u0026quot;white\u0026quot;) + geom_linerange() + xlab(\u0026quot;seed (i.e., simulation index)\u0026quot;) + scale_y_continuous(\u0026quot;probability of hitting the ball\u0026quot;, limits = c(0, 1), breaks = c(0, .25, .35, 1)) Overall, those intervals look pretty good. They’re fairly narrow and are hovering around the data generating \\(p = .35\\). But it seems many are still crossing the .25 threshold. Let’s see the results of a formal summary.\nsim2 %\u0026gt;% mutate(width = .upper - .lower) %\u0026gt;% summarise(`conventional power` = mean(.lower \u0026gt; .25), `mean width` = mean(width), `width below .2` = mean(width \u0026lt; .2)) ## # A tibble: 1 x 3 ## `conventional power` `mean width` `width below .2` ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.56 0.155 1 All widths were narrower than .2 and the mean width was about .16. In the abstract that might seem reasonably precise. But we’re still not precise enough to reject \\(H_0\\) with a conventional power level. Depending on your needs, adjust the \\(n\\) accordingly and simulate again.\nNow you’ve got a sense of how to work with the binomial likelihood for (aggregated)binary data, next time we’ll play with Likert-type data.\n Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.1.9000 brms_2.12.0 Rcpp_1.0.3 ## [4] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 ## [7] purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 ## [10] tibble_2.1.3 ggplot2_3.2.1 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.2 ## [3] rsconnect_0.8.16 markdown_1.1 ## [5] base64enc_0.1-3 fs_1.3.1 ## [7] rstudioapi_0.10 farver_2.0.3 ## [9] rstan_2.19.2 svUnit_0.7-12 ## [11] DT_0.11 fansi_0.4.1 ## [13] mvtnorm_1.0-12 lubridate_1.7.4 ## [15] xml2_1.2.2 bridgesampling_0.8-1 ## [17] knitr_1.26 shinythemes_1.1.2 ## [19] bayesplot_1.7.1 jsonlite_1.6.1 ## [21] broom_0.5.3 dbplyr_1.4.2 ## [23] shiny_1.4.0 compiler_3.6.2 ## [25] httr_1.4.1 backports_1.1.5 ## [27] assertthat_0.2.1 Matrix_1.2-18 ## [29] fastmap_1.0.1 lazyeval_0.2.2 ## [31] cli_2.0.1 later_1.0.0 ## [33] htmltools_0.4.0 prettyunits_1.1.1 ## [35] tools_3.6.2 igraph_1.2.4.2 ## [37] coda_0.19-3 gtable_0.3.0 ## [39] glue_1.3.1 reshape2_1.4.3 ## [41] cellranger_1.1.0 vctrs_0.2.2 ## [43] nlme_3.1-142 blogdown_0.17 ## [45] crosstalk_1.0.0 xfun_0.12 ## [47] ps_1.3.0 rvest_0.3.5 ## [49] mime_0.8 miniUI_0.1.1.1 ## [51] lifecycle_0.1.0 gtools_3.8.1 ## [53] zoo_1.8-7 scales_1.1.0 ## [55] colourpicker_1.0 hms_0.5.3 ## [57] promises_1.1.0 Brobdingnag_1.2-6 ## [59] parallel_3.6.2 inline_0.3.15 ## [61] shinystan_2.5.0 yaml_2.2.1 ## [63] gridExtra_2.3 StanHeaders_2.19.0 ## [65] loo_2.2.0 stringi_1.4.6 ## [67] dygraphs_1.1.1.6 pkgbuild_1.0.6 ## [69] rlang_0.4.5 pkgconfig_2.0.3 ## [71] matrixStats_0.55.0 evaluate_0.14 ## [73] lattice_0.20-38 rstantools_2.0.0 ## [75] htmlwidgets_1.5.1 labeling_0.3 ## [77] tidyselect_1.0.0 processx_3.4.1 ## [79] plyr_1.8.5 magrittr_1.5 ## [81] bookdown_0.17 R6_2.4.1 ## [83] generics_0.0.2 DBI_1.1.0 ## [85] pillar_1.4.3 haven_2.2.0 ## [87] withr_2.1.2 xts_0.12-0 ## [89] abind_1.4-5 modelr_0.1.5 ## [91] crayon_1.3.4 arrayhelpers_1.0-20160527 ## [93] utf8_1.1.4 rmarkdown_2.0 ## [95] grid_3.6.2 readxl_1.3.1 ## [97] callr_3.4.1 threejs_0.3.3 ## [99] reprex_0.3.0 digest_0.6.23 ## [101] xtable_1.8-4 httpuv_1.5.2 ## [103] stats4_3.6.2 munsell_0.5.0 ## [105] shinyjs_1.1  Footnote   In case this is all new to you and you and you had the question in your mind: Yes, you can add predictors to the logistic regression model. Say we had a model with two predictors, \\(x_1\\) and \\(x_2\\). Our statistical model would then follow the form \\(\\operatorname{logit} (p_i) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}\\).↩\n   ","date":1566864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566864000,"objectID":"9a0eb8e0b683d1133bd9ae8070435517","permalink":"/post/bayesian-power-analysis-part-iii-b/","publishdate":"2019-08-27T00:00:00Z","relpermalink":"/post/bayesian-power-analysis-part-iii-b/","section":"post","summary":"Version 1.0.0 In the last post, we covered how the Poisson distribution is handy for modeling count data. Binary data are even weirder than counts. They typically only take on two values: 0 and 1. Sometimes 0 is a stand-in for “no” and 1 for “yes” (e.g., Are you an expert in Bayesian power analysis? For me that would be 0). You can also have data of this kind if you asked people whether they’d like to choose option A or B.","tags":["Bayesian","brms","power","R","tidyverse","tutorial"],"title":"Bayesian power analysis: Part III.b. What about 0/1 data?","type":"post"},{"authors":[],"categories":[],"content":" Version 1.0.1  Orientation So far we’ve covered Bayesian power simulations from both a null hypothesis orientation (see part I) and a parameter width perspective (see part II). In both instances, we kept things simple and stayed with Gaussian (i.e., normally distributed) data. But not all data follow that form, so it might do us well to expand our skill set a bit. In the next few posts, we’ll cover how we might perform power simulations with other kinds of data. In this post, we’ll focus on how to use the Poisson likelihood to model counts. In follow-up posts, we’ll explore how to model binary and Likert-type data.\n The Poisson distribution is handy for counts. In the social sciences, count data arise when we ask questions like:\n How many sexual partners have you had? How many pets do you have at home? How many cigarettes did you smoke, yesterday?  The values these data will take are discrete 1 in that you’ve either slept with 9 or 10 people, but definitely not 9.5. The values cannot go below zero in that even if you quit smoking cold turkey 15 years ago and have been a health nut since, you still could not have smoked -3 cigarettes, yesterday. Zero is as low as it goes.\nThe canonical distribution for data of this type–non-negative integers–is the Poisson. It’s named after the French mathematician Siméon Denis Poisson, who had quite the confident stare in his youth. The Poisson distribution has one parameter, \\(\\lambda\\), which controls both its mean and variance. Although the numbers the Poisson describes are counts, the \\(\\lambda\\) parameter does not need to be an integer. For example, here’s the plot of 1,000 draws from a Poisson for which \\(\\lambda = 3.2\\).\nlibrary(tidyverse) theme_set(theme_gray() + theme(panel.grid = element_blank())) tibble(x = rpois(n = 1e3, lambda = 3.2)) %\u0026gt;% mutate(x = factor(x)) %\u0026gt;% ggplot(aes(x = x)) + geom_bar() In case you missed it, the key function for generating those data was rpois() (see ?rpois). I’m not going to go into a full-blown tutorial on the Poisson distribution or on count regression. For more thorough introductions, check out Atkins et al’s [-@atkinsTutorialOnCount2013] A tutorial on count regression and zero-altered count models for longitudinal substance use data, chapters 9 through 11 in McElreath’s [-@mcelreathStatisticalRethinkingBayesian2015] Statistical Rethinking, or, if you really want to dive in, Agresti’s [-@agrestiFoundationsLinearGeneralized2015] Foundations of linear and generalized linear models.\nFor our power example, let’s say you were interested in drinking. Using data from the National Epidemiologic Survey on Alcohol and Related Conditions [@niaaaNationalEpidemiologicSurvey2006], Christopher Ingraham [-@ingrahamThinkYouDrink2014] presented a data visualization of the average number of alcoholic drinks American adults consume, per week. By decile, the numbers were:\n0.00 0.00 0.00 0.02 0.14 0.63 2.17 6.25 15.28 73.85  Let’s say you wanted to run a study where you planned on comparing two demographic groups by their weekly drinking levels. Let’s further say you suspected one of those groups drank like the American adults in the 7th decile and the other drank like American adults in the 8th. We’ll call them low and high drinkers, respectively. For convenience, let’s further presume you’ll be able to recruit equal numbers of participants from both groups. The objective for our power analysis–or sample size analysis if you prefer to avoid the language of power–is to determine how many you’d need per group to detect reliable differences. Using \\(n = 50\\) as a starting point, here’s what the data for our hypothetical groups might look like.\nmu_7 \u0026lt;- 2.17 mu_8 \u0026lt;- 6.25 n \u0026lt;- 50 set.seed(3) d \u0026lt;- tibble(low = rpois(n = n, lambda = mu_7), high = rpois(n = n, lambda = mu_8)) %\u0026gt;% gather(group, count) d %\u0026gt;% mutate(count = factor(count)) %\u0026gt;% ggplot(aes(x = count)) + geom_bar() + facet_wrap(~group, ncol = 1) This will be our primary data type. Our next step is to determine how to express our research question as a regression model. Like with our two-group Gaussian models, we can predict counts in terms of an intercept (i.e., standing for the expected value on the reference group) and slope (i.e., standing for the expected difference between the reference group and the comparison group). If we coded our two groups by a high variable for which 0 stood for low drinkers and 1 stood for high drinkers, the basic model would follow the form\n\\[ \\begin{align*} \\text{drinks_per_week}_i \u0026amp; \\sim \\operatorname{Poisson}(\\lambda_i) \\\\ \\log(\\lambda_i) \u0026amp; = \\beta_0 + \\beta_1 \\text{high}_i. \\end{align*} \\]\nHere’s how to set the data up for that model.\nd \u0026lt;- d %\u0026gt;% mutate(high = ifelse(group == \u0026quot;low\u0026quot;, 0, 1)) If you were attending closely to our model formula, you noticed we ran into a detail. Count regression, such as with the Poisson likelihood, tends to use the log link. Why? you ask. Recall that counts need to be 0 and above. Same deal for our \\(\\lambda\\) parameter. In order to make sure our models don’t yield silly estimates for \\(\\lambda\\), like -2 or something, we typically use the log link. You don’t have to, of course. The world is your playground. But this is the method most of your colleagues are likely to use and it’s the one I suggest you use until you have compelling reasons to do otherwise.\nSo then since we’re now fitting a model with a log link, it might seem challenging to pick good priors. As a place to start, we can use the brms::get_prior() function to see the brms defaults.\nlibrary(brms) get_prior(data = d, family = poisson, count ~ 0 + Intercept + high) ## prior class coef group resp dpar nlpar bound ## 1 b ## 2 b high ## 3 b Intercept Hopefully two things popped out. First, there’s no prior of class = sigma. Since the Poisson distribution only has one parameter \\(\\lambda\\), we don’t need to set a prior for \\(\\sigma\\). Our model won’t have one. Second, because we’re continuing to use the 0 + Intercept syntax for our model intercept, both our intercept and slope are of prior class = b and those currently have default flat priors with brms. To be sure, flat priors aren’t the best. But maybe if this was your first time playing around with a Poisson model, default flat priors might seem like a safe place to start. Feel free to disagree. In the meantime, here’s how to fit that default Poisson model with brms::brm().\nfit1 \u0026lt;- brm(data = d, family = poisson, count ~ 0 + Intercept + high, seed = 3) print(fit1) ## Family: poisson ## Links: mu = log ## Formula: count ~ 0 + Intercept + high ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.59 0.11 0.38 0.79 1.01 917 1133 ## high 1.27 0.12 1.03 1.51 1.01 935 1182 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Since we used the log link, our model results are in the log metric, too. If you’d like them in the metric of the data, you’d work directly with the poster samples and exponentiate.\npost \u0026lt;- posterior_samples(fit1) %\u0026gt;% mutate(`beta_0 (i.e., low)` = exp(b_Intercept), `beta_1 (i.e., difference score for high)` = exp(b_high)) We can then just summarize our parameters of interest.\npost %\u0026gt;% select(starts_with(\u0026quot;beta_\u0026quot;)) %\u0026gt;% gather() %\u0026gt;% group_by(key) %\u0026gt;% summarise(mean = mean(value), lower = quantile(value, prob = .025), upper = quantile(value, prob = .975)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 4 ## key mean lower upper ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 beta_0 (i.e., low) 1.81 1.46 2.21 ## 2 beta_1 (i.e., difference score for high) 3.58 2.81 4.53 For the sake of simulation, it’ll be easier if we press on with evaluating the parameters on the log metric, though. If you’re working within a null-hypothesis oriented power paradigm, you’ll be happy to know zero is still the number to beat for evaluating our 95% intervals for \\(\\beta_1\\), even when that parameter is in the log metric. Here it is, again.\nlibrary(broom) tidy(fit1, prob = .95) %\u0026gt;% filter(term == \u0026quot;b_high\u0026quot;) ## term estimate std.error lower upper ## 1 b_high 1.269044 0.1211455 1.033061 1.510889 So our first fit suggests we’re on good footing to run a quick power simulation holding \\(n = 50\\). As in the prior blog posts, our lives will be simpler if we set up a custom simulation function. Since we’ll be using it to simulate the data and fit the model in one step, let’s call it sim_data_fit().\nsim_data_fit \u0026lt;- function(seed, n) { # define our mus in the function mu_7 \u0026lt;- 2.17 mu_8 \u0026lt;- 6.25 # make your results reproducible set.seed(seed) # simulate the data d \u0026lt;- tibble(high = rep(0:1, each = n), count = c(rpois(n = n, lambda = mu_7), rpois(n = n, lambda = mu_8))) # fit and summarise update(fit1, newdata = d, seed = seed) %\u0026gt;% tidy(prob = .95) %\u0026gt;% filter(term == \u0026quot;b_high\u0026quot;) %\u0026gt;% select(lower:upper) } Here’s the simulation for a simple 100 iterations.\nsim1 \u0026lt;- tibble(seed = 1:100) %\u0026gt;% mutate(ci = map(seed, sim_data_fit, n = 50)) %\u0026gt;% unnest() That went quick–just a little over a minute on my laptop. Here’s what those 100 \\(\\beta_1\\) intervals look like in bulk.\nsim1 %\u0026gt;% ggplot(aes(x = seed, ymin = lower, ymax = upper)) + geom_hline(yintercept = 0, color = \u0026quot;white\u0026quot;) + geom_linerange() + labs(x = \u0026quot;seed (i.e., simulation index)\u0026quot;, y = expression(beta[1])) None of them are anywhere near the null value 0. So it appears we’re well above .8 power to reject the typical \\(H_0\\) with \\(n = 50\\). Switching to the precision orientation, here’s the distribution of their widths.\nsim1 %\u0026gt;% mutate(width = upper - lower) %\u0026gt;% ggplot(aes(x = width)) + geom_histogram(binwidth = 0.01) + geom_rug(size = 1/6) What if we wanted a mean width of 0.25 on the log scale? We might try the simulation with \\(n = 150\\).\nsim2 \u0026lt;- tibble(seed = 1:100) %\u0026gt;% mutate(ci = map(seed, sim_data_fit, n = 150)) %\u0026gt;% unnest() Here we’ll summarize the widths both in terms of their mean and what proportion were smaller than 0.25.\nsim2 %\u0026gt;% mutate(width = upper - lower) %\u0026gt;% summarise(`mean width` = mean(width), `below 0.25` = mean(width \u0026lt; 0.25)) ## # A tibble: 1 x 2 ## `mean width` `below 0.25` ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.252 0.43 If we wanted to focus on the mean, we did pretty good. Perhaps set the \\(n = 155\\) and simulate a full 1,000+ iterations for a serious power analysis. But if we wanted to make the stricter criteria of all below 0.25, we’d need to up the \\(n\\) quite a bit more. And of course, once you have a little experience working with Poisson models, you might do the power simulations with more ambitious priors. For example, if your count values are lower than like 1,000, there’s a good chance a normal(0, 6) prior on your \\(\\beta\\) parameters will be nearly flat within the reasonable neighborhoods of the parameter space.\n But logs are hard. If we approach our Bayesian power analysis from a precision perspective, it can be difficult to settle on a reasonable interval width when they’re on the log scale. So let’s modify our simulation flow so it converts the width summaries back into the natural metric. Before we go big, let’s practice with a single iteration.\nseed \u0026lt;- 0 set.seed(seed) # simulate the data d \u0026lt;- tibble(high = rep(0:1, each = n), count = c(rpois(n = n, lambda = mu_7), rpois(n = n, lambda = mu_8))) # fit the model fit2 \u0026lt;- update(fit1, newdata = d, seed = seed)  Now summarize.\nlibrary(tidybayes) fit2 %\u0026gt;% posterior_samples() %\u0026gt;% transmute(`beta_1` = exp(b_high)) %\u0026gt;% mean_qi() ## beta_1 .lower .upper .width .point .interval ## 1 2.705404 2.16512 3.341729 0.95 mean qi Before we used the handy broom::tidy() function to extract our intervals, which took the brms fit object as input. Here we took a different approach. Because we are transforming \\(\\beta_1\\), we used the posterior_samples() function to work directly with the posterior draws. We then exponentiated within transmute(), which returned a single-column tibble, not a brms fit object. So instead of broom::tidy(), it’s easier to get our summary statistics with the tidybayes::mean_qi() function. Do note that now our lower and upper levels are named .lower and .upper, respectively (i.e., they now have a . prefix).\nNow we’ve practiced with the new flow, let’s redefine our simulation function.\nsim_data_fit \u0026lt;- function(seed, n) { # define our mus in the function mu_7 \u0026lt;- 2.17 mu_8 \u0026lt;- 6.25 # make your results reproducible set.seed(seed) # simulate the data d \u0026lt;- tibble(high = rep(0:1, each = n), count = c(rpois(n = n, lambda = mu_7), rpois(n = n, lambda = mu_8))) # fit and summarize update(fit1, newdata = d, seed = seed) %\u0026gt;% posterior_samples() %\u0026gt;% transmute(`beta_1` = exp(b_high)) %\u0026gt;% mean_qi() } Simulate.\nsim3 \u0026lt;- tibble(seed = 1:100) %\u0026gt;% mutate(ci = map(seed, sim_data_fit, n = 50)) %\u0026gt;% unnest() Here’s what those 100 \\(\\beta_1\\) intervals look like in bulk.\nsim3 %\u0026gt;% ggplot(aes(x = seed, y = beta_1, ymin = .lower, ymax = .upper)) + geom_hline(yintercept = 0, color = \u0026quot;white\u0026quot;) + geom_pointrange(fatten = 1) + labs(x = \u0026quot;seed (i.e., simulation index)\u0026quot;, y = expression(beta[1])) Inspect the distribution of their widths.\nsim3 %\u0026gt;% mutate(width = .upper - .lower) %\u0026gt;% ggplot(aes(x = width)) + geom_histogram(binwidth = 0.05) + geom_rug(size = 1/6) What if we wanted a mean 95% interval width of 1? Let’s run the simulation again, this time with \\(n = 100\\).\nsim4 \u0026lt;- tibble(seed = 1:100) %\u0026gt;% mutate(ci = map(seed, sim_data_fit, n = 100)) %\u0026gt;% unnest() %\u0026gt;% mutate(width = .upper - .lower) Here’s the new width distribution.\nsim4 %\u0026gt;% ggplot(aes(x = width)) + geom_histogram(binwidth = 0.05) + geom_rug(size = 1/6) And the mean width is:\nsim4 %\u0026gt;% summarise(mean_width = mean(width)) ## # A tibble: 1 x 1 ## mean_width ## \u0026lt;dbl\u0026gt; ## 1 0.913 Nice! If we want a mean width of 1, it looks like we’re a little overpowered with \\(n = 100\\). The next step would be to up your iterations to 1,000 or so to do a proper simulation.\nNow you’ve got a sense of how to work with the Poisson likelihood, next time we’ll play with binary data.\n Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.1.1 broom_0.5.5 brms_2.13.0 Rcpp_1.0.5 ## [5] forcats_0.5.0 stringr_1.4.0 dplyr_1.0.1 purrr_0.3.4 ## [9] readr_1.3.1 tidyr_1.1.1 tibble_3.0.3 ggplot2_3.3.2 ## [13] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] TH.data_1.0-10 colorspace_1.4-1 ellipsis_0.3.1 ## [4] ggridges_0.5.2 rsconnect_0.8.16 estimability_1.3 ## [7] markdown_1.1 base64enc_0.1-3 fs_1.4.1 ## [10] rstudioapi_0.11 farver_2.0.3 rstan_2.19.3 ## [13] svUnit_1.0.3 DT_0.13 fansi_0.4.1 ## [16] mvtnorm_1.1-0 lubridate_1.7.8 xml2_1.3.1 ## [19] codetools_0.2-16 splines_3.6.3 bridgesampling_1.0-0 ## [22] knitr_1.28 shinythemes_1.1.2 bayesplot_1.7.1 ## [25] jsonlite_1.7.0 dbplyr_1.4.2 ggdist_2.1.1 ## [28] shiny_1.5.0 compiler_3.6.3 httr_1.4.1 ## [31] emmeans_1.4.5 backports_1.1.8 assertthat_0.2.1 ## [34] Matrix_1.2-18 fastmap_1.0.1 cli_2.0.2 ## [37] later_1.1.0.1 htmltools_0.5.0 prettyunits_1.1.1 ## [40] tools_3.6.3 igraph_1.2.5 coda_0.19-3 ## [43] gtable_0.3.0 glue_1.4.1 reshape2_1.4.4 ## [46] cellranger_1.1.0 vctrs_0.3.2 nlme_3.1-144 ## [49] blogdown_0.18 crosstalk_1.1.0.1 xfun_0.13 ## [52] ps_1.3.4 rvest_0.3.5 mime_0.9 ## [55] miniUI_0.1.1.1 lifecycle_0.2.0 gtools_3.8.2 ## [58] MASS_7.3-51.5 zoo_1.8-7 scales_1.1.1 ## [61] colourpicker_1.0 hms_0.5.3 promises_1.1.1 ## [64] Brobdingnag_1.2-6 sandwich_2.5-1 parallel_3.6.3 ## [67] inline_0.3.15 shinystan_2.5.0 yaml_2.2.1 ## [70] gridExtra_2.3 StanHeaders_2.21.0-1 loo_2.2.0 ## [73] stringi_1.4.6 dygraphs_1.1.1.6 pkgbuild_1.1.0 ## [76] rlang_0.4.7 pkgconfig_2.0.3 matrixStats_0.56.0 ## [79] evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [82] htmlwidgets_1.5.1 labeling_0.3 tidyselect_1.1.0 ## [85] processx_3.4.3 plyr_1.8.6 magrittr_1.5 ## [88] bookdown_0.18 R6_2.4.1 generics_0.0.2 ## [91] multcomp_1.4-13 DBI_1.1.0 pillar_1.4.6 ## [94] haven_2.2.0 withr_2.2.0 xts_0.12-0 ## [97] survival_3.1-12 abind_1.4-5 modelr_0.1.6 ## [100] crayon_1.3.4 arrayhelpers_1.1-0 utf8_1.1.4 ## [103] rmarkdown_2.1 grid_3.6.3 readxl_1.3.1 ## [106] callr_3.4.3 threejs_0.3.3 reprex_0.3.0 ## [109] digest_0.6.25 xtable_1.8-4 httpuv_1.5.4 ## [112] stats4_3.6.3 munsell_0.5.0 shinyjs_1.1  References   Yes, one can smoke half a cigarette or drink 1/3 of a drink. Ideally, we’d have the exact amount of nicotine in your blood at a given moment and over time and the same for the amount of alcohol in your system relative to your blood volume and such. But in practice, substance use researchers just don’t tend to have access to data of that quality. Instead, we’re typically stuck with simple counts. And I look forward to the day the right team of engineers, computer scientists, and substance use researchers (and whoever else I forgot to mention) release the cheap, non-invasive technology we need to passively measure these things. Until then: How many standard servings of alcohol did you drink, last night?↩\n   ","date":1565481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565481600,"objectID":"47fc155b31de15cea6d2362232162ef2","permalink":"/post/bayesian-power-analysis-part-iii-a/","publishdate":"2019-08-11T00:00:00Z","relpermalink":"/post/bayesian-power-analysis-part-iii-a/","section":"post","summary":"Version 1.0.1  Orientation So far we’ve covered Bayesian power simulations from both a null hypothesis orientation (see part I) and a parameter width perspective (see part II). In both instances, we kept things simple and stayed with Gaussian (i.e., normally distributed) data. But not all data follow that form, so it might do us well to expand our skill set a bit. In the next few posts, we’ll cover how we might perform power simulations with other kinds of data.","tags":["Bayesian","power","R","tidyverse","tutorial"],"title":"Bayesian power analysis: Part III.a. Counts are special.","type":"post"},{"authors":[],"categories":[],"content":" Version 1.0.1  tl;dr When researchers decide on a sample size for an upcoming project, there are more things to consider than null-hypothesis-oriented power. Bayesian researchers might like to frame their concerns in terms of precision. Stick around to learn what and how.\n Are Bayesians doomed to refer to \\(H_0\\)1 with sample-size planning? If you read the first post in this series (click here for a refresher), you may have found yourself thinking: Sure, last time you avoided computing \\(p\\)-values with your 95% Bayesian credible intervals. But weren’t you still operating like a NHSTesting frequentist with all that \\(H_0 / H_1\\) talk?\nSolid criticism. We didn’t even bother discussing all the type-I versus type-II error details. Yet they too were lurking in the background the way we just chose the typical .8 power benchmark. That’s not to say that a \\(p\\)-value oriented approach isn’t legitimate. It’s certainly congruent with what most reviewers would expect.2 But this all seems at odds with a model-oriented Bayesian approach, which is what I generally prefer. Happily, we have other options to explore.\n Let’s just pick up where we left off. Load our primary statistical packages.\nlibrary(tidyverse) library(brms) library(broom) As a recap, here’s how we performed the last simulation-based Bayesian power analysis from part I. First, we simulated a single data set and fit an initial model.\n# define the means mu_c \u0026lt;- 0 mu_t \u0026lt;- 0.5 # determine the group size n \u0026lt;- 50 # simulate the data set.seed(1) d \u0026lt;- tibble(group = rep(c(\u0026quot;control\u0026quot;, \u0026quot;treatment\u0026quot;), each = n)) %\u0026gt;% mutate(treatment = ifelse(group == \u0026quot;control\u0026quot;, 0, 1), y = ifelse(group == \u0026quot;control\u0026quot;, rnorm(n, mean = mu_c, sd = 1), rnorm(n, mean = mu_t, sd = 1))) # fit the model fit \u0026lt;- brm(data = d, family = gaussian, y ~ 0 + intercept + treatment, prior = c(prior(normal(0, 2), class = b), prior(student_t(3, 1, 1), class = sigma)), seed = 1) Next, we made a custom function that both simulated data sets and used the update() function to update that initial fit in order to avoid additional compilation time.\nsim_d_and_fit \u0026lt;- function(seed, n) { mu_c \u0026lt;- 0 mu_t \u0026lt;- 0.5 set.seed(seed) d \u0026lt;- tibble(group = rep(c(\u0026quot;control\u0026quot;, \u0026quot;treatment\u0026quot;), each = n)) %\u0026gt;% mutate(treatment = ifelse(group == \u0026quot;control\u0026quot;, 0, 1), y = ifelse(group == \u0026quot;control\u0026quot;, rnorm(n, mean = mu_c, sd = 1), rnorm(n, mean = mu_t, sd = 1))) update(fit, newdata = d, seed = seed) %\u0026gt;% tidy(prob = .95) %\u0026gt;% filter(term == \u0026quot;b_treatment\u0026quot;) } Then we finally iterated over n_sim \u0026lt;- 100 times.\nn_sim \u0026lt;- 100 s3 \u0026lt;- tibble(seed = 1:n_sim) %\u0026gt;% mutate(tidy = map(seed, sim_d_and_fit, n = 50)) %\u0026gt;% unnest(tidy) The results looked like so:\ntheme_set(theme_grey() + theme(panel.grid = element_blank())) s3 %\u0026gt;% ggplot(aes(x = seed, y = estimate, ymin = lower, ymax = upper)) + geom_hline(yintercept = c(0, .5), color = \u0026quot;white\u0026quot;) + geom_pointrange(fatten = 1/2) + labs(x = \u0026quot;seed (i.e., simulation index)\u0026quot;, y = expression(beta[1])) It’s time to build on the foundation.\n We might evaluate “power” by widths. Instead of just ordering the point-ranges by their seed values, we might instead arrange them by the lower levels.\ns3 %\u0026gt;% ggplot(aes(x = reorder(seed, lower), y = estimate, ymin = lower, ymax = upper)) + geom_hline(yintercept = c(0, .5), color = \u0026quot;white\u0026quot;) + geom_pointrange(fatten = 1/2) + scale_x_discrete(\u0026quot;reordered by the lower level of the 95% intervals\u0026quot;, breaks = NULL) + ylab(expression(beta[1])) + coord_cartesian(ylim = c(-.5, 1.3)) Notice how this arrangement highlights the differences in widths among the intervals. The wider the interval, the less precise the estimate. Some intervals were wider than others, but all tended to hover in a similar range. We might quantify those ranges by computing a width variable.\ns3 \u0026lt;- s3 %\u0026gt;% mutate(width = upper - lower) head(s3) ## # A tibble: 6 x 7 ## seed term estimate std.error lower upper width ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 b_treatment 0.513 0.183 0.153 0.874 0.721 ## 2 2 b_treatment 0.300 0.241 -0.170 0.776 0.945 ## 3 3 b_treatment 0.640 0.174 0.297 0.990 0.693 ## 4 4 b_treatment 0.225 0.183 -0.135 0.583 0.717 ## 5 5 b_treatment 0.432 0.194 0.0552 0.810 0.755 ## 6 6 b_treatment 0.305 0.209 -0.101 0.714 0.815 Here’s the width distribution.\ns3 %\u0026gt;% ggplot(aes(x = width)) + geom_histogram(binwidth = .01) The widths of our 95% intervals range from 0.6 to 0.95, with the bulk sitting around 0.8. Let’s focus a bit and take a random sample from one of the simulation iterations.\nset.seed(1) s3 %\u0026gt;% sample_n(1) %\u0026gt;% mutate(seed = seed %\u0026gt;% as.character()) %\u0026gt;% ggplot(aes(x = estimate, xmin = lower, xmax = upper, y = seed)) + geom_vline(xintercept = c(0, .5), color = \u0026quot;white\u0026quot;) + geom_pointrange() + labs(x = expression(beta[1]), y = \u0026quot;seed #\u0026quot;) + xlim(0, 1) Though the posterior mean suggests the most probable value for \\(\\beta_1\\) is about 0.6, the intervals suggest values from about 0.2 to almost 1 are within the 95% probability range. That’s a wide spread. Within psychology, a standardized mean difference of 0.2 would typically be considered small, whereas a difference of 1 would be large enough to raise a skeptical eyebrow or two.\nSo instead of focusing on rejecting a null hypothesis like \\(\\mu_\\text{control} = \\mu_\\text{treatment}\\), we might instead use our simulation skills to determine the sample size we need to have most of our 95% intervals come in at a certain level of precision. This has been termed the accuracy in parameter estimation (AIPE; Maxwell et al., 2008; see also Kruschke, 2015) approach to sample size planning.\nThinking in terms of AIPE, in terms of precision, let’s say we wanted widths of 0.7 or smaller. Here’s how we did with s3.\ns3 %\u0026gt;% mutate(check = ifelse(width \u0026lt; .7, 1, 0)) %\u0026gt;% summarise(`width power` = mean(check)) ## # A tibble: 1 x 1 ## `width power` ## \u0026lt;dbl\u0026gt; ## 1 0.07 We did terrible. I’m not sure the term “width power” is even a thing. But hopefully you get the point. Our baby 100-iteration simulation suggests we have about a .08 probability of achieving 95% CI widths of 0.7 or smaller with \\(n = 50\\) per group. Though we’re pretty good at excluding zero, we don’t tend to do so with precision above that.\nThat last bit about excluding zero brings up an important point. Once we’re concerned about width size, about precision, the null hypothesis is no longer of direct relevance. And since we’re no longer wed to thinking in terms of the null hypothesis, there’s no real need to stick with a .8 threshold for evaluating width power (okay, I’ll stop using that term). Now if we wanted to stick with .8, we could. Though a little nonsensical, the .8 criterion would give our AIPE analyses a sense of familiarity with traditional power analyses, which some reviewers might appreciate. But in his text, Kruschke mentioned several other alternatives. One would be to set maximum value for our CI widths and simulate to find the \\(n\\) necessary so all our simulations pass that criterion. Another would follow Joseph, Wolfson, and du Berger (1995a, 1995b), who suggested we shoot for an \\(n\\) that produces widths that pass that criterion on average. Here’s how we did based on the average-width criterion.\ns3 %\u0026gt;% summarise(`average width` = mean(width)) ## # A tibble: 1 x 1 ## `average width` ## \u0026lt;dbl\u0026gt; ## 1 0.784 Close. Let’s see how increasing our sample size to 75 per group effects these metrics.\ns4 \u0026lt;- tibble(seed = 1:n_sim) %\u0026gt;% mutate(tidy = map(seed, sim_d_and_fit, n = 75)) %\u0026gt;% unnest(tidy) %\u0026gt;% mutate(width = upper - lower) Here’s what our new batch of 95% intervals looks like.\ns4 %\u0026gt;% ggplot(aes(x = reorder(seed, lower), y = estimate, ymin = lower, ymax = upper)) + geom_hline(yintercept = c(0, .5), color = \u0026quot;white\u0026quot;) + geom_pointrange(fatten = 1/2) + scale_x_discrete(\u0026quot;reordered by the lower level of the 95% intervals\u0026quot;, breaks = NULL) + ylab(expression(beta[1])) + # this kept the scale on the y-axis the same as the simulation with n = 50 coord_cartesian(ylim = c(-.5, 1.3)) Some of the intervals are still more precise than others, but they all now hover more tightly around their true data-generating value of 0.5. Here’s our updated “power” for producing interval widths smaller than 0.7.\ns4 %\u0026gt;% mutate(check = ifelse(width \u0026lt; .7, 1, 0)) %\u0026gt;% summarise(`proportion below 0.7` = mean(check), `average width` = mean(width)) ## # A tibble: 1 x 2 ## `proportion below 0.7` `average width` ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.96 0.639 If we hold to the NHST-oriented .8 threshold, we did great and are even “overpowered”. We didn’t quite meet Kruschke’s strict limiting-worst-precision threshold, but we got close enough we’d have a good sense of what range of \\(n\\) values we might evaluate over next. As far as the mean-precision criterion, we did great by that one and even beat it by about 0.04.\nHere’s a look at how this batch of widths is distributed.\ns4 %\u0026gt;% ggplot(aes(x = width)) + geom_histogram(binwidth = .02) + geom_rug(size = 1/6) Let’s see if we can nail down the \\(n\\)s for our three AIPE criteria. Since we’re so close to fulfilling Kruschke’s limiting-worst-precision criterion, we’ll start there. I’m thinking \\(n = 85\\) should just about do it.\ns5 \u0026lt;- tibble(seed = 1:n_sim) %\u0026gt;% mutate(tidy = map(seed, sim_d_and_fit, n = 85)) %\u0026gt;% unnest(tidy) %\u0026gt;% mutate(width = upper - lower) Did we pass?\ns5 %\u0026gt;% mutate(check = ifelse(width \u0026lt; .7, 1, 0)) %\u0026gt;% summarise(`proportion below 0.7` = mean(check)) ## # A tibble: 1 x 1 ## `proportion below 0.7` ## \u0026lt;dbl\u0026gt; ## 1 1 Success! We might look at how they’re distributed.\ns5 %\u0026gt;% ggplot(aes(x = width)) + geom_histogram(binwidth = .01) + geom_rug(size = 1/6) Two of our simulated widths were pretty close to the 0.7 boundary. If we were to do a proper simulation with 1,000+ iterations, I’d worry one or two would creep over that boundary. So perhaps \\(n = 90\\) would be a better candidate for a large-scale simulation.\nIf we just wanted to meet the mean-precision criterion, we might look at something like \\(n = 65\\).\ns6 \u0026lt;- tibble(seed = 1:n_sim) %\u0026gt;% mutate(tidy = map(seed, sim_d_and_fit, n = 65)) %\u0026gt;% unnest(tidy) %\u0026gt;% mutate(width = upper - lower) Did we pass the mean-precision criterion?\ns6 %\u0026gt;% summarise(`average width` = mean(width)) ## # A tibble: 1 x 1 ## `average width` ## \u0026lt;dbl\u0026gt; ## 1 0.690 We got it! It looks like something like \\(n = 65\\) would be a good candidate for a larger-scale simulation. Here’s the distribution.\ns6 %\u0026gt;% ggplot(aes(x = width)) + geom_histogram(binwidth = .02) + geom_rug(size = 1/6) For our final possible criterion, just get .8 of the widths below the threshold, we’ll want an \\(n\\) somewhere between 65 and 85. 70, perhaps?\ns7 \u0026lt;- tibble(seed = 1:n_sim) %\u0026gt;% mutate(tidy = map(seed, sim_d_and_fit, n = 70)) %\u0026gt;% unnest(tidy) %\u0026gt;% mutate(width = upper - lower) Did we pass the .8-threshold criterion?\ns7 %\u0026gt;% mutate(check = ifelse(width \u0026lt; .7, 1, 0)) %\u0026gt;% summarise(`proportion below 0.7` = mean(check)) ## # A tibble: 1 x 1 ## `proportion below 0.7` ## \u0026lt;dbl\u0026gt; ## 1 0.8 Yep. Here’s the distribution.\ns7 %\u0026gt;% ggplot(aes(x = width)) + geom_histogram(binwidth = .02) + geom_rug(size = 1/6)  How are we defining our widths? In frequentist analyses, we typically work with 95% confidence intervals because of their close connection to the conventional \\(p \u0026lt; .05\\) threshold. Another consequence of dropping our focus on rejecting \\(H_0\\) is that it no longer seems necessary to evaluate our posteriors with 95% intervals. And as it turns out, some Bayesians aren’t fans of the 95% interval. McElreath, for example, defiantly used 89% intervals in both editions of his (2020, 2015) text. In contrast, Gelman has blogged on his fondness for 50% intervals. Just for kicks, let’s follow Gelman’s lead and practice evaluating an \\(n\\) based on 50% intervals. This will require us to update our sim_d_and_fit() function to allow us to change the prob setting in the broom::tidy() function.\nsim_d_and_fit \u0026lt;- function(seed, n, prob) { mu_c \u0026lt;- 0 mu_t \u0026lt;- 0.5 set.seed(seed) d \u0026lt;- tibble(group = rep(c(\u0026quot;control\u0026quot;, \u0026quot;treatment\u0026quot;), each = n)) %\u0026gt;% mutate(treatment = ifelse(group == \u0026quot;control\u0026quot;, 0, 1), y = ifelse(group == \u0026quot;control\u0026quot;, rnorm(n, mean = mu_c, sd = 1), rnorm(n, mean = mu_t, sd = 1))) update(fit, newdata = d, seed = seed) %\u0026gt;% tidy(prob = prob) %\u0026gt;% filter(term == \u0026quot;b_treatment\u0026quot;) } Now simulate to examine those 50% intervals. We’ll start with the original \\(n = 50\\)\nn_sim \u0026lt;- 100 s8 \u0026lt;- tibble(seed = 1:n_sim) %\u0026gt;% mutate(tidy = map(seed, sim_d_and_fit, n = 50, prob = .5)) %\u0026gt;% unnest(tidy) %\u0026gt;% mutate(width = upper - lower) Here is the distribution of our 50% interval widths.\ns8 %\u0026gt;% mutate(width = upper - lower) %\u0026gt;% ggplot(aes(x = width)) + geom_histogram(binwidth = .01) + geom_rug(size = 1/6) Since we’ve gone from 95% to 50% intervals, it should be no surprise that their widths are narrower. Accordingly, we should evaluate then with a higher standard. Perhaps it’s more reasonable to ask for an average width of 0.1. Let’s see how close \\(n = 150\\) gets us.\ns9 \u0026lt;- tibble(seed = 1:n_sim) %\u0026gt;% mutate(tidy = map(seed, sim_d_and_fit, n = 150, prob = .5)) %\u0026gt;% unnest(tidy) %\u0026gt;% mutate(width = upper - lower) Look at the distribution.\ns9 %\u0026gt;% ggplot(aes(x = width)) + geom_histogram(binwidth = .0025) + geom_rug(size = 1/6) Nope, we’re not there yet. Perhaps \\(n = 200\\) or \\(250\\) is the ticket. This is an iterative process. Anyway, once we’re talking that AIPE/precision/interval-width talk, we can get all kinds of creative with which intervals we’re even interested in. As far as I can tell, the topic is wide open for fights and collaborations between statisticians, methodologists, and substantive researchers to find sensible ways forward.\nMaybe you should write a dissertation on it.\nRegardless, get ready for part III where we’ll liberate ourselves from the tyranny of the Gauss.\n Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] broom_0.5.5 brms_2.13.0 Rcpp_1.0.5 forcats_0.5.0 ## [5] stringr_1.4.0 dplyr_1.0.1 purrr_0.3.4 readr_1.3.1 ## [9] tidyr_1.1.1 tibble_3.0.3 ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] TH.data_1.0-10 colorspace_1.4-1 ellipsis_0.3.1 ## [4] ggridges_0.5.2 rsconnect_0.8.16 estimability_1.3 ## [7] markdown_1.1 base64enc_0.1-3 fs_1.4.1 ## [10] rstudioapi_0.11 farver_2.0.3 rstan_2.19.3 ## [13] DT_0.13 fansi_0.4.1 mvtnorm_1.1-0 ## [16] lubridate_1.7.8 xml2_1.3.1 codetools_0.2-16 ## [19] splines_3.6.3 bridgesampling_1.0-0 knitr_1.28 ## [22] shinythemes_1.1.2 bayesplot_1.7.1 jsonlite_1.7.0 ## [25] dbplyr_1.4.2 shiny_1.5.0 compiler_3.6.3 ## [28] httr_1.4.1 emmeans_1.4.5 backports_1.1.8 ## [31] assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [34] cli_2.0.2 later_1.1.0.1 htmltools_0.5.0 ## [37] prettyunits_1.1.1 tools_3.6.3 igraph_1.2.5 ## [40] coda_0.19-3 gtable_0.3.0 glue_1.4.1 ## [43] reshape2_1.4.4 cellranger_1.1.0 vctrs_0.3.2 ## [46] nlme_3.1-144 blogdown_0.18 crosstalk_1.1.0.1 ## [49] xfun_0.13 ps_1.3.4 rvest_0.3.5 ## [52] mime_0.9 miniUI_0.1.1.1 lifecycle_0.2.0 ## [55] gtools_3.8.2 MASS_7.3-51.5 zoo_1.8-7 ## [58] scales_1.1.1 colourpicker_1.0 hms_0.5.3 ## [61] promises_1.1.1 Brobdingnag_1.2-6 sandwich_2.5-1 ## [64] parallel_3.6.3 inline_0.3.15 shinystan_2.5.0 ## [67] yaml_2.2.1 gridExtra_2.3 loo_2.2.0 ## [70] StanHeaders_2.21.0-1 stringi_1.4.6 dygraphs_1.1.1.6 ## [73] pkgbuild_1.1.0 rlang_0.4.7 pkgconfig_2.0.3 ## [76] matrixStats_0.56.0 evaluate_0.14 lattice_0.20-38 ## [79] labeling_0.3 rstantools_2.0.0 htmlwidgets_1.5.1 ## [82] tidyselect_1.1.0 processx_3.4.3 plyr_1.8.6 ## [85] magrittr_1.5 bookdown_0.18 R6_2.4.1 ## [88] generics_0.0.2 multcomp_1.4-13 DBI_1.1.0 ## [91] pillar_1.4.6 haven_2.2.0 withr_2.2.0 ## [94] xts_0.12-0 survival_3.1-12 abind_1.4-5 ## [97] modelr_0.1.6 crayon_1.3.4 utf8_1.1.4 ## [100] rmarkdown_2.1 grid_3.6.3 readxl_1.3.1 ## [103] callr_3.4.3 threejs_0.3.3 reprex_0.3.0 ## [106] digest_0.6.25 xtable_1.8-4 httpuv_1.5.4 ## [109] stats4_3.6.3 munsell_0.5.0 shinyjs_1.1  References Joseph, L., Wolfson, D. B., \u0026amp; Berger, R. D. (1995a). Sample size calculations for binomial proportions via highest posterior density intervals. Journal of the Royal Statistical Society: Series D (the Statistician), 44(2), 143–154. https://doi.org/10.2307/2348439\n Joseph, L., Wolfson, D. B., \u0026amp; Berger, R. D. (1995b). Some comments on Bayesian sample size determination. Journal of the Royal Statistical Society: Series D (the Statistician), 44(2), 167–171. https://doi.org/10.2307/2348442\n Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n Maxwell, S. E., Kelley, K., \u0026amp; Rausch, J. R. (2008). Sample size planning for statistical power and accuracy in parameter estimation. Annual Review of Psychology, 59(1), 537–563. https://doi.org/10.1146/annurev.psych.59.103006.093735\n McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/\n Morey, R. D., \u0026amp; Rouder, J. N. (2011). Bayes factor approaches for testing interval null hypotheses. Psychological Methods, 16(4), 406–419. https://doi.org/10.1037/a0024377\n Rouder, J. N., Speckman, P. L., Sun, D., Morey, R. D., \u0026amp; Iverson, G. (2009). Bayesian t tests for accepting and rejecting the null hypothesis. Psychonomic Bulletin \u0026amp; Review, 16(2), 225–237. https://doi.org/10.3758/PBR.16.2.225\n Wasserstein, R. L., Schirm, A. L., \u0026amp; Lazar, N. A. (2019). Moving to a World Beyond “p \\(\u0026lt;\\) 0.05”. The American Statistician, 73(sup1), 1–19. https://doi.org/10.1080/00031305.2019.1583913\n    To be clear, one can consider the null hypothesis within the Bayesian paradigm. I don’t tend to take this approach, but it’d be unfair not to at least mention some resources. Kurschke covered the topic in chapters 11 and 12 in his (2015) text, Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. You might also check out Rouder et al. (2009), Bayesian t tests for accepting and rejecting the null hypothesis, or Morey \u0026amp; Rouder (2011), Bayes factor approaches for testing interval null hypotheses.↩\n For a contemporary discussion of the uses and misuses of \\(p\\)-values, see Wasserstein et al. (2019) and the other articles contained in that special issue of The American Statistician.↩\n   ","date":1563926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563926400,"objectID":"9aa90b28b2598ed89d8ea8a96d6dbe47","permalink":"/post/bayesian-power-analysis-part-ii/","publishdate":"2019-07-24T00:00:00Z","relpermalink":"/post/bayesian-power-analysis-part-ii/","section":"post","summary":"Version 1.0.1  tl;dr When researchers decide on a sample size for an upcoming project, there are more things to consider than null-hypothesis-oriented power. Bayesian researchers might like to frame their concerns in terms of precision. Stick around to learn what and how.\n Are Bayesians doomed to refer to \\(H_0\\)1 with sample-size planning? If you read the first post in this series (click here for a refresher), you may have found yourself thinking: Sure, last time you avoided computing \\(p\\)-values with your 95% Bayesian credible intervals.","tags":["Bayesian","brms","power","R","tutorial","tidyverse"],"title":"Bayesian power analysis: Part II. Some might prefer precision to power","type":"post"},{"authors":[],"categories":[],"content":" Version 1.0.1  tl;dr If you’d like to learn how to do Bayesian power calculations using brms, stick around for this multi-part blog series. Here with part I, we’ll set the foundation.\n Power is hard, especially for Bayesians. Many journals, funding agencies, and dissertation committees require power calculations for your primary analyses. Frequentists have a variety of tools available to perform these calculations (e.g., here). Bayesians, however, have a more difficult time of it. Most of our research questions and data issues are sufficiently complicated that we cannot solve the problems by hand. We need Markov chain Monte Carlo methods to iteratively sample from the posterior to summarize the parameters from our models. Same deal for power. If you’d like to compute the power for a given combination of \\(N\\), likelihood \\(p(\\text{data} | \\theta)\\), and set of priors \\(p (\\theta)\\), you’ll need to simulate.\nIt’s been one of my recent career goals to learn how to do this. You know how they say: The best way to learn is to teach. This series of blog posts is the evidence of me learning by teaching. It will be an exploration of what a Bayesian power simulation workflow might look like. The overall statistical framework will be within R (R Core Team, 2020), with an emphasis on code style based on the tidyverse (Wickham, 2019; Wickham et al., 2019). We’ll be fitting our Bayesian models with Bürkner’s brms package (Bürkner, 2017, 2018, 2020a).\nWhat this series is not, however, is an introduction to statistical power itself. Keep reading if you’re ready to roll up your sleeves, put on your applied hat, and learn how to get things done. If you’re more interested in introductions to power, see the references in the next section.\n I make assumptions. For this series, I’m presuming you are familiar with linear regression, familiar with the basic differences between frequentist and Bayesian approaches to statistics, and have a basic sense of what we mean by statistical power. Here are some resources if you’d like to shore up.\n If you’re unfamiliar with statistical power, Kruschke covered it in chapter 13 of his (2015) text. You might also check out the (2008) review paper by Maxwell, Kelley, and Rausch. There’s always, of course, the original work by Cohen (e.g., Cohen, 1988). You might also like this Khan Academy video. To learn about Bayesian regression, I recommend the introductory text books by either McElreath (2020, 2015) or Kruschke (2015). Both authors host blogs (here and here, respectively). If you go with McElreath, do check out his online lectures and my (2020a, 2020c) ebooks translating his text to brms and tidyverse code. I have an ebook for Kruschke’s text (Kurz, 2020b), too. For even more brms-related resources, you can find vignettes and documentation at https://cran.r-project.org/package=brms/index.html. For tidyverse introductions, your best bets are Grolemund and Wickham’s (2017) R for data science and Wickham’s (2020) The tidyverse style guide. We’ll be simulating data. If that’s new to you, both Kruschke and McElreath cover that a little in their texts. You can find nice online tutorials here and here, too. We’ll also be making a couple custom functions. If that’s new, you might check out R4DS, chapter 19 or chapter 14 of Roger Peng’s (2019) R Programming for Data Science.   We need to warm up before jumping into power. Let’s load our primary packages. The tidyverse helps organize data, we model with brms, and broom will help organize the model summaries.\nlibrary(tidyverse) library(brms) library(broom) Consider a case where you have some dependent variable \\(Y\\) that you’d like to compare between two groups, which we’ll call treatment and control. Here we presume \\(Y\\) is continuous and, for the sake of simplicity, is in a standardized metric for the control condition. Letting \\(c\\) stand for control and \\(i\\) index the data row for a given case, we might write that as \\(y_{i, c} \\sim \\operatorname{Normal} (0, 1)\\). The mean for our treatment condition is 0.5, with the standard deviation still in the standardized metric. In the social sciences a standardized mean difference of 0.5 would typically be considered a medium effect size. Here’s what that’d look like.\n# set our theme because, though I love the default ggplot theme, I hate gridlines theme_set(theme_grey() + theme(panel.grid = element_blank())) # define the means mu_c \u0026lt;- 0 mu_t \u0026lt;- 0.5 # set up the data tibble(x = seq(from = -4, to = 5, by = .01)) %\u0026gt;% mutate(c = dnorm(x, mean = mu_c, sd = 1), t = dnorm(x, mean = mu_t, sd = 1)) %\u0026gt;% # plot ggplot(aes(x = x, ymin = 0)) + geom_ribbon(aes(ymax = c), size = 0, alpha = 1/3, fill = \u0026quot;grey25\u0026quot;) + geom_ribbon(aes(ymax = t), size = 0, alpha = 1/3, fill = \u0026quot;blue2\u0026quot;) + geom_text(data = tibble(x = c(-.5, 1), y = .385, label = c(\u0026quot;control\u0026quot;, \u0026quot;treatment\u0026quot;), hjust = 1:0), aes(y = y, label = label, color = label, hjust = hjust), size = 5, show.legend = F) + scale_x_continuous(NULL, breaks = -4:5) + scale_y_continuous(NULL, breaks = NULL) + scale_color_manual(values = c(\u0026quot;grey25\u0026quot;, \u0026quot;blue2\u0026quot;)) Sure, those distributions have a lot of overlap. But their means are clearly different and we’d like to make sure we plan on collecting enough data to do a good job showing that. A power analysis will help.\nWithin the conventional frequentist paradigm, power is the probability of rejecting the null hypothesis \\(H_0\\) in favor of the alternative hypothesis \\(H_1\\), given the alternative hypothesis is “true.” In this case, the typical null hypothesis is\n\\[H_0\\text{: } \\mu_c = \\mu_t,\\]\nor put differently,\n\\[ H_0\\text{: } \\mu_t - \\mu_c = 0. \\]\nAnd the alternative hypothesis is often just\n\\[H_1\\text{: } \\mu_c \\neq \\mu_t,\\]\nor otherwise put,\n\\[ H_1\\text{: } \\mu_t - \\mu_c \\neq 0. \\]\nWithin the regression framework, we’ll be comparing \\(\\mu\\)s using the formula\n\\[ \\begin{align*} y_i \u0026amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i \u0026amp; = \\beta_0 + \\beta_1 \\text{treatment}_i, \\end{align*} \\]\nwhere \\(\\text{treatment}\\) is a dummy variable coded 0 = control 1 = treatment and varies across cases indexed by \\(i\\). In this setup, \\(\\beta_0\\) is the estimate for \\(\\mu_c\\) and \\(\\beta_1\\) is the estimate of the difference between condition means, \\(\\mu_t - \\mu_c\\). Thus our focal parameter, the one we care about the most in our power analysis, will be \\(\\beta_1\\).\nWithin the frequentist paradigm, we typically compare these hypotheses using a \\(p\\)-value for \\(H_0\\) with the critical value, \\(\\alpha\\), set to .05. Thus, power is the probability we’ll have \\(p \u0026lt; .05\\) when it is indeed the case that \\(\\mu_c \\neq \\mu_t\\). We won’t be computing \\(p\\)-values in this project, but we will use 95% intervals. Recall that the result of a Bayesian analysis, the posterior distribution, is the probability of the parameters, given the data \\(p (\\theta | \\text{data})\\). With our 95% Bayesian credible intervals, we’ll be able to describe the parameter space over which our estimate of \\(\\mu_t - \\mu_c\\) is 95% probable. That is, for our power analysis, we’re interested in the probability our 95% credible intervals for \\(\\beta_1\\) contain zero within their bounds when we know a priori \\(\\mu_c \\neq \\mu_t\\).\nThe reason we know \\(\\mu_c \\neq \\mu_t\\) is because we’ll be simulating the data that way. What our power analysis will help us determine is how many cases we’ll need to achieve a predetermined level of power. The conventional threshold is .8.\nDry run number 1. To make this all concrete, let’s start with a simple example. We’ll simulate a single set of data, fit a Bayesian regression model, and examine the results for the critical parameter \\(\\beta_1\\). For the sake of simplicity, let’s keep our two groups, treatment and control, the same size. We’ll start with \\(n = 50\\) for each.\nn \u0026lt;- 50 We already decided above that\n\\[ \\begin{align*} y_{i, c} \u0026amp; \\sim \\operatorname{Normal}(0, 1) \\text{ and}\\\\ y_{i, t} \u0026amp; \\sim \\operatorname{Normal}(0.5, 1). \\end{align*} \\]\nHere’s how we might simulate data along those lines.\nset.seed(1) d \u0026lt;- tibble(group = rep(c(\u0026quot;control\u0026quot;, \u0026quot;treatment\u0026quot;), each = n)) %\u0026gt;% mutate(treatment = ifelse(group == \u0026quot;control\u0026quot;, 0, 1), y = ifelse(group == \u0026quot;control\u0026quot;, rnorm(n, mean = mu_c, sd = 1), rnorm(n, mean = mu_t, sd = 1))) glimpse(d) ## Rows: 100 ## Columns: 3 ## $ group \u0026lt;chr\u0026gt; \u0026quot;control\u0026quot;, \u0026quot;control\u0026quot;, \u0026quot;control\u0026quot;, \u0026quot;control\u0026quot;, \u0026quot;control\u0026quot;, \u0026quot;con… ## $ treatment \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ y \u0026lt;dbl\u0026gt; -0.62645381, 0.18364332, -0.83562861, 1.59528080, 0.3295077… In case it wasn’t clear, the two variables group and treatment are redundant. Whereas the former is composed of names, the latter is the dummy-variable equivalent (i.e., control = 0, treatment = 1). The main event was how we used the rnorm() function to simulate the normally-distributed values for y.\nBefore we fit our model, we need to decide on priors. To give us ideas, here are the brms defaults for our model and data.\nget_prior(data = d, family = gaussian, y ~ 0 + Intercept + treatment) ## prior class coef group resp dpar nlpar bound ## 1 b ## 2 b Intercept ## 3 b treatment ## 4 student_t(3, 0, 2.5) sigma A few things: Notice that here we’re using the 0 + Intercept syntax. This is because brms handles the priors for the default intercept under the presumption you’ve mean-centered all your predictor variables. However, since our treatment variable is a dummy, that assumption won’t fly. The 0 + Intercept allows us to treat the model intercept as just another \\(\\beta\\) parameter, which makes no assumptions about centering. Along those lines, you’ll notice brms currently defaults to flat priors for the \\(\\beta\\) parameters (i.e., those for which class = b). And finally, the default prior on \\(\\sigma\\) is moderately wide student_t(3, 0, 2.5). By default, brms also sets the left bounds for \\(\\sigma\\) parameters at zero, making that a folded-\\(t\\) distribution. If you’re confused by these details, spend some time with the brms reference manual (Bürkner, 2020b), particularly the brm and brmsformula sections.\nIn this project, we’ll be primarily using two kinds of priors: default flat priors and weakly-regularizing priors. Hopefully flat priors are self-explanatory. They let the likelihood (data) dominate the posterior and tend to produce results similar to those from frequentist estimators.\nAs for weakly-regularizing priors, McElreath covered them in his text. They’re mentioned a bit in the Stan team’s Prior Choice Recommendations wiki, and you can learn even more from Gelman, Simpson, and Betancourt’s (2017) The prior can only be understood in the context of the likelihood. These priors aren’t strongly informative and aren’t really representative of our research hypotheses. But they’re not as absurd as flat priors, either. Rather, with just a little bit of knowledge about the data, these priors are set to keep the MCMC chains on target. Since our y variable has a mean near zero and a standard deviation near one and since our sole predictor, treatment is a dummy, setting \\(\\operatorname{Normal}(0, 2)\\) as the prior for both \\(\\beta\\) parameters might be a good place to start. The prior is permissive enough that it will let likelihood dominate the posterior, but it also rules out ridiculous parts of the parameter space (e.g., a standardized mean difference of 20, an intercept of -93). And since we know the data are on the unit scale, we might just center our folded-Student-\\(t\\) prior on one and add a gentle scale setting of one.\nFeel free to disagree and use your own priors. The great thing about priors is that they can be proposed, defended, criticized and improved. The point is to settle on the priors you can defend with written reasons. Select ones you’d feel comfortable defending to a skeptical reviewer.\nHere’s how we might fit the model.\nfit \u0026lt;- brm(data = d, family = gaussian, y ~ 0 + Intercept + treatment, prior = c(prior(normal(0, 2), class = b), prior(student_t(3, 1, 1), class = sigma)), seed = 1) Before we look at the summary, we might check the chains in a trace plot. We’re looking for “stuck” chains that don’t appear to come from a normal distribution (the chains are a profile-like view rather than histogram, allowing for inspection of dependence between samples).\nplot(fit) Yep, the chains all look good. Here’s the parameter summary.\nprint(fit) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 0 + Intercept + treatment ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.11 0.13 -0.15 0.36 1.00 2159 2192 ## treatment 0.51 0.18 0.15 0.87 1.00 2230 2445 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.91 0.07 0.80 1.06 1.00 2369 2397 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The 95% credible intervals for our \\(\\beta_1\\) parameter, termed treatment in the output, are well above zero.\nAnother way to look at the model summary is with the handy broom::tidy() function.\ntidy(fit, prob = .95) ## term estimate std.error lower upper ## 1 b_Intercept 0.1052398 0.12859993 -0.1523971 0.3574843 ## 2 b_treatment 0.5129811 0.18274118 0.1533267 0.8742095 ## 3 sigma 0.9130187 0.06601476 0.7987716 1.0565980 ## 4 lp__ -136.3229864 1.21403488 -139.3761462 -134.9182722 It’s important to keep in mind that by default, tidy() returns 90% intervals for brm() fit objects. To get the conventional 95% intervals, you’ll need to specify prob = .95. The intervals are presented for each parameter in the lower and upper columns. Once we start simulating in bulk, the tidy() function will come in handy. You’ll see.\n You can reuse a fit. Especially with simple models like this, a lot of the time we spend waiting for brms::brm() to return the model is wrapped up in compilation. This is because brms is a collection of user-friendly functions designed to fit models with Stan (Stan Development Team, 2020a, 2020b, 2020c). With each new model, brm() translates your model into Stan code, which then gets translated to C++ and is compiled afterwards (see here or here). However, we can use the update() function to update a previously-compiled fit object with new data. This cuts out the compilation time and allows us to get directly to sampling. Here’s how to do it.\n# set a new seed set.seed(2) # simulate new data based on that new seed d \u0026lt;- tibble(group = rep(c(\u0026quot;control\u0026quot;, \u0026quot;treatment\u0026quot;), each = n)) %\u0026gt;% mutate(treatment = ifelse(group == \u0026quot;control\u0026quot;, 0, 1), y = ifelse(group == \u0026quot;control\u0026quot;, rnorm(n, mean = mu_c, sd = 1), rnorm(n, mean = mu_t, sd = 1))) updated_fit \u0026lt;- update(fit, newdata = d, seed = 2) Behold the tidy() summary of our updated model.\ntidy(updated_fit, prob = .95) ## term estimate std.error lower upper ## 1 b_Intercept 0.06781182 0.1702447 -0.2730871 0.3946365 ## 2 b_treatment 0.29955816 0.2406686 -0.1696387 0.7755439 ## 3 sigma 1.17774894 0.0869274 1.0235238 1.3687243 ## 4 lp__ -161.35925839 1.3208628 -164.6955451 -159.8811234 Well how about that? In this case, our 95% credible intervals for \\(\\beta_1\\) did include zero within their bounds. Though the posterior mean, 0.30, is still well away from zero, here we’d fail to reject \\(H_0\\) at the conventional level. This is why we simulate.\nTo recap, we’ve\ndetermined our primary data type, cast our research question in terms of a regression model, identified the parameter of interest, settled on defensible priors, picked an initial sample size, fit an initial model with a single simulated data set, and practiced reusing that fit with update().  We’re more than half way there! It’s time to do our first power simulation.\n  Simulate to determine power. In this post, we’ll play with three ways to do a Bayesian power simulation. They’ll all be similar, but hopefully you’ll learn a bit as we transition from one to the next. Though if you’re impatient and all this seems remedial, you could probably just skip down to the final example, Version 3.\nVersion 1: Let’s introduce making a custom model-fitting function. For our power analysis, we’ll need to simulate a large number of data sets, each of which we’ll fit a model to. Here we’ll make a custom function, sim_d(), that will simulate new data sets just like before. Our function will have two parameters: we’ll set our seeds with seed and determine how many cases we’d like per group with n.\nsim_d \u0026lt;- function(seed, n) { mu_t \u0026lt;- .5 mu_c \u0026lt;- 0 set.seed(seed) tibble(group = rep(c(\u0026quot;control\u0026quot;, \u0026quot;treatment\u0026quot;), each = n)) %\u0026gt;% mutate(treatment = ifelse(group == \u0026quot;control\u0026quot;, 0, 1), y = ifelse(group == \u0026quot;control\u0026quot;, rnorm(n, mean = mu_c, sd = 1), rnorm(n, mean = mu_t, sd = 1))) } Here’s a quick example of how our function works.\nsim_d(seed = 123, n = 2) ## # A tibble: 4 x 3 ## group treatment y ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 control 0 -0.560 ## 2 control 0 -0.230 ## 3 treatment 1 2.06 ## 4 treatment 1 0.571 Now we’re ready to get down to business. We’re going to be saving our simulation results in a nested data frame, s. Initially, s will have one column of seed values. These will serve a dual function. First, they are the values we’ll be feeding into the seed argument of our custom data-generating function, sim_d(). Second, since the seed values serially increase, they also stand in as iteration indexes.\nFor our second step, we add the data simulations and save them in a nested column, d. In the first argument of the purrr::map() function, we indicate we want to iterate over the values in seed. In the second argument, we indicate we want to serially plug those seed values into the first argument within the sim_d() function. That argument, recall, is the well-named seed argument. With the final argument in map(), n = 50, we hard code 50 into the n argument of sim_d().\nFor the third step, we expand our purrr::map() skills from above to purrr::map2(), which allows us to iteratively insert two arguments into a function. Within this paradigm, the two arguments are generically termed .x and .y. Thus our approach will be .x = d, .y = seed. For our function, we specify ~update(fit, newdata = .x, seed = .y). Thus we’ll be iteratively inserting our simulated d data into the newdata argument and will be simultaneously inserting our seed values into the seed argument.\nAlso notice that the number of iterations we’ll be working with is determined by the number of rows in the seed column. We are defining that number as n_sim. Since this is just a blog post, I’m going to take it easy and use 100. But if this was a real power analysis for one of your projects, something like 1,000 would be better.\nFinally, you don’t have to do this, but I’m timing my simulation by saving Sys.time() values at the beginning and end of the simulation.\n# how many simulations would you like? n_sim \u0026lt;- 100 # this will help us track time t1 \u0026lt;- Sys.time() # here\u0026#39;s the main event! s \u0026lt;- tibble(seed = 1:n_sim) %\u0026gt;% mutate(d = map(seed, sim_d, n = 50)) %\u0026gt;% mutate(fit = map2(d, seed, ~update(fit, newdata = .x, seed = .y))) t2 \u0026lt;- Sys.time() The entire simulation took just over a minute on my new laptop.\nt2 - t1 ## Time difference of 1.024339 mins Your mileage may vary.\nLet’s take a look at what we’ve done.\nhead(s) ## # A tibble: 6 x 3 ## seed d fit ## \u0026lt;int\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 1 \u0026lt;tibble [100 × 3]\u0026gt; \u0026lt;brmsfit\u0026gt; ## 2 2 \u0026lt;tibble [100 × 3]\u0026gt; \u0026lt;brmsfit\u0026gt; ## 3 3 \u0026lt;tibble [100 × 3]\u0026gt; \u0026lt;brmsfit\u0026gt; ## 4 4 \u0026lt;tibble [100 × 3]\u0026gt; \u0026lt;brmsfit\u0026gt; ## 5 5 \u0026lt;tibble [100 × 3]\u0026gt; \u0026lt;brmsfit\u0026gt; ## 6 6 \u0026lt;tibble [100 × 3]\u0026gt; \u0026lt;brmsfit\u0026gt; In our 100-row nested tibble, we have all our simulated data sets in the d column and all of our brms fit objects nested in the fit column. Next we’ll use broom::tidy() and a little wrangling to extract the parameter of interest, b_treatment (i.e., \\(\\beta_1\\)), from each simulation.\ns %\u0026gt;% mutate(treatment = map(fit, tidy, prob = .95)) %\u0026gt;% unnest(treatment) %\u0026gt;% filter(term == \u0026quot;b_treatment\u0026quot;) %\u0026gt;% head() ## # A tibble: 6 x 8 ## seed d fit term estimate std.error lower upper ## \u0026lt;int\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 \u0026lt;tibble [100 × 3]\u0026gt; \u0026lt;brmsfit\u0026gt; b_treatme… 0.513 0.183 0.153 0.874 ## 2 2 \u0026lt;tibble [100 × 3]\u0026gt; \u0026lt;brmsfit\u0026gt; b_treatme… 0.300 0.241 -0.170 0.776 ## 3 3 \u0026lt;tibble [100 × 3]\u0026gt; \u0026lt;brmsfit\u0026gt; b_treatme… 0.640 0.174 0.297 0.990 ## 4 4 \u0026lt;tibble [100 × 3]\u0026gt; \u0026lt;brmsfit\u0026gt; b_treatme… 0.225 0.183 -0.135 0.583 ## 5 5 \u0026lt;tibble [100 × 3]\u0026gt; \u0026lt;brmsfit\u0026gt; b_treatme… 0.432 0.194 0.0552 0.810 ## 6 6 \u0026lt;tibble [100 × 3]\u0026gt; \u0026lt;brmsfit\u0026gt; b_treatme… 0.305 0.209 -0.101 0.714 As an aside, I know I’m moving kinda fast with all this wacky purrr::map()/purrr::map2() stuff. If you’re new to using the tidyverse for iterating and saving the results in nested data structures, I recommend fixing an adult beverage and cozying up with Hadley Wickham’s presentation, Managing many models. And if you really hate it, both Kruschke and McElreath texts contain many examples of how to iterate in a more base R sort of way.\nAnyway, here’s what those 100 \\(\\beta_1\\) summaries look like in bulk.\ns %\u0026gt;% mutate(treatment = map(fit, tidy, prob = .95)) %\u0026gt;% unnest(treatment) %\u0026gt;% filter(term == \u0026quot;b_treatment\u0026quot;) %\u0026gt;% ggplot(aes(x = seed, y = estimate, ymin = lower, ymax = upper)) + geom_hline(yintercept = c(0, .5), color = \u0026quot;white\u0026quot;) + geom_pointrange(fatten = 1/2) + labs(x = \u0026quot;seed (i.e., simulation index)\u0026quot;, y = expression(beta[1])) The horizontal lines show the idealized effect size (0.5) and the null hypothesis (0). Already, it’s apparent that most of our intervals indicate there’s more than a 95% probability the null hypothesis is not credible. Several do. Here’s how to quantify that.\ns %\u0026gt;% mutate(treatment = map(fit, tidy, prob = .95)) %\u0026gt;% unnest(treatment) %\u0026gt;% filter(term == \u0026quot;b_treatment\u0026quot;) %\u0026gt;% mutate(check = ifelse(lower \u0026gt; 0, 1, 0)) %\u0026gt;% summarise(power = mean(check)) ## # A tibble: 1 x 1 ## power ## \u0026lt;dbl\u0026gt; ## 1 0.67 With the second mutate() line, we used a logical statement within ifelse() to code all instances where the lower limit of the 95% interval was greater than 0 as a 1, with the rest as 0. That left us with a vector of 1’s and 0’s, which we saved as check. In the summarise() line, we took the mean of that column, which returned our Bayesian power estimate.\nThat is, in 67 of our 100 simulations, an \\(n = 50\\) per group was enough to produce a 95% Bayesian credible interval that did not straddle 0.\nI should probably point out that a 95% interval for which upper \u0026lt; 0 would have also been consistent with the alternative hypothesis of \\(\\mu_c \\neq \\mu_t\\). However, I didn’t bother to work that option into the definition of our check variable because I knew from the outset that that would be a highly unlikely result. But if you’d like to work more rigor into your checks, by all means do.\nAnd if you’ve gotten this far and have been following along with code of your own, congratulations! You did it! You’ve estimated the power of a Bayesian model with a given \\(n\\). Now let’s refine our approach.\n Version 2: We might should be more careful with memory. I really like it that our s object contains all our brm() fits. It makes it really handy to do global diagnostics like making sure our \\(\\widehat R\\) values are all within a respectable range.\ns %\u0026gt;% mutate(rhat = map(fit, rhat)) %\u0026gt;% unnest(rhat) %\u0026gt;% ggplot(aes(x = rhat)) + geom_histogram(bins = 20) Man those \\(\\widehat R\\) values look sweet. It’s great to have a workflow that lets you check them. But holding on to all those fits can take up a lot of memory. If the only thing you’re interested in are the parameter summaries, a better approach might be to do the model refitting and parameter extraction in one step. That way you only save the parameter summaries. Here’s how you might do that.\nt3 \u0026lt;- Sys.time() s2 \u0026lt;- tibble(seed = 1:n_sim) %\u0026gt;% mutate(d = map(seed, sim_d, n = 50)) %\u0026gt;% # here\u0026#39;s the new part mutate(tidy = map2(d, seed, ~update(fit, newdata = .x, seed = .y) %\u0026gt;% tidy(prob = .95) %\u0026gt;% filter(term == \u0026quot;b_treatment\u0026quot;))) t4 \u0026lt;- Sys.time() Like before, this only about a minute.\nt4 - t3 ## Time difference of 59.99365 secs As a point of comparison, here are the sizes of the results from our first approach to those from the second.\nobject.size(s) ## 79615920 bytes object.size(s2) ## 502320 bytes That’s a big difference. Hopefully you get the idea. With more complicated models and 10+ times the number of simulations, size will eventually matter.\nAnyway, here are the results.\ns2 %\u0026gt;% unnest(tidy) %\u0026gt;% ggplot(aes(x = seed, y = estimate, ymin = lower, ymax = upper)) + geom_hline(yintercept = c(0, .5), color = \u0026quot;white\u0026quot;) + geom_pointrange(fatten = 1/2) + labs(x = \u0026quot;seed (i.e., simulation index)\u0026quot;, y = expression(beta[1])) Same parameter summaries, lower memory burden.\n Version 3: Still talking about memory, we can be even stingier. So far, both of our simulation attempts resulted in our saving the simulated data sets. It’s a really nice option if you ever want to go back and take a look at those simulated data. For example, you might want to inspect a random subset of the data simulations with box plots.\nset.seed(1) s2 %\u0026gt;% sample_n(12) %\u0026gt;% unnest(d) %\u0026gt;% ggplot(aes(x = group, y = y)) + geom_boxplot(aes(fill = group), alpha = 2/3, show.legend = F) + scale_fill_manual(values = c(\u0026quot;grey25\u0026quot;, \u0026quot;blue2\u0026quot;)) + xlab(NULL) + facet_wrap(~seed) In this case, it’s no big deal if we keep the data around or not. The data sets are fairly small and we’re only simulating 100 of them. But in cases where the data are larger and you’re doing thousands of simulations, keeping the data could become a memory drain.\nIf you’re willing to forgo the luxury of inspecting your data simulations, it might make sense to run our power analysis in a way that avoids saving them. One way to do so would be to just wrap the data simulation and model fitting all in one function. We’ll call it sim_d_and_fit().\nsim_d_and_fit \u0026lt;- function(seed, n) { mu_t \u0026lt;- .5 mu_c \u0026lt;- 0 set.seed(seed) d \u0026lt;- tibble(group = rep(c(\u0026quot;control\u0026quot;, \u0026quot;treatment\u0026quot;), each = n)) %\u0026gt;% mutate(treatment = ifelse(group == \u0026quot;control\u0026quot;, 0, 1), y = ifelse(group == \u0026quot;control\u0026quot;, rnorm(n, mean = mu_c, sd = 1), rnorm(n, mean = mu_t, sd = 1))) update(fit, newdata = d, seed = seed) %\u0026gt;% tidy(prob = .95) %\u0026gt;% filter(term == \u0026quot;b_treatment\u0026quot;) } Now iterate 100 times once more.\nt5 \u0026lt;- Sys.time() s3 \u0026lt;- tibble(seed = 1:n_sim) %\u0026gt;% mutate(tidy = map(seed, sim_d_and_fit, n = 50)) %\u0026gt;% unnest(tidy) t6 \u0026lt;- Sys.time() That was pretty quick.\nt6 - t5 ## Time difference of 58.62384 secs Here’s what it returned.\nhead(s3) ## # A tibble: 6 x 6 ## seed term estimate std.error lower upper ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 b_treatment 0.513 0.183 0.153 0.874 ## 2 2 b_treatment 0.300 0.241 -0.170 0.776 ## 3 3 b_treatment 0.640 0.174 0.297 0.990 ## 4 4 b_treatment 0.225 0.183 -0.135 0.583 ## 5 5 b_treatment 0.432 0.194 0.0552 0.810 ## 6 6 b_treatment 0.305 0.209 -0.101 0.714 By wrapping our data simulation, model fitting, and parameter extraction steps all in one function, we simplified the output such that we’re no longer holding on to the data simulations or the brms fit objects. We just have the parameter summaries and the seed, making the product even smaller.\ntibble(object = c(\u0026quot;s\u0026quot;, \u0026quot;s2\u0026quot;, \u0026quot;s3\u0026quot;)) %\u0026gt;% mutate(bytes = map_dbl(object, ~get(.) %\u0026gt;% object.size())) ## # A tibble: 3 x 2 ## object bytes ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 s 79615920 ## 2 s2 502320 ## 3 s3 5944 But the primary results are the same.\ns3 %\u0026gt;% ggplot(aes(x = seed, y = estimate, ymin = lower, ymax = upper)) + geom_hline(yintercept = c(0, .5), color = \u0026quot;white\u0026quot;) + geom_pointrange(fatten = 1/2) + labs(x = \u0026quot;seed (i.e., simulation index)\u0026quot;, y = expression(beta[1])) We still get the same power estimate, too.\ns3 %\u0026gt;% mutate(check = ifelse(lower \u0026gt; 0, 1, 0)) %\u0026gt;% summarise(power = mean(check)) ## # A tibble: 1 x 1 ## power ## \u0026lt;dbl\u0026gt; ## 1 0.67   Next steps But my goal was to figure out what \\(n\\) will get me power of .8 or more!, you say. Fair enough. Try increasing n to 65 or something.\nIf that seems unsatisfying, welcome to the world of simulation. Since our Bayesian models are complicated, we don’t have the luxury of plugging a few values into some quick power formula. Just as simulation is an iterative process, determining on the right values to simulate over might well be an iterative process, too.\n Wrap-up Anyway, that’s the essence of the brms/tidyverse workflow for Bayesian power analysis. You follow these steps:\nDetermine your primary data type. Determine your primary regression model and parameter(s) of interest. Pick defensible priors for all parameters–the kinds of priors you intend to use once you have the real data in hand. Select a sample size. Fit an initial model and save the fit object. Simulate some large number of data sets all following your prechosen form and use the update() function to iteratively fit the models. Extract the parameter(s) of interest. Summarize.  In addition, we played with a few approaches based on logistical concerns like memory. In the next post, part II, we’ll see how the precision-oriented approach to sample-size planning is a viable alternative to power focused on rejecting null hypotheses.\n I had help. Special thanks to Christopher Peters (@statwonk) for the helpful edits and suggestions.\n Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.3 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] broom_0.5.5 brms_2.13.0 Rcpp_1.0.5 forcats_0.5.0 ## [5] stringr_1.4.0 dplyr_1.0.1 purrr_0.3.4 readr_1.3.1 ## [9] tidyr_1.1.1 tibble_3.0.3 ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] TH.data_1.0-10 colorspace_1.4-1 ellipsis_0.3.1 ## [4] ggridges_0.5.2 rsconnect_0.8.16 estimability_1.3 ## [7] markdown_1.1 base64enc_0.1-3 fs_1.4.1 ## [10] rstudioapi_0.11 farver_2.0.3 rstan_2.19.3 ## [13] DT_0.13 fansi_0.4.1 mvtnorm_1.1-0 ## [16] lubridate_1.7.8 xml2_1.3.1 codetools_0.2-16 ## [19] splines_3.6.3 bridgesampling_1.0-0 knitr_1.28 ## [22] shinythemes_1.1.2 bayesplot_1.7.1 jsonlite_1.7.0 ## [25] dbplyr_1.4.2 shiny_1.5.0 compiler_3.6.3 ## [28] httr_1.4.1 emmeans_1.4.5 backports_1.1.8 ## [31] assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [34] cli_2.0.2 later_1.1.0.1 htmltools_0.5.0 ## [37] prettyunits_1.1.1 tools_3.6.3 igraph_1.2.5 ## [40] coda_0.19-3 gtable_0.3.0 glue_1.4.1 ## [43] reshape2_1.4.4 cellranger_1.1.0 vctrs_0.3.2 ## [46] nlme_3.1-144 blogdown_0.18 crosstalk_1.1.0.1 ## [49] xfun_0.13 ps_1.3.4 rvest_0.3.5 ## [52] mime_0.9 miniUI_0.1.1.1 lifecycle_0.2.0 ## [55] gtools_3.8.2 MASS_7.3-51.5 zoo_1.8-7 ## [58] scales_1.1.1 colourpicker_1.0 hms_0.5.3 ## [61] promises_1.1.1 Brobdingnag_1.2-6 sandwich_2.5-1 ## [64] parallel_3.6.3 inline_0.3.15 shinystan_2.5.0 ## [67] yaml_2.2.1 gridExtra_2.3 loo_2.2.0 ## [70] StanHeaders_2.21.0-1 stringi_1.4.6 dygraphs_1.1.1.6 ## [73] pkgbuild_1.1.0 rlang_0.4.7 pkgconfig_2.0.3 ## [76] matrixStats_0.56.0 evaluate_0.14 lattice_0.20-38 ## [79] labeling_0.3 rstantools_2.0.0 htmlwidgets_1.5.1 ## [82] tidyselect_1.1.0 processx_3.4.3 plyr_1.8.6 ## [85] magrittr_1.5 bookdown_0.18 R6_2.4.1 ## [88] generics_0.0.2 multcomp_1.4-13 DBI_1.1.0 ## [91] pillar_1.4.6 haven_2.2.0 withr_2.2.0 ## [94] xts_0.12-0 survival_3.1-12 abind_1.4-5 ## [97] modelr_0.1.6 crayon_1.3.4 utf8_1.1.4 ## [100] rmarkdown_2.1 grid_3.6.3 readxl_1.3.1 ## [103] callr_3.4.3 threejs_0.3.3 reprex_0.3.0 ## [106] digest_0.6.25 xtable_1.8-4 httpuv_1.5.4 ## [109] stats4_3.6.3 munsell_0.5.0 shinyjs_1.1 # for the hard-core scrollers: # if you increase n to 65, the power becomes about .84 n_sim \u0026lt;- 100 t7 \u0026lt;- Sys.time() s4 \u0026lt;- tibble(seed = 1:n_sim) %\u0026gt;% mutate(tidy = map(seed, sim_d_and_fit, n = 65)) t8 \u0026lt;- Sys.time() t8 - t7 object.size(s4) s4 %\u0026gt;% unnest(tidy) %\u0026gt;% mutate(check = ifelse(lower \u0026gt; 0, 1, 0)) %\u0026gt;% summarise(power = mean(check))  References Bürkner, P.-C. (2017). brms: An R package for Bayesian multilevel models using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01\n Bürkner, P.-C. (2018). Advanced Bayesian multilevel modeling with the R package brms. The R Journal, 10(1), 395–411. https://doi.org/10.32614/RJ-2018-017\n Bürkner, P.-C. (2020a). brms: Bayesian regression models using ’Stan’. https://CRAN.R-project.org/package=brms\n Bürkner, P.-C. (2020b). brms reference manual, Version 2.12.0. https://CRAN.R-project.org/package=brms/brms.pdf\n Cohen, J. (1988). Statistical power analysis for the behavioral sciences. L. Erlbaum Associates. https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467\n Gelman, A., Simpson, D., \u0026amp; Betancourt, M. (2017). The prior can often only be understood in the context of the likelihood. Entropy, 19(10), 555. https://doi.org/10.3390/e19100555\n Grolemund, G., \u0026amp; Wickham, H. (2017). R for data science. O’Reilly. https://r4ds.had.co.nz\n Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n Kurz, A. S. (2020a). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.1.0). https://doi.org/10.5281/zenodo.3693202\n Kurz, A. S. (2020b). Doing Bayesian data analysis in brms and the tidyverse (version 0.2.0). https://bookdown.org/content/3686/\n Kurz, A. S. (2020c). Statistical rethinking with brms, ggplot2, and the tidyverse: Second edition (version 0.0.2). https://bookdown.org/content/4857/\n Maxwell, S. E., Kelley, K., \u0026amp; Rausch, J. R. (2008). Sample size planning for statistical power and accuracy in parameter estimation. Annual Review of Psychology, 59(1), 537–563. https://doi.org/10.1146/annurev.psych.59.103006.093735\n McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/\n Peng, R. D. (2019). R programming for data science. https://bookdown.org/rdpeng/rprogdatascience/\n R Core Team. (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/\n Stan Development Team. (2020a). RStan: The R interface to Stan. https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html\n Stan Development Team. (2020b). Stan reference manual, Version 2.23. https://mc-stan.org/docs/2_23/reference-manual/\n Stan Development Team. (2020c). Stan user’s guide, Version 2.23. https://mc-stan.org/docs/2_23/stan-users-guide/index.html\n Wickham, H. (2019). tidyverse: Easily install and load the ’tidyverse’. https://CRAN.R-project.org/package=tidyverse\n Wickham, H. (2020). The tidyverse style guide. https://style.tidyverse.org/\n Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686\n   ","date":1563408000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563408000,"objectID":"a8d27ac122fa9b9cde0298fb36621122","permalink":"/post/bayesian-power-analysis-part-i/","publishdate":"2019-07-18T00:00:00Z","relpermalink":"/post/bayesian-power-analysis-part-i/","section":"post","summary":"Version 1.0.1  tl;dr If you’d like to learn how to do Bayesian power calculations using brms, stick around for this multi-part blog series. Here with part I, we’ll set the foundation.\n Power is hard, especially for Bayesians. Many journals, funding agencies, and dissertation committees require power calculations for your primary analyses. Frequentists have a variety of tools available to perform these calculations (e.g., here). Bayesians, however, have a more difficult time of it.","tags":["Bayesian","brms","power","R","tutorial","tidyverse"],"title":"Bayesian power analysis: Part I. Prepare to reject $H_0$ with simulation.","type":"post"},{"authors":[],"categories":[],"content":" A colleague reached out to me earlier this week with a plotting question. They had fit a series of Bayesian models, all containing a common parameter of interest. They knew how to plot their focal parameter one model at a time, but were stumped on how to combine the plots across models into a seamless whole. It reminded me a bit of this gif\nwhich I originally got from Jenny Bryan’s great talk, Behind every great plot there’s a great deal of wrangling.\nThe goal of this post is to provide solutions. We’ll practice a few different ways you can combine the posterior samples from your Bayesian models into a single plot. As usual, we’ll be fitting our models with brms, wrangling with packages from the tidyverse, and getting a little help from the tidybayes package.\nI make assumptions. For this post, I’m presuming you are familiar Bayesian regression using brms. I’m also assuming you’ve coded using some of the foundational functions from the tidyverse. If you’d like to firm up your foundations a bit, check out these resources.\n To learn about Bayesian regression, I recommend the introductory text books by either McElreath (here) or Kruschke (here). Both authors host blogs (here and here, respectively). If you go with McElreath, do check out his online lectures and my project translating his text to brms and tidyverse code. I’m working on a similar project for Kruschke’s text, but it still has a ways to go before I release it in full. For even more brms-related resources, you can find vignettes and documentation here. For tidyverse introductions, your best bets are R4DS and The tidyverse style guide.   Same parameter, different models Let’s load our primary statistical packages.\nlibrary(tidyverse) library(brms) library(tidybayes) Simulate \\(n = 150\\) draws from the standard normal distribution.\nn \u0026lt;- 150 set.seed(1) d \u0026lt;- tibble(y = rnorm(n, mean = 0, sd = 1)) head(d) ## # A tibble: 6 x 1 ## y ## \u0026lt;dbl\u0026gt; ## 1 -0.626 ## 2 0.184 ## 3 -0.836 ## 4 1.60 ## 5 0.330 ## 6 -0.820 Here we’ll fit three intercept-only models for y. Each will follow the form\n\\[ \\begin{align*} y_i \u0026amp; \\sim \\text{Normal} (\\mu, \\sigma) \\\\ \\mu \u0026amp; = \\beta_0 \\\\ \\beta_0 \u0026amp; \\sim \\text{Normal} (0, x) \\\\ \\sigma \u0026amp; \\sim \\text{Student-t}(3, 0, 10) \\end{align*} \\]\nwhere \\(\\beta_0\\) is the unconditional intercept (i.e., an intercept not conditioned on any predictors). We will be fitting three alternative models. All will have the same prior for \\(\\sigma\\), \\(\\text{Student-t}(3, 0, 10)\\), which is the brms default in this case. [If you’d like to check, use the get_prior() function.] The only way the models will differ is by their prior on the intercept \\(\\beta_0\\). By model, those priors will be\n fit1: \\(\\beta_0 \\sim \\text{Normal} (0, 10)\\), fit2: \\(\\beta_0 \\sim \\text{Normal} (0, 1)\\), and fit3: \\(\\beta_0 \\sim \\text{Normal} (0, 0.1)\\).  So if you were wondering, the \\(x\\) in the \\(\\beta_0 \\sim \\text{Normal} (0, x)\\) line, above, was a stand-in for the varying hyperparameter.\nHere we fit the models in bulk.\nfit1 \u0026lt;- brm(data = d, family = gaussian, y ~ 1, prior(normal(0, 10), class = Intercept), seed = 1) fit2 \u0026lt;- update(fit1, prior = prior(normal(0, 1), class = Intercept), seed = 1) fit3 \u0026lt;- update(fit1, prior = prior(normal(0, 0.1), class = Intercept), seed = 1) Normally we’d use plot() to make sure the chains look good and then use something like print() or posterior_summary() to summarize the models’ results. I’ve checked and they’re all fine. For the sake of space, let’s press forward.\nIf you were going to plot the results of an individual fit using something like the tidybayes::geom_halfeyeh() function, the next step would be extracting the posterior draws. Here we’ll do so with the brms::posterior_samples() function.\npost1 \u0026lt;- posterior_samples(fit1) post2 \u0026lt;- posterior_samples(fit2) post3 \u0026lt;- posterior_samples(fit3) Focusing on fit1, here’s how we’d plot the results for the intercept \\(\\beta_0\\).\n# this part is unnecessary; it just adjusts some theme defaults to my liking theme_set(theme_gray() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank())) # plot! post1 %\u0026gt;% ggplot(aes(x = b_Intercept, y = 0)) + geom_halfeyeh() + scale_y_continuous(NULL, breaks = NULL) But how might we get the posterior draws from all three fits into one plot? The answer is by somehow combining the posterior draws from each into one data frame. There are many ways to do this. Perhaps the simplest is with the bind_rows() function.\nposts \u0026lt;- bind_rows( post1, post2, post3 ) %\u0026gt;% mutate(prior = str_c(\u0026quot;normal(0, \u0026quot;, c(10, 1, 0.1), \u0026quot;)\u0026quot;) %\u0026gt;% rep(., each = 4000)) head(posts) ## b_Intercept sigma lp__ prior ## 1 -0.052176422 0.8568091 -204.1751 normal(0, 10) ## 2 0.124990457 0.8983495 -204.1663 normal(0, 10) ## 3 -0.006294612 0.9475288 -203.5528 normal(0, 10) ## 4 0.060904410 0.9410559 -203.5319 normal(0, 10) ## 5 0.177594575 0.9457762 -205.4991 normal(0, 10) ## 6 0.134879573 0.9267331 -204.3765 normal(0, 10) The bind_rows() function worked well, here, because all three post objects had the same number of columns of the same names. So we just stacked them three high. That is, we went from three data objects of 4,000 rows and 3 columns to one data object with 12,000 rows and 3 columns. But with the mutate() function we did add a fourth column, prior, that indexed which model each row came from. Now our data are ready, we can plot.\nposts %\u0026gt;% ggplot(aes(x = b_Intercept, y = prior)) + geom_halfeyeh() Our plot arrangement made it easy to compare the results of tightening the prior on \\(\\beta_0\\); the narrower the prior, the narrower the posterior.\n What if my posterior_samples() aren’t of the same dimensions across models? For the next examples, we need new data. Here we’ll simulate three predictors–x1, x2, and x3. We then simulate our criterion y as a linear additive function of those predictors.\nset.seed(1) d \u0026lt;- tibble(x1 = rnorm(n, mean = 0, sd = 1), x2 = rnorm(n, mean = 0, sd = 1), x3 = rnorm(n, mean = 0, sd = 1)) %\u0026gt;% mutate(y = rnorm(n, mean = 0 + x1 * 0 + x2 * 0.2 + x3 * -0.4)) head(d) ## # A tibble: 6 x 4 ## x1 x2 x3 y ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -0.626 0.450 0.894 0.694 ## 2 0.184 -0.0186 -1.05 -0.189 ## 3 -0.836 -0.318 1.97 -1.61 ## 4 1.60 -0.929 -0.384 -1.59 ## 5 0.330 -1.49 1.65 -2.41 ## 6 -0.820 -1.08 1.51 -0.764 We are going to work with these data in two ways. For the first example, we’ll fit a series of univariable models following the same basic form, but each with a different predictor. For the second example, we’ll fit a series of multivariable models with various combinations of the predictors. Each requires its own approach.\nSame form, different predictors. This time we’re just using the brms default priors. As such, the models all follow the form\n\\[ \\begin{align*} y_i \u0026amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i \u0026amp; = \\beta_0 + \\beta_n x_n\\\\ \\beta_0 \u0026amp; \\sim \\text{Student-t}(3, 0, 10) \\\\ \\sigma \u0026amp; \\sim \\text{Student-t}(3, 0, 10) \\end{align*} \\]\nYou may be wondering What about the prior for \\(\\beta_n\\)? The brms defaults for those are improper flat priors. We define \\(\\beta_n x_n\\) for the next three models as\n fit4: \\(\\beta_1 x_1\\), fit5: \\(\\beta_2 x_2\\), and fit5: \\(\\beta_3 x_3\\).  Let’s fit the models.\nfit4 \u0026lt;- brm(data = d, family = gaussian, y ~ 1 + x1, seed = 1) fit5 \u0026lt;- update(fit4, newdata = d, y ~ 1 + x2, seed = 1) fit6 \u0026lt;- update(fit4, newdata = d, y ~ 1 + x3, seed = 1) Like before, save the posterior draws for each as separate data frames.\npost4 \u0026lt;- posterior_samples(fit4) post5 \u0026lt;- posterior_samples(fit5) post6 \u0026lt;- posterior_samples(fit6) This time, our simple bind_rows() trick won’t work well.\nbind_rows( post4, post5, post6 ) %\u0026gt;% head() ## b_Intercept b_x1 sigma lp__ b_x2 b_x3 ## 1 0.167513067 -0.179568244 1.154730 -243.4799 NA NA ## 2 -0.017092084 -0.281401589 1.145705 -243.1930 NA NA ## 3 -0.036944855 -0.204756757 1.191577 -242.1853 NA NA ## 4 0.041075341 -0.009902425 1.183252 -242.0840 NA NA ## 5 0.032423912 -0.050100545 1.147125 -241.8424 NA NA ## 6 0.003649314 -0.161764444 1.183537 -241.7794 NA NA We don’t want separate columns for b_x1, b_x2, and b_x3. We want them all stacked atop one another. One simple solution is a two-step wherein we (1) select the relevant columns from each and bind them together with bind_cols() and then (2) stack them atop one another with the gather() function.\nposts \u0026lt;- bind_cols( post4 %\u0026gt;% select(b_x1), post5 %\u0026gt;% select(b_x2), post6 %\u0026gt;% select(b_x3) ) %\u0026gt;% gather() %\u0026gt;% mutate(predictor = str_remove(key, \u0026quot;b_\u0026quot;)) head(posts) ## key value predictor ## 1 b_x1 -0.179568244 x1 ## 2 b_x1 -0.281401589 x1 ## 3 b_x1 -0.204756757 x1 ## 4 b_x1 -0.009902425 x1 ## 5 b_x1 -0.050100545 x1 ## 6 b_x1 -0.161764444 x1 That mutate() line at the end wasn’t necessary, but it will make the plot more attractive.\nposts %\u0026gt;% ggplot(aes(x = value, y = predictor)) + geom_halfeyeh()  Different combinations of predictors in different forms. Now we fit a series of multivariable models. The first three will have combinations of two of the predictors. The final model will have all three. For simplicity, we continue to use the brms default priors.\nfit7 \u0026lt;- brm(data = d, family = gaussian, y ~ 1 + x1 + x2, seed = 1) fit8 \u0026lt;- update(fit7, newdata = d, y ~ 1 + x1 + x3, seed = 1) fit9 \u0026lt;- update(fit7, newdata = d, y ~ 1 + x2 + x3, seed = 1) fit10 \u0026lt;- update(fit7, newdata = d, y ~ 1 + x1 + x2 + x3, seed = 1) Individually extract the posterior draws.\npost7 \u0026lt;- posterior_samples(fit7) post8 \u0026lt;- posterior_samples(fit8) post9 \u0026lt;- posterior_samples(fit9) post10 \u0026lt;- posterior_samples(fit10) Take a look at what happens this time when we use the bind_rows() approach.\nposts \u0026lt;- bind_rows( post7, post8, post9, post10 ) glimpse(posts) ## Observations: 16,000 ## Variables: 6 ## $ b_Intercept \u0026lt;dbl\u0026gt; 0.09509318, 0.08186866, 0.02571336, -0.18844144, -0.06763395, 0.06137342, 0.0… ## $ b_x1 \u0026lt;dbl\u0026gt; -0.117031758, 0.004563560, -0.129202123, -0.125793634, -0.041442345, -0.02864… ## $ b_x2 \u0026lt;dbl\u0026gt; 0.19180539, 0.19254784, 0.31151419, 0.33083881, 0.08777655, 0.32273994, 0.158… ## $ sigma \u0026lt;dbl\u0026gt; 1.118284, 1.122873, 1.159854, 1.108438, 1.041766, 1.214929, 1.220967, 1.28653… ## $ lp__ \u0026lt;dbl\u0026gt; -239.1799, -239.4599, -238.7769, -240.8443, -241.7296, -239.6411, -239.5687, … ## $ b_x3 \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… We still have the various data frames stacked atop another, with the data from post7 in the first 4,000 rows. See how the values in the b_x3 column are all missing (i.e., filled with NA values)? That’s because fit7 didn’t contain x3 as a predictor. Similarly, if we were to look at rows 4,001 through 8,000, we’d see column b_x2 would be the one filled with NAs. This behavior is a good thing, here. After a little more wrangling, we’ll plot and it should be become clear why. Here’s the wrangling.\nposts \u0026lt;- posts %\u0026gt;% select(starts_with(\u0026quot;b_x\u0026quot;)) %\u0026gt;% mutate(contains = rep(c(\u0026quot;\u0026lt;1, 1, 0\u0026gt;\u0026quot;, \u0026quot;\u0026lt;1, 0, 1\u0026gt;\u0026quot;, \u0026quot;\u0026lt;0, 1, 1\u0026gt;\u0026quot;, \u0026quot;\u0026lt;1, 1, 1\u0026gt;\u0026quot;), each = 4000)) %\u0026gt;% gather(key, value, -contains) %\u0026gt;% mutate(coefficient = str_remove(key, \u0026quot;b_x\u0026quot;) %\u0026gt;% str_c(\u0026quot;beta[\u0026quot;, ., \u0026quot;]\u0026quot;)) head(posts) ## contains key value coefficient ## 1 \u0026lt;1, 1, 0\u0026gt; b_x1 -0.11703176 beta[1] ## 2 \u0026lt;1, 1, 0\u0026gt; b_x1 0.00456356 beta[1] ## 3 \u0026lt;1, 1, 0\u0026gt; b_x1 -0.12920212 beta[1] ## 4 \u0026lt;1, 1, 0\u0026gt; b_x1 -0.12579363 beta[1] ## 5 \u0026lt;1, 1, 0\u0026gt; b_x1 -0.04144234 beta[1] ## 6 \u0026lt;1, 1, 0\u0026gt; b_x1 -0.02864308 beta[1] With the contains variable, we indexed which fit the draws came from. The 1s and 0s within the angle brackets indicate which of the three predictors were present within the model with the 1s indicating they were and the 0s indicating they were not. For example, \u0026lt;1, 1, 0\u0026gt; in the first row indicated this was the model including x1 and x2. Importantly, we also added a coefficient index. This is just a variant of key that’ll make the strip labels in our plot more attractive. Behold:\nposts %\u0026gt;% ggplot(aes(x = value, y = contains)) + geom_halfeyeh() + ylab(NULL) + facet_wrap(~coefficient, ncol = 1, labeller = label_parsed) Hopefully now it’s clear why it was good to save those cells with the NAs.\n  Bonus: You can streamline your workflow. The workflows above are generally fine. But they’re a little inefficient. If you’d like to reduce the amount of code you’re writing and the number of objects you have floating around in your environment, you might consider a more streamlined workflow where you work with your fit objects in bulk. Here we’ll demonstrate a nested tibble approach with the first three fits.\nposts \u0026lt;- tibble(name = str_c(\u0026quot;fit\u0026quot;, 1:3), prior = str_c(\u0026quot;normal(0, \u0026quot;, c(10, 1, 0.1), \u0026quot;)\u0026quot;)) %\u0026gt;% mutate(fit = map(name, get)) %\u0026gt;% mutate(post = map(fit, posterior_samples)) head(posts) ## # A tibble: 3 x 4 ## name prior fit post ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 fit1 normal(0, 10) \u0026lt;brmsfit\u0026gt; \u0026lt;df[,3] [4,000 × 3]\u0026gt; ## 2 fit2 normal(0, 1) \u0026lt;brmsfit\u0026gt; \u0026lt;df[,3] [4,000 × 3]\u0026gt; ## 3 fit3 normal(0, 0.1) \u0026lt;brmsfit\u0026gt; \u0026lt;df[,3] [4,000 × 3]\u0026gt; We have a 3-row nested tibble. The first column, name is just a character vector with the names of the fits. The next column isn’t necessary, but it nicely explicates the main difference in the models: the prior we used on the intercept. It’s in the map() functions within the two mutate()lines where all the magic happens. With the first, we used the get() function to snatch up the brms fit objects matching the names in the name column. In the second, we used the posterior_samples() function to extract the posterior draws from each of the fits saved in fit. Do you see how each for in the post column contains an entire \\(4,000 \\times 3\\) data frame? That’s why we refer to this as a nested tibble. We have data frames compressed within data frames. If you’d like to access the data within the post column, just unnest().\nposts %\u0026gt;% unnest(post) ## # A tibble: 12,000 x 6 ## name prior fit b_Intercept sigma lp__ ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 fit1 normal(0, 10) \u0026lt;brmsfit\u0026gt; -0.0522 0.857 -204. ## 2 fit1 normal(0, 10) \u0026lt;brmsfit\u0026gt; 0.125 0.898 -204. ## 3 fit1 normal(0, 10) \u0026lt;brmsfit\u0026gt; -0.00629 0.948 -204. ## 4 fit1 normal(0, 10) \u0026lt;brmsfit\u0026gt; 0.0609 0.941 -204. ## 5 fit1 normal(0, 10) \u0026lt;brmsfit\u0026gt; 0.178 0.946 -205. ## 6 fit1 normal(0, 10) \u0026lt;brmsfit\u0026gt; 0.135 0.927 -204. ## 7 fit1 normal(0, 10) \u0026lt;brmsfit\u0026gt; -0.0777 0.874 -204. ## 8 fit1 normal(0, 10) \u0026lt;brmsfit\u0026gt; 0.116 0.973 -205. ## 9 fit1 normal(0, 10) \u0026lt;brmsfit\u0026gt; 0.134 0.852 -205. ## 10 fit1 normal(0, 10) \u0026lt;brmsfit\u0026gt; 0.0197 0.929 -203. ## # … with 11,990 more rows After un-nesting, we can remake the plot from above.\nposts %\u0026gt;% unnest(post) %\u0026gt;% ggplot(aes(x = b_Intercept, y = prior)) + geom_halfeyeh() To learn more about using the tidyverse for iterating and saving the results in nested tibbles, check out Hadley Wickham’s great talk, Managing many models.\n Session information sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.1.9000 brms_2.12.0 Rcpp_1.0.3 forcats_0.4.0 ## [5] stringr_1.4.0 dplyr_0.8.4 purrr_0.3.3 readr_1.3.1 ## [9] tidyr_1.0.2 tibble_2.1.3 ggplot2_3.2.1 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 ## [4] rsconnect_0.8.16 markdown_1.1 base64enc_0.1-3 ## [7] fs_1.3.1 rstudioapi_0.10 farver_2.0.3 ## [10] rstan_2.19.2 svUnit_0.7-12 DT_0.11 ## [13] fansi_0.4.1 mvtnorm_1.0-12 lubridate_1.7.4 ## [16] xml2_1.2.2 bridgesampling_0.8-1 knitr_1.26 ## [19] shinythemes_1.1.2 bayesplot_1.7.1 jsonlite_1.6.1 ## [22] broom_0.5.3 dbplyr_1.4.2 shiny_1.4.0 ## [25] compiler_3.6.2 httr_1.4.1 backports_1.1.5 ## [28] assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [31] lazyeval_0.2.2 cli_2.0.1 later_1.0.0 ## [34] htmltools_0.4.0 prettyunits_1.1.1 tools_3.6.2 ## [37] igraph_1.2.4.2 coda_0.19-3 gtable_0.3.0 ## [40] glue_1.3.1 reshape2_1.4.3 cellranger_1.1.0 ## [43] vctrs_0.2.2 nlme_3.1-142 blogdown_0.17 ## [46] crosstalk_1.0.0 xfun_0.12 ps_1.3.0 ## [49] rvest_0.3.5 mime_0.8 miniUI_0.1.1.1 ## [52] lifecycle_0.1.0 gtools_3.8.1 zoo_1.8-7 ## [55] scales_1.1.0 colourpicker_1.0 hms_0.5.3 ## [58] promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.2 ## [61] inline_0.3.15 shinystan_2.5.0 yaml_2.2.1 ## [64] gridExtra_2.3 loo_2.2.0 StanHeaders_2.19.0 ## [67] stringi_1.4.6 dygraphs_1.1.1.6 pkgbuild_1.0.6 ## [70] rlang_0.4.5 pkgconfig_2.0.3 matrixStats_0.55.0 ## [73] evaluate_0.14 lattice_0.20-38 labeling_0.3 ## [76] rstantools_2.0.0 htmlwidgets_1.5.1 tidyselect_1.0.0 ## [79] processx_3.4.1 plyr_1.8.5 magrittr_1.5 ## [82] bookdown_0.17 R6_2.4.1 generics_0.0.2 ## [85] DBI_1.1.0 pillar_1.4.3 haven_2.2.0 ## [88] withr_2.1.2 xts_0.12-0 abind_1.4-5 ## [91] modelr_0.1.5 crayon_1.3.4 arrayhelpers_1.0-20160527 ## [94] utf8_1.1.4 rmarkdown_2.0 grid_3.6.2 ## [97] readxl_1.3.1 callr_3.4.1 threejs_0.3.3 ## [100] reprex_0.3.0 digest_0.6.23 xtable_1.8-4 ## [103] httpuv_1.5.2 stats4_3.6.2 munsell_0.5.0 ## [106] shinyjs_1.1  ","date":1562976000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562976000,"objectID":"cad5ad2d183e00a60b07dd701e063aae","permalink":"/post/would-you-like-all-your-posteriors-in-one-plot/","publishdate":"2019-07-13T00:00:00Z","relpermalink":"/post/would-you-like-all-your-posteriors-in-one-plot/","section":"post","summary":"A colleague reached out to me earlier this week with a plotting question. They had fit a series of Bayesian models, all containing a common parameter of interest. They knew how to plot their focal parameter one model at a time, but were stumped on how to combine the plots across models into a seamless whole. It reminded me a bit of this gif\nwhich I originally got from Jenny Bryan’s great talk, Behind every great plot there’s a great deal of wrangling.","tags":["Bayesian","brms","plot","R","tidyerse","tutorial"],"title":"Would you like all your posteriors in one plot?","type":"post"},{"authors":null,"categories":[],"content":" tl;dr  Sometimes a mathematical result is strikingly contrary to generally held belief even though an obviously valid proof is given. Charles Stein of Stanford University discovered such a paradox in statistics in 1995. His result undermined a century and a half of work on estimation theory. (Efron \u0026amp; Morris, 1977, p. 119)\n The James-Stein estimator leads to better predictions than simple means. Though I don’t recommend you actually use the James-Stein estimator in applied research, understanding why it works might help clarify why it’s time social scientists consider defaulting to multilevel models for their work-a-day projects.\n The James-Stein can help us understand multilevel models. I recently noticed someone—I wish I could recall who—tweet about Efron and Morris’s classic, Stein’s Paradox in Statistics. At the time, I was vaguely aware of the paper but hadn’t taken the chance to read it. The tweet’s author mentioned how good a read it was. Now I’ve looked at it, I concur. I’m not a sports fan, but I really appreciated their primary example using batting averages from baseball players in 1970. It clarified why partial pooling leads to better estimates than taking simple averages.\nIn this project, I’ll walk out Efron and Morris’s baseball example in R and then link it to contemporary Bayesian multilevel models.\nI assume things. For this project, I’m presuming you are familiar with logistic regression, vaguely familiar with the basic differences between frequentist and Bayesian approaches to fitting regression models, and have heard of multilevel models. All code in is R, with a heavy use of the tidyverse—which you might learn a lot about here, especially chapter 5—, and the brms package for Bayesian regression.\n Behold the baseball data.  Stein’s paradox concerns the use of observed averages to estimate unobservable quantities. Averaging is the second most basic process in statistics, the first being the simple act of counting. A baseball player who gets seven hits in 20 official times at bat is said to have a batting average of .350. In computing this statistic we are forming an estimate of the payer’s true batting ability in terms of his observed average rate of success. Asked how well the player will do in his next 100 times at bat, we would probably predict 35 more hits. In traditional statistical theory it can be proved that no other estimation rule is uniformly better than the observed average.\nThe paradoxical element in Stein’s result is that it sometimes contradicts this elementary law of statistical theory. If we have three or more baseball players, and if we are interested in predicting future batting averages for each of them, then there is a procedure that is better than simply extrapolating from the three separate averages…\nAs our primary data we shall consider the batting averages of 18 major-league players as they were recorded after their first 45 times at bat in the 1970 season. (p. 119)\n Let’s enter the baseball data.\nlibrary(tidyverse) baseball \u0026lt;- tibble(player = c(\u0026quot;Clemente\u0026quot;, \u0026quot;F Robinson\u0026quot;, \u0026quot;F Howard\u0026quot;, \u0026quot;Johnstone\u0026quot;, \u0026quot;Berry\u0026quot;, \u0026quot;Spencer\u0026quot;, \u0026quot;Kessinger\u0026quot;, \u0026quot;L Alvarado\u0026quot;, \u0026quot;Santo\u0026quot;, \u0026quot;Swoboda\u0026quot;, \u0026quot;Unser\u0026quot;, \u0026quot;Williams\u0026quot;, \u0026quot;Scott\u0026quot;, \u0026quot;Petrocelli\u0026quot;, \u0026quot;E Rodriguez\u0026quot;, \u0026quot;Campaneris\u0026quot;, \u0026quot;Munson\u0026quot;, \u0026quot;Alvis\u0026quot;), hits = c(18:15, 14, 14:12, 11, 11, rep(10, times = 5), 9:7), times_at_bat = 45, true_ba = c(.346, .298, .276, .222, .273, .27, .263, .21, .269, .23, .264, .256, .303, .264, .226, .286, .316, .2)) Here’s what they look like.\nglimpse(baseball) ## Observations: 18 ## Variables: 4 ## $ player \u0026lt;chr\u0026gt; \u0026quot;Clemente\u0026quot;, \u0026quot;F Robinson\u0026quot;, \u0026quot;F Howard\u0026quot;, \u0026quot;Johnstone\u0026quot;, \u0026quot;Berry\u0026quot;, \u0026quot;Spencer\u0026quot;, \u0026quot;Kess… ## $ hits \u0026lt;dbl\u0026gt; 18, 17, 16, 15, 14, 14, 13, 12, 11, 11, 10, 10, 10, 10, 10, 9, 8, 7 ## $ times_at_bat \u0026lt;dbl\u0026gt; 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45 ## $ true_ba \u0026lt;dbl\u0026gt; 0.346, 0.298, 0.276, 0.222, 0.273, 0.270, 0.263, 0.210, 0.269, 0.230, 0.264,… We have data from 18 players. The main columns are of the number of hits for their first 45 times_at_bat. I got the player, hits, and times_at_bat values directly from the paper. However, Efron and Morris didn’t include the batting averages for the end of the season in the paper. Happily, I was able to find those values online. They’re included in the true_ba column.\n …These were all the players who happened to have batted exactly 45 times the day the data were tabulated. A batting average is defined, of course, simply as the number of hits divided by the number of times at bat; it is always a number between 0 and 1. (p. 119)\n I like use a lot of plots to better understand what I’m doing. Before we start plotting, I should point out the color theme in this project comes from here. [Haters gonna hate.]\nnavy_blue \u0026lt;- \u0026quot;#0C2C56\u0026quot; nw_green \u0026lt;- \u0026quot;#005C5C\u0026quot; silver \u0026lt;- \u0026quot;#C4CED4\u0026quot; theme_set(theme_grey() + theme(panel.grid = element_blank(), panel.background = element_rect(fill = silver), strip.background = element_rect(fill = silver))) We might use a histogram to get a sense of the hits.\nbaseball %\u0026gt;% ggplot(aes(x = hits)) + geom_histogram(color = nw_green, fill = navy_blue, size = 1/10, binwidth = 1) + scale_x_continuous(\u0026quot;hits during the first 45 trials\u0026quot;, breaks = 7:18) And here is the distribution of the end-of-the-season batting averages, true_ba.\nlibrary(tidybayes) baseball %\u0026gt;% ggplot(aes(x = true_ba, y = 0)) + geom_halfeyeh(color = navy_blue, fill = alpha(nw_green, 2/3), point_range = median_qi, .width = .5) + geom_rug(color = navy_blue, size = 1/3, alpha = 1/2) + ggtitle(NULL, subtitle = \u0026quot;The dot and horizontal line are the median and\\ninterquartile range, respectively.\u0026quot;)  James-Stein will help us achieve our goal. For each of the 18 players in the data, our goal is to the best job possible to use the data for their first 45 times at bat (i.e., hits and times_at_bat) to predict their batting averages at the end of the season (i.e., true_ba). Before Charles Stein, the conventional reasoning was their initial batting averages (i.e., hits / times_at_bat) are the best way to do this. It turns out that would be naïve. To see why, let\n y (i.e., \\(y\\)) = the batting average for the first 45 times at bat y_bar (i.e., \\(\\overline y\\)) = the grand mean for the first 45 times at bat c (i.e., \\(c\\)) = shrinking factor z (i.e., \\(z\\)) = James-Stein estimate true_ba (i.e., theta, \\(\\theta\\)) = the batting average at the end of the season   The first step in applying Stein’s method is to determine the average of the averages. Obviously this grand average, which we give the symbol \\(\\overline y\\), must also lie between 0 and 1. The essential process in Stein’s method is the “shrinking” of all the individual averages toward this grand average. If a player’s hitting record is better than the grand average, then it must be reduced; if he is not hitting as well as the grand average, then his hitting record must be increased. The resulting shrunken value for each player we designate \\(z\\). (p. 119)\n As such, the James-Stein estimator is:\n\\[z = \\overline y + c(y - \\overline y)\\]\nAnd in the paper, \\(c = .212\\). Let’s get some of those values into the baseball data.\n( baseball \u0026lt;- baseball %\u0026gt;% mutate(y = hits / times_at_bat) %\u0026gt;% mutate(y_bar = mean(y), c = .212) %\u0026gt;% mutate(z = y_bar + c * (y - y_bar), theta = true_ba) ) ## # A tibble: 18 x 9 ## player hits times_at_bat true_ba y y_bar c z theta ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Clemente 18 45 0.346 0.4 0.265 0.212 0.294 0.346 ## 2 F Robinson 17 45 0.298 0.378 0.265 0.212 0.289 0.298 ## 3 F Howard 16 45 0.276 0.356 0.265 0.212 0.285 0.276 ## 4 Johnstone 15 45 0.222 0.333 0.265 0.212 0.280 0.222 ## 5 Berry 14 45 0.273 0.311 0.265 0.212 0.275 0.273 ## 6 Spencer 14 45 0.27 0.311 0.265 0.212 0.275 0.27 ## 7 Kessinger 13 45 0.263 0.289 0.265 0.212 0.270 0.263 ## 8 L Alvarado 12 45 0.21 0.267 0.265 0.212 0.266 0.21 ## 9 Santo 11 45 0.269 0.244 0.265 0.212 0.261 0.269 ## 10 Swoboda 11 45 0.23 0.244 0.265 0.212 0.261 0.23 ## 11 Unser 10 45 0.264 0.222 0.265 0.212 0.256 0.264 ## 12 Williams 10 45 0.256 0.222 0.265 0.212 0.256 0.256 ## 13 Scott 10 45 0.303 0.222 0.265 0.212 0.256 0.303 ## 14 Petrocelli 10 45 0.264 0.222 0.265 0.212 0.256 0.264 ## 15 E Rodriguez 10 45 0.226 0.222 0.265 0.212 0.256 0.226 ## 16 Campaneris 9 45 0.286 0.2 0.265 0.212 0.252 0.286 ## 17 Munson 8 45 0.316 0.178 0.265 0.212 0.247 0.316 ## 18 Alvis 7 45 0.2 0.156 0.265 0.212 0.242 0.2  Which set of values, \\(y\\) or \\(z\\), is the better indicator of batting ability for the 18 players in our example? In order to answer that question in a precise way one would have to know the “true batting ability” of each player. This true average we shall designate \\(\\theta\\) (the Greek letter theta). Actually it is an unknowable quantity, an abstraction representing the probability that a player will get a hit on any given time at bat. Although \\(\\theta\\) is unobservable, we have a good approximation to it: the subsequent performance of the batters. It is sufficient to consider just the remainder of the 1970 season, which includes about nine times as much data as the preliminary averages were based on. (p. 119)\n Now we have both \\(y\\) and \\(z\\) in the data, let’s compare their distributions.\nbaseball %\u0026gt;% select(y, z) %\u0026gt;% gather() %\u0026gt;% mutate(label = ifelse(key == \u0026quot;z\u0026quot;, \u0026quot;the James-Stein estimate\u0026quot;, \u0026quot;early-season batting average\u0026quot;)) %\u0026gt;% ggplot(aes(x = value, y = label)) + geom_vline(color = \u0026quot;white\u0026quot;, xintercept = 0.2654321, linetype = 2) + geom_halfeyeh(color = navy_blue, fill = alpha(nw_green, 2/3), point_range = median_qi, .width = .5, relative_scale = 4) + labs(x = \u0026quot;batting average\u0026quot;, y = NULL) + coord_cartesian(ylim = c(1.25, 5.25)) ## Warning: ## In geom_halfeyeh(): The `relative_scale` argument is a deprecated alias for `scale`. ## Use the `scale` argument instead. ## See help(\u0026quot;tidybayes-deprecated\u0026quot;). ## Warning: Ignoring unknown parameters: point_range As implied in the formula, the James-Stein estimates are substantially shrunken towards the grand mean, y_bar. To get a sense of which estimate is better, we can subtract the estimate from theta, the end of the season batting average.\nbaseball \u0026lt;- baseball %\u0026gt;% mutate(y_error = theta - y, z_error = theta - z) Since y_error and y_error are error distributions, we prefer values to be as close to zero as possible. Let’s take a look.\nbaseball %\u0026gt;% select(y_error:z_error) %\u0026gt;% gather() %\u0026gt;% ggplot(aes(x = value, y = key)) + geom_vline(xintercept = 0, linetype = 2, color = \u0026quot;white\u0026quot;) + geom_halfeyeh(color = navy_blue, fill = alpha(nw_green, 2/3), point_range = median_qi, .width = .5, relative_scale = 2.5) + labs(x = NULL, y = NULL) + coord_cartesian(ylim = c(1.25, 4)) ## Warning: ## In geom_halfeyeh(): The `relative_scale` argument is a deprecated alias for `scale`. ## Use the `scale` argument instead. ## See help(\u0026quot;tidybayes-deprecated\u0026quot;). ## Warning: Ignoring unknown parameters: point_range The James-Stein errors (i.e., z_error) are much more concentrated toward zero. In the paper, we read: “One method of evaluating the two estimates is by simply counting their successes and failures. For 16 of the 18 players the James-Stein estimator \\(z\\) is closer than the observed average \\(y\\) to the ‘true,’ or seasonal, average \\(\\theta\\)” (pp. 119–121). We can compute that with a little ifelse().\nbaseball %\u0026gt;% transmute(closer_to_theta = ifelse(abs(y_error) - abs(z_error) == 0, \u0026quot;equal\u0026quot;, ifelse(abs(y_error) - abs(z_error) \u0026gt; 0, \u0026quot;z\u0026quot;, \u0026quot;y\u0026quot;))) %\u0026gt;% group_by(closer_to_theta) %\u0026gt;% count() ## # A tibble: 2 x 2 ## # Groups: closer_to_theta [2] ## closer_to_theta n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 y 2 ## 2 z 16  A more quantitative way of comparing the two techniques is through the total squared error of estimation… The observed averages \\(y\\) have a total squared error of .077, whereas the squared error of the James-Stein estimators is only .022. By this comparison, then, Stein’s method is 3.5 times as accurate. (p. 121)\n baseball %\u0026gt;% select(y_error:z_error) %\u0026gt;% gather() %\u0026gt;% group_by(key) %\u0026gt;% summarise(total_squared_error = sum(value * value)) ## # A tibble: 2 x 2 ## key total_squared_error ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 y_error 0.0755 ## 2 z_error 0.0214 We can get the 3.5 value with simple division.\n0.07548795 / 0.02137602 ## [1] 3.531431 So it does indeed turn out that shrinking each player’s initial estimate toward the grand mean of those initial estimates does a better job of predicting their end-of-the-season batting averages than using their individual batting averages. To get a sense of what this looks like, let’s make our own version of the figure on page 121.\nbaseball %\u0026gt;% select(y, z, theta, player) %\u0026gt;% gather(key, value, -player) %\u0026gt;% mutate(time = ifelse(key == \u0026quot;theta\u0026quot;, \u0026quot;theta\u0026quot;, \u0026quot;estimate\u0026quot;)) %\u0026gt;% bind_rows( baseball %\u0026gt;% select(player, theta) %\u0026gt;% rename(value = theta) %\u0026gt;% mutate(key = \u0026quot;theta\u0026quot;, time = \u0026quot;theta\u0026quot;) ) %\u0026gt;% mutate(facet = rep(c(\u0026quot;estimate = y\u0026quot;, \u0026quot;estimate = z\u0026quot;), each = n() / 4) %\u0026gt;% rep(., times = 2)) %\u0026gt;% ggplot(aes(x = time, y = value, group = player)) + geom_hline(yintercept = 0.2654321, linetype = 2, color = \u0026quot;white\u0026quot;) + geom_line(alpha = 1/2, color = nw_green) + geom_point(alpha = 1/2, color = navy_blue) + labs(x = NULL, y = \u0026quot;batting average\u0026quot;) + theme(axis.ticks.x = element_blank()) + facet_wrap(~facet) The James-Stein estimator works because of its shrinkage. The shrinkage factor is \\(c\\). In the first parts of the paper, Efron and Morris just told us \\(c = .212\\). A little later in the paper, they gave the actual formula for \\(c\\). If you let \\(k\\) be the number of means (i.e., the number of clusters), then:\n\\[c = 1 - \\frac{(k - 3)\\sigma^2}{\\sum (y - \\overline y)^2}\\]\nThe difficulty of that formula is we don’t know the value for \\(\\sigma^2\\). It’s not the simple variance of \\(y\\) (i.e., var(y)). An answer to this stackexchange question appears to have uncovered the method Efron and Morris used in the paper. I’ll reproduce it in detail:\nFollowing along, we can compute sigma_squared like so:\n(sigma_squared \u0026lt;- mean(baseball$y) * (1 - mean(baseball$y)) / 45) ## [1] 0.004332842 Now we can reproduce the \\(c\\) value from the paper.\nbaseball %\u0026gt;% select(player, y:c) %\u0026gt;% mutate(squared_deviation = (y - y_bar)^2) %\u0026gt;% summarise(c_by_hand = 1 - ((n() - 3) * sigma_squared / sum(squared_deviation))) ## # A tibble: 1 x 1 ## c_by_hand ## \u0026lt;dbl\u0026gt; ## 1 0.212   Let’s go Bayesian. This has been fun. But I don’t recommend you actually use the James-Stein estimator in your research.\n The James-Stein estimator is not the only one that is known to be better than the sample averages…\nThe search for new estimators continues. Recent efforts [in the 1970s, that is] have been concentrated on achieving results like those obtained with Stein’s method for problems involving distributions other than the normal distribution. Several lines of work, including Stein’s and Robbins’ and more formal Bayesian methods seem to be converging on a powerful general theory of parameter estimation. (p. 127, emphasis added)\n The James-Stein estimator is not Bayesian, but it is a precursor to the kind of analyses we now do with Bayesian multilevel models, which pool cluster-level means toward a grand mean. To get a sense of this, let’s fit a couple models. First, let’s load the brms package.\nlibrary(brms) I typically work with the linear regression paradigm. If we were to analyze the baseball data, we’d use an aggregated binomial mode, which is a particular kind of logistic regression. You can learn more about it here or here. If we wanted a model that corresponded to the \\(y\\) estimates, above, we’d use hits as the criterion and allow each player to get his own separate estimate. Since we’re working within the Bayesian paradigm, we also need to assign priors. In this case, we’ll use a weakly-regularizing \\(\\text{Normal} (0, 1.5)\\) on the intercepts. See this wiki for more on weakly-regularizing priors.\nHere’s the code to fit the model in brms.\nfit_y \u0026lt;- brm(data = baseball, family = binomial, hits | trials(45) ~ 0 + player, prior(normal(0, 1.5), class = b), seed = 1) If you were curious, that model followed the statistical formula\n\\[ \\begin{eqnarray} \\text{hits}_i \u0026amp; \\sim \u0026amp; \\text{Binomial} (n = 45, p_i) \\\\ \\text{logit}(p_i) \u0026amp; = \u0026amp; \\alpha_\\text{player} \\\\ \\alpha_\\text{player} \u0026amp; \\sim \u0026amp; \\text{Normal} (0, 1.5) \\end{eqnarray} \\]\nwhere \\(p_i\\) is the probability of player \\(i\\), \\(\\alpha_\\text{player}\\) is a vector of \\(\\text{player}\\)-specific intercepts from within the logistic regression model, and each of those intercepts are given a \\(\\text{Normal} (0, 1.5)\\) prior on the log-odds scale. (If this is all new and confusing, don’t worry. I’ll recommended some resources at the end of this post.)\nFor our analogue to the James-Stein estimate \\(z\\), we’ll fit the multilevel version of that last model. While each player still gets his own estimate, those estimates are now partially-pooled toward the grand mean.\nfit_z \u0026lt;- brm(data = baseball, family = binomial, hits | trials(45) ~ 1 + (1 | player), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1.5), class = sd)), seed = 1) And that model followed the statistical formula\n\\[ \\begin{eqnarray} \\text{hits}_i \u0026amp; \\sim \u0026amp; \\text{Binomial} (n = 45, p_i) \\\\ \\text{logit}(p_i) \u0026amp; = \u0026amp; \\alpha + \\alpha_\\text{player} \\\\ \\alpha \u0026amp; \\sim \u0026amp; \\text{Normal} (0, 1.5) \\\\ \\alpha_\\text{player} \u0026amp; \\sim \u0026amp; \\text{Normal} (0, \\sigma_\\text{player}) \\\\ \\sigma_\\text{player} \u0026amp; \\sim \u0026amp; \\text{HalfNormal} (0, 1.5) \\end{eqnarray} \\]\nwhere \\(\\alpha\\) is the grand mean among the \\(\\text{player}\\)-specific intercepts, \\(\\alpha_\\text{player}\\) is the vector of \\(\\text{player}\\)-specific deviations from the grand mean, which are Normally distributed with a mean of zero and a standard deviation of \\(\\sigma_\\text{player}\\), which is estimated from the data.\nHere are the model summaries.\nfit_y$fit ## Inference for Stan model: 1d2456d7f7a08ebf8ef5fda01ce9b808. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_playerAlvis -1.63 0.00 0.39 -2.44 -1.89 -1.62 -1.35 -0.89 7862 1 ## b_playerBerry -0.78 0.00 0.31 -1.41 -0.98 -0.77 -0.56 -0.19 8886 1 ## b_playerCampaneris -1.34 0.00 0.35 -2.05 -1.57 -1.33 -1.10 -0.69 7628 1 ## b_playerClemente -0.39 0.00 0.30 -0.99 -0.59 -0.39 -0.19 0.18 10134 1 ## b_playerERodriguez -1.21 0.00 0.35 -1.93 -1.45 -1.20 -0.97 -0.55 10145 1 ## b_playerFHoward -0.59 0.00 0.31 -1.22 -0.79 -0.58 -0.38 0.03 10787 1 ## b_playerFRobinson -0.49 0.00 0.30 -1.08 -0.69 -0.49 -0.28 0.10 10544 1 ## b_playerJohnstone -0.69 0.00 0.31 -1.32 -0.88 -0.68 -0.48 -0.09 9763 1 ## b_playerKessinger -0.88 0.00 0.33 -1.55 -1.10 -0.88 -0.67 -0.28 9094 1 ## b_playerLAlvarado -0.98 0.00 0.32 -1.63 -1.20 -0.97 -0.76 -0.37 10622 1 ## b_playerMunson -1.48 0.00 0.38 -2.27 -1.72 -1.46 -1.21 -0.77 11067 1 ## b_playerPetrocelli -1.21 0.00 0.33 -1.89 -1.43 -1.20 -0.99 -0.59 9253 1 ## b_playerSanto -1.10 0.00 0.33 -1.78 -1.32 -1.09 -0.87 -0.46 9619 1 ## b_playerScott -1.22 0.00 0.36 -1.94 -1.45 -1.20 -0.98 -0.54 10948 1 ## b_playerSpencer -0.78 0.00 0.33 -1.45 -0.99 -0.77 -0.55 -0.14 8511 1 ## b_playerSwoboda -1.10 0.00 0.35 -1.81 -1.33 -1.10 -0.87 -0.42 10665 1 ## b_playerUnser -1.21 0.00 0.35 -1.92 -1.44 -1.21 -0.97 -0.54 11893 1 ## b_playerWilliams -1.22 0.00 0.35 -1.96 -1.45 -1.20 -0.97 -0.56 8597 1 ## lp__ -73.45 0.08 2.93 -79.97 -75.21 -73.15 -71.34 -68.48 1444 1 ## ## Samples were drawn using NUTS(diag_e) at Sat Feb 23 17:19:53 2019. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). fit_z$fit ## Inference for Stan model: 33295e60ce033f843c74128ac973bc03. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_Intercept -1.02 0.00 0.09 -1.21 -1.08 -1.02 -0.96 -0.84 3116 1 ## sd_player__Intercept 0.17 0.00 0.11 0.01 0.08 0.16 0.24 0.42 1643 1 ## r_player[Alvis,Intercept] -0.13 0.00 0.20 -0.62 -0.22 -0.07 0.00 0.17 2411 1 ## r_player[Berry,Intercept] 0.05 0.00 0.17 -0.26 -0.03 0.02 0.13 0.44 4251 1 ## r_player[Campaneris,Intercept] -0.08 0.00 0.17 -0.51 -0.16 -0.04 0.02 0.21 3621 1 ## r_player[Clemente,Intercept] 0.14 0.00 0.20 -0.13 0.00 0.09 0.25 0.63 2902 1 ## r_player[E.Rodriguez,Intercept] -0.05 0.00 0.16 -0.43 -0.12 -0.02 0.04 0.27 4722 1 ## r_player[F.Howard,Intercept] 0.09 0.00 0.18 -0.20 -0.01 0.05 0.19 0.54 3081 1 ## r_player[F.Robinson,Intercept] 0.12 0.00 0.19 -0.17 0.00 0.07 0.22 0.58 2766 1 ## r_player[Johnstone,Intercept] 0.07 0.00 0.17 -0.22 -0.02 0.04 0.15 0.47 4122 1 ## r_player[Kessinger,Intercept] 0.03 0.00 0.16 -0.29 -0.05 0.01 0.09 0.40 4051 1 ## r_player[L.Alvarado,Intercept] 0.00 0.00 0.17 -0.37 -0.08 0.00 0.08 0.36 4060 1 ## r_player[Munson,Intercept] -0.10 0.00 0.19 -0.59 -0.18 -0.05 0.01 0.19 3625 1 ## r_player[Petrocelli,Intercept] -0.05 0.00 0.17 -0.46 -0.14 -0.02 0.04 0.25 4014 1 ## r_player[Santo,Intercept] -0.02 0.00 0.16 -0.40 -0.09 -0.01 0.05 0.30 4388 1 ## r_player[Scott,Intercept] -0.05 0.00 0.17 -0.45 -0.13 -0.02 0.04 0.26 3650 1 ## r_player[Spencer,Intercept] 0.05 0.00 0.17 -0.27 -0.04 0.02 0.13 0.43 3611 1 ## r_player[Swoboda,Intercept] -0.03 0.00 0.16 -0.38 -0.10 -0.01 0.05 0.28 4562 1 ## r_player[Unser,Intercept] -0.05 0.00 0.16 -0.44 -0.13 -0.02 0.04 0.25 3412 1 ## r_player[Williams,Intercept] -0.05 0.00 0.17 -0.44 -0.13 -0.02 0.04 0.26 4306 1 ## lp__ -73.87 0.13 4.11 -82.53 -76.49 -73.67 -71.00 -66.54 1053 1 ## ## Samples were drawn using NUTS(diag_e) at Sat Feb 23 17:20:43 2019. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). If you’re new to aggregated binomial or logistic regression, those estimates might be confusing. For technical reasons—see here—, they’re in a log-odds metric. But we can use the brms::inv_logit_scaled() function to convert them back to a probability metric. Why would we want a probability metric?, you might ask. As it turns out, batting average is in a probability metric, too. So you might also think of the inv_logit_scaled() function as turning the model results into a batting-average metric. For example, if we wanted to get the estimated batting average for E. Rodriguez baed on the y_fit model (i.e., the model corresponding to the \\(y\\) estimator), we might do something like this.\nfixef(fit_y)[\u0026quot;playerERodriguez\u0026quot;, 1] %\u0026gt;% inv_logit_scaled() ## [1] 0.2293629 To double check the model returned a sensible estimate, here’s the corresponding y value from the baseball data.\nbaseball %\u0026gt;% filter(player == \u0026quot;E Rodriguez\u0026quot;) %\u0026gt;% select(y) ## # A tibble: 1 x 1 ## y ## \u0026lt;dbl\u0026gt; ## 1 0.222 It’s a little off, but in the right ballpark. Here is the corresponding estimate from the multilevel model, fit_z:\ncoef(fit_z)$player[\u0026quot;E Rodriguez\u0026quot;, 1, ] %\u0026gt;% inv_logit_scaled() ## [1] 0.2558496 And indeed that’s pretty close to the z value from the baseball data, too.\nbaseball %\u0026gt;% filter(player == \u0026quot;E Rodriguez\u0026quot;) %\u0026gt;% select(z) ## # A tibble: 1 x 1 ## z ## \u0026lt;dbl\u0026gt; ## 1 0.256 So now we have these too competing ways to model the data of the first 45 times at bat, let’s see how well their estimates predict the true_ba values. We’ll do so with a couple plots. This first one is of the single-level model which did not pool the batting averages.\n# get the `fitted()` draws and wrangle a bit f_y \u0026lt;- baseball %\u0026gt;% distinct(player) %\u0026gt;% add_fitted_draws(fit_y, dpar = \u0026quot;mu\u0026quot;) %\u0026gt;% left_join(baseball %\u0026gt;% select(player, true_ba)) # save the plot p1 \u0026lt;- f_y %\u0026gt;% ggplot(aes(x = mu, y = reorder(player, true_ba))) + geom_vline(xintercept = mean(baseball$true_ba), color = \u0026quot;white\u0026quot;) + stat_intervalh(.width = .95, alpha = 1/3, color = nw_green) + stat_intervalh(.width = .50, alpha = 1/3, color = nw_green) + geom_point(data = baseball, aes(x = true_ba), size = 2, alpha = 3/4, color = navy_blue) + labs(x = \u0026quot;batting average\u0026quot;, y = NULL, subtitle = \u0026quot;fit_y, the no pooling model\u0026quot;) + coord_cartesian(xlim = c(0, .6)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), plot.subtitle = element_text(hjust = .5)) Note our use of some handy convenience functions (i.e., add_fitted_draws() and stat_intervalh()) from the tidybayes package.\nThis second plot is almost the same as the previous one, but this time based on the partial-pooling multilevel model.\nf_z \u0026lt;- baseball %\u0026gt;% distinct(player) %\u0026gt;% add_fitted_draws(fit_z, dpar = \u0026quot;mu\u0026quot;) %\u0026gt;% left_join(baseball %\u0026gt;% select(player, true_ba)) p2 \u0026lt;- f_z %\u0026gt;% ggplot(aes(x = mu, y = reorder(player, true_ba))) + geom_vline(xintercept = mean(baseball$true_ba), color = \u0026quot;white\u0026quot;) + stat_intervalh(.width = .95, alpha = 1/3, color = nw_green) + stat_intervalh(.width = .50, alpha = 1/3, color = nw_green) + geom_point(data = baseball, aes(x = true_ba), size = 2, alpha = 3/4, color = navy_blue) + labs(x = \u0026quot;batting average\u0026quot;, y = NULL, subtitle = \u0026quot;fit_z, the multilevel pooling model\u0026quot;) + coord_cartesian(xlim = c(0, .6)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), plot.subtitle = element_text(hjust = .5)) Here we join them together.\nlibrary(gridExtra) grid.arrange(p1, p2, ncol = 2) In both panels, the end-of-the-season batting averages (i.e., \\(\\theta\\)) are the blue dots. The model-implied estimates are depicted by 95% and 50% interval bands (i.e., the lighter and darker green horizontal lines, respectively). The white line in the background marks off the mean of \\(\\theta\\). Although neither model was perfect, the multilevel model, our analogue to the James-Stein estimates, yielded predictions that appear both more valid and more precise.\nWe might also compare the models by their prediction errors. Here we’ll subtract the end-of-the-season batting averages from the model estimates. But unlike with y and z estimates, above, our fit_y and fit_z models yielded entire posterior distributions. Therefore, we’ll express our prediction errors in terms of error distributions, rather than single values.\n# save the `fit_y` plot p3 \u0026lt;- f_y %\u0026gt;% # the error distribution is just the model-implied values minus # the true end-of-season values mutate(error = mu - true_ba) %\u0026gt;% ggplot(aes(x = error, y = reorder(player, true_ba))) + geom_vline(xintercept = c(0, -.2, .2), size = c(1/2, 1/4, 1/4), linetype = c(1, 3, 3), color = \u0026quot;white\u0026quot;) + geom_halfeyeh(point_interval = mean_qi, .width = .95, color = navy_blue, fill = alpha(nw_green, 2/3)) + coord_cartesian(xlim = c(-.35, .35)) + labs(x = \u0026quot;error\u0026quot;, y = NULL, subtitle = \u0026quot;fit_y, the no pooling model\u0026quot;) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), plot.subtitle = element_text(hjust = .5)) # save the `fit_z` plot p4 \u0026lt;- f_z %\u0026gt;% mutate(error = mu - true_ba) %\u0026gt;% ggplot(aes(x = error, y = reorder(player, true_ba))) + geom_vline(xintercept = c(0, -.2, .2), size = c(1/2, 1/4, 1/4), linetype = c(1, 3, 3), color = \u0026quot;white\u0026quot;) + geom_halfeyeh(point_interval = mean_qi, .width = .95, color = navy_blue, fill = alpha(nw_green, 2/3)) + coord_cartesian(xlim = c(-.35, .35)) + labs(x = \u0026quot;error\u0026quot;, y = NULL, subtitle = \u0026quot;fit_z, the multilevel pooling model\u0026quot;) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), plot.subtitle = element_text(hjust = .5)) # now combine the two and behold grid.arrange(p3, p4, ncol = 2) For consistency, I’ve ordered the players along the y-axis the same as above. In both panels, we see the prediction error distribution for each player in green and then summarize those distributions in terms of their means and percentile-based 95% intervals. Since these are error distributions, we prefer them to be as close to zero as possible. Although neither model made perfect predictions, the overall errors in the multilevel model were clearly smaller. Much like with the James-Stein estimator, the partial pooling of the multilevel model made for better end-of-the-season estimates.\n The paradoxical [consequence of Bayesian multilevel models] is that [they can contradict] this elementary law of statistical theory. If we have [two] or more baseball players, and if we are interested in predicting future batting averages for each of them, then [the Bayesian multilevel model can be better] than simply extrapolating from [the] separate averages. (p. 119)\n This is another example of how the KISS principle isn’t always the best bet with data analysis.\n Next steps If you’re new to logistic regression, multilevel models or Bayesian statistics, I recommend any of the following texts:\n Statistical Rethinking Doing Bayesian Data Analysis Data Analysis Using Regression and Multilevel/Hierarchical Models  And if you choose Statistical Rethinking, do check out these great lectures on the text or my project translating the code in the text to brms and the tidyverse.\nAlso, don’t miss the provocative preprint by Davis-Stober, Dana and Rouder, When are sample means meaningful? The role of modern estimation in psychological science.\n Reference Efron, B., \u0026amp; Morris, C. (1977). Stein’s paradox in statistics. Scientific American, 236, 119–127, doi: 10.1038/scientificamerican0577-119\n Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## Random number generation: ## RNG: Mersenne-Twister ## Normal: Inversion ## Sample: Rounding ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] gridExtra_2.3 brms_2.12.0 Rcpp_1.0.3 tidybayes_2.0.1.9000 ## [5] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 purrr_0.3.3 ## [9] readr_1.3.1 tidyr_1.0.2 tibble_2.1.3 ggplot2_3.2.1 ## [13] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 ## [4] rsconnect_0.8.16 markdown_1.1 base64enc_0.1-3 ## [7] fs_1.3.1 rstudioapi_0.10 farver_2.0.3 ## [10] rstan_2.19.2 svUnit_0.7-12 DT_0.11 ## [13] fansi_0.4.1 mvtnorm_1.0-12 lubridate_1.7.4 ## [16] xml2_1.2.2 bridgesampling_0.8-1 knitr_1.26 ## [19] shinythemes_1.1.2 bayesplot_1.7.1 jsonlite_1.6.1 ## [22] broom_0.5.3 dbplyr_1.4.2 shiny_1.4.0 ## [25] compiler_3.6.2 httr_1.4.1 backports_1.1.5 ## [28] assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [31] lazyeval_0.2.2 cli_2.0.1 later_1.0.0 ## [34] prettyunits_1.1.1 htmltools_0.4.0 tools_3.6.2 ## [37] igraph_1.2.4.2 coda_0.19-3 gtable_0.3.0 ## [40] glue_1.3.1 reshape2_1.4.3 cellranger_1.1.0 ## [43] vctrs_0.2.2 nlme_3.1-142 blogdown_0.17 ## [46] crosstalk_1.0.0 xfun_0.12 ps_1.3.0 ## [49] rvest_0.3.5 mime_0.8 miniUI_0.1.1.1 ## [52] lifecycle_0.1.0 gtools_3.8.1 zoo_1.8-7 ## [55] scales_1.1.0 colourpicker_1.0 hms_0.5.3 ## [58] promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.2 ## [61] inline_0.3.15 shinystan_2.5.0 yaml_2.2.1 ## [64] StanHeaders_2.19.0 loo_2.2.0 stringi_1.4.6 ## [67] dygraphs_1.1.1.6 pkgbuild_1.0.6 rlang_0.4.5 ## [70] pkgconfig_2.0.3 matrixStats_0.55.0 evaluate_0.14 ## [73] lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 ## [76] labeling_0.3 processx_3.4.1 tidyselect_1.0.0 ## [79] plyr_1.8.5 magrittr_1.5 bookdown_0.17 ## [82] R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [85] pillar_1.4.3 haven_2.2.0 withr_2.1.2 ## [88] xts_0.12-0 abind_1.4-5 modelr_0.1.5 ## [91] crayon_1.3.4 arrayhelpers_1.0-20160527 utf8_1.1.4 ## [94] rmarkdown_2.0 grid_3.6.2 readxl_1.3.1 ## [97] callr_3.4.1 threejs_0.3.3 reprex_0.3.0 ## [100] digest_0.6.23 xtable_1.8-4 httpuv_1.5.2 ## [103] stats4_3.6.2 munsell_0.5.0 shinyjs_1.1  ","date":1550880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550880000,"objectID":"bdc6c1452a7e0d12d57936f151412893","permalink":"/post/stein-s-paradox-and-what-partial-pooling-can-do-for-you/","publishdate":"2019-02-23T00:00:00Z","relpermalink":"/post/stein-s-paradox-and-what-partial-pooling-can-do-for-you/","section":"post","summary":"tl;dr  Sometimes a mathematical result is strikingly contrary to generally held belief even though an obviously valid proof is given. Charles Stein of Stanford University discovered such a paradox in statistics in 1995. His result undermined a century and a half of work on estimation theory. (Efron \u0026amp; Morris, 1977, p. 119)\n The James-Stein estimator leads to better predictions than simple means. Though I don’t recommend you actually use the James-Stein estimator in applied research, understanding why it works might help clarify why it’s time social scientists consider defaulting to multilevel models for their work-a-day projects.","tags":["Bayesian","brms","multilevel","R","tutorial"],"title":"Stein’s Paradox and What Partial Pooling Can Do For You","type":"post"},{"authors":null,"categories":[],"content":" tl;dr There’s more than one way to fit a Bayesian correlation in brms.\n Here’s the deal. In the last post, we considered how we might estimate correlations when our data contain influential outlier values. Our big insight was that if we use variants of Student’s \\(t\\)-distribution as the likelihood rather than the conventional normal distribution, our correlation estimates were less influenced by those outliers. And we mainly did that as Bayesians using the brms package. Click here for a refresher.\nSince the brms package is designed to fit regression models, it can be surprising when you discover it’s handy for correlations, too. In short, you can fit them using a few tricks based on the multivariate syntax.\nShortly after uploading the post, it occurred to me we had more options and it might be useful to walk through them a bit.\n I assume things. For this post, I’m presuming you are vaguely familiar with linear regression–both univariate and multivariate–, have a little background with Bayesian statistics, and have used Paul Bürkner’s brms packge. As you might imagine, all code in is R, with a heavy use of the tidyverse.\n We need data. First, we’ll load our main packages.\nlibrary(mvtnorm) library(brms) library(tidyverse) We’ll use the mvtnorm package to simulate three positively correlated variables.\nm \u0026lt;- c(10, 15, 20) # the means s \u0026lt;- c(10, 20, 30) # the sigmas r \u0026lt;- c(.9, .6, .3) # the correlations # here\u0026#39;s the variance/covariance matrix v \u0026lt;- matrix(c((s[1] * s[1]), (s[2] * s[1] * r[1]), (s[3] * s[1] * r[2]), (s[2] * s[1] * r[1]), (s[2] * s[2]), (s[3] * s[2] * r[3]), (s[3] * s[1] * r[2]), (s[3] * s[2] * r[3]), (s[3] * s[3])), nrow = 3, ncol = 3) # after setting our seed, we\u0026#39;re ready to simulate with `rmvnorm()` set.seed(1) d \u0026lt;- rmvnorm(n = 50, mean = m, sigma = v) %\u0026gt;% as_tibble() %\u0026gt;% set_names(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;, \u0026quot;z\u0026quot;) Our data look like so.\nlibrary(GGally) theme_set(theme_gray() + theme(panel.grid = element_blank())) d %\u0026gt;% ggpairs() Do note the Pearson’s correlation coefficients in the upper triangle.\nIn order to exploit all the methods we’ll cover in this post, we need to standardize our data. Here we do so by hand using the typical formula\n\\[z_{x_i} = \\frac{x_i - \\overline x}{s_x}\\]\nwhere \\(\\overline x\\) is the observed mean and \\(s_x\\) is the observed standard deviation.\nd \u0026lt;- d %\u0026gt;% mutate(x_s = (x - mean(x)) / sd(x), y_s = (y - mean(y)) / sd(y), z_s = (z - mean(z)) / sd(z)) head(d) ## # A tibble: 6 x 6 ## x y z x_s y_s z_s ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 3.90 11.5 -6.90 -0.723 -0.308 -0.928 ## 2 17.7 29.5 4.01 0.758 0.653 -0.512 ## 3 20.4 33.8 41.5 1.05 0.886 0.917 ## 4 20.3 42.1 34.8 1.04 1.33 0.663 ## 5 -3.64 -26.8 43.5 -1.53 -2.36 0.994 ## 6 13.9 17.3 47.6 0.347 0.00255 1.15 There are at least two broad ways to get correlations out of standardized data in brms. One way uses the typical univariate syntax. The other way is an extension of the multivariate cbind() approach. Let’s start univariate.\nAnd for a point of clarification, we’re presuming the Gaussian likelihood for all the examples in this post.\n Univariate If you fit a simple univariate model with standardized data and a single predictor, the coefficient for the slope will be in a correlation-like metric. Happily, since the data are all standardized, it’s easy to use regularizing priors.\nf1 \u0026lt;- brm(data = d, family = gaussian, y_s ~ 1 + x_s, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = b), prior(normal(0, 1), class = sigma)), chains = 4, cores = 4, seed = 1) Take a look at the model summary.\nprint(f1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y_s ~ 1 + x_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.00 0.06 -0.11 0.12 1.00 3782 2599 ## x_s 0.91 0.06 0.79 1.02 1.00 3847 2946 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.42 0.04 0.35 0.52 1.00 3811 2729 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The ‘Population-Level Effects’ has the summary information for our intercept and slope. Notice how our x_s slope is the same as the Pearson’s correlation.\ncor(d$x, d$y) ## [1] 0.9119708 Since this approach only yields one correlation at a time, we have to fit two more models to get the other two correlations. To do so with haste, we can use the update() syntax.\nf2 \u0026lt;- update(f1, newdata = d, formula = z_s ~ 1 + x_s) f3 \u0026lt;- update(f2, newdata = d, formula = z_s ~ 1 + y_s) With the fixef() function, we can easily isolate the \\(\\beta\\) estimates.\nfixef(f2)[2, ] ## Estimate Est.Error Q2.5 Q97.5 ## 0.5836596 0.1155676 0.3569717 0.8123137 fixef(f3)[2, ] ## Estimate Est.Error Q2.5 Q97.5 ## 0.31047431 0.13742697 0.03672921 0.57820500 There’s another thing I’d like to point out. Plotting the model results will help make the point.\n# define the predictor values you\u0026#39;d like the fitted values for nd \u0026lt;- tibble(x_s = seq(from = -3, to = 3, length.out = d %\u0026gt;% nrow())) # wrangle fitted(f1, newdata = nd) %\u0026gt;% as_tibble() %\u0026gt;% bind_cols(nd) %\u0026gt;% # plot ggplot(aes(x_s)) + geom_vline(xintercept = 0, color = \u0026quot;white\u0026quot;) + geom_hline(yintercept = 0, color = \u0026quot;white\u0026quot;) + geom_point(data = d, aes(y = y_s)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = \u0026quot;identity\u0026quot;, alpha = 1/4, size = 1/2) + coord_cartesian(xlim = range(d$x_s), ylim = range(d$y_s)) The blue line is the posterior mean and the surrounding gray ribbon depicts the 95% posterior interval. Notice how the data and their respective fitted lines pass through [0, 0]? This is a consequence of modeling standardized data. We should always expect the intercept of a model like this to be 0. Here are the intercept summaries for all three models.\nfixef(f1)[\u0026quot;Intercept\u0026quot;, ] %\u0026gt;% round(3) ## Estimate Est.Error Q2.5 Q97.5 ## 0.001 0.060 -0.114 0.119 fixef(f2)[\u0026quot;Intercept\u0026quot;, ] %\u0026gt;% round(3) ## Estimate Est.Error Q2.5 Q97.5 ## 0.002 0.117 -0.226 0.233 fixef(f3)[\u0026quot;Intercept\u0026quot;, ] %\u0026gt;% round(3) ## Estimate Est.Error Q2.5 Q97.5 ## 0.000 0.134 -0.261 0.266 Within simulation error, they’re all centered on zero. So instead of estimating the intercept, why not just bake that into the models? Here we refit the models by fixing the intercept for each to zero.\nf4 \u0026lt;- update(f1, formula = y_s ~ 0 + x_s) f5 \u0026lt;- update(f4, newdata = d, formula = z_s ~ 0 + x_s) f6 \u0026lt;- update(f4, newdata = d, formula = z_s ~ 0 + y_s) Let’s take a look at the summary for the first.\nprint(f4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y_s ~ x_s - 1 ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## x_s 0.91 0.06 0.79 1.03 1.00 2390 2083 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.42 0.04 0.35 0.51 1.00 2791 2916 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Even though it may have seemed like we substantially changed the models by fixing the intercepts to 0, the summaries are essentially the same as when we estimated the intercepts. Here we’ll confirm the summaries with a plot, like above.\n# wrangle fitted(f4, newdata = nd) %\u0026gt;% as_tibble() %\u0026gt;% bind_cols(nd) %\u0026gt;% # plot ggplot(aes(x_s)) + geom_vline(xintercept = 0, color = \u0026quot;white\u0026quot;) + geom_hline(yintercept = 0, color = \u0026quot;white\u0026quot;) + geom_point(data = d, aes(y = y_s)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = \u0026quot;identity\u0026quot;, alpha = 1/4, size = 1/2) + coord_cartesian(xlim = range(d$x_s), ylim = range(d$y_s)) The difference is subtle. By fixing the intercepts at 0, we estimated the slopes (i.e., the correlations) with increased precision as demonstrated by the slightly smaller posterior standard deviations (i.e., the values in the ‘Est.Error’ columns).\nHere are the correlation summaries for those last three models.\nfixef(f4) %\u0026gt;% round(3) ## Estimate Est.Error Q2.5 Q97.5 ## x_s 0.909 0.06 0.789 1.033 fixef(f5) %\u0026gt;% round(3) ## Estimate Est.Error Q2.5 Q97.5 ## x_s 0.581 0.117 0.356 0.809 fixef(f6) %\u0026gt;% round(3) ## Estimate Est.Error Q2.5 Q97.5 ## y_s 0.311 0.137 0.047 0.585 But anyway, you get the idea. If you want to estimate a correlation in brms using simple univariate syntax, just (a) standardize the data and (b) fit a univariate model with or without an intercept. The slop will be in a correlation-like metric.\n Let’s go multivariate. If you don’t recall the steps to fit correlations in brms with the multivariate syntax, here they are:\n List the variables you’d like correlations for within cbind(). Place the cbind() function within the left side of the model formula. On the right side of the model formula, indicate you only want intercepts (i.e., ~ 1).  f7 \u0026lt;- brm(data = d, family = gaussian, cbind(x_s, y_s, z_s) ~ 1, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(1, 1), class = sigma, resp = xs), prior(normal(1, 1), class = sigma, resp = ys), prior(normal(1, 1), class = sigma, resp = zs), prior(lkj(2), class = rescor)), chains = 4, cores = 4, seed = 1) ## Warning: Using \u0026#39;cbind\u0026#39; for multivariate models is deprecated. Please use ## \u0026#39;mvbind\u0026#39; instead. ## Warning: Using \u0026#39;cbind\u0026#39; for multivariate models is deprecated. Please use ## \u0026#39;mvbind\u0026#39; instead. ## Warning: Specifying global priors for regression coefficients in multivariate ## models is deprecated and may not work as expected. Behold the summary.\nprint(f7) ## Family: MV(gaussian, gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: x_s ~ 1 ## y_s ~ 1 ## z_s ~ 1 ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## xs_Intercept -0.00 0.14 -0.27 0.27 1.00 2290 2309 ## ys_Intercept -0.00 0.14 -0.28 0.26 1.00 2576 2541 ## zs_Intercept 0.00 0.14 -0.28 0.28 1.00 3207 2724 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_xs 0.98 0.10 0.82 1.19 1.00 2378 2551 ## sigma_ys 1.00 0.10 0.83 1.22 1.00 2596 2477 ## sigma_zs 1.02 0.10 0.84 1.25 1.00 2891 2257 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## rescor(xs,ys) 0.89 0.03 0.83 0.94 1.00 2778 2696 ## rescor(xs,zs) 0.55 0.09 0.35 0.72 1.00 3279 2744 ## rescor(ys,zs) 0.25 0.13 -0.01 0.48 1.00 3036 2845 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Look at the ‘Residual Correlations:’ section at the bottom of the output. Since there are no predictors in the model, the residual correlations are just correlations. Now notice how the intercepts in this model are also hovering around 0, just like in our univariate models. Yep, we can fix those, too.\nf8 \u0026lt;- brm(data = d, family = gaussian, cbind(x_s, y_s, z_s) ~ 0, prior = c(prior(normal(1, 1), class = sigma, resp = xs), prior(normal(1, 1), class = sigma, resp = ys), prior(normal(1, 1), class = sigma, resp = zs), prior(lkj(2), class = rescor)), chains = 4, cores = 4, seed = 1) ## Warning: Using \u0026#39;cbind\u0026#39; for multivariate models is deprecated. Please use ## \u0026#39;mvbind\u0026#39; instead. ## Warning: Using \u0026#39;cbind\u0026#39; for multivariate models is deprecated. Please use ## \u0026#39;mvbind\u0026#39; instead. Without the intercepts, the rest of the model is the same within simulation variance.\nprint(f8) ## Family: MV(gaussian, gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: x_s ~ 0 ## y_s ~ 0 ## z_s ~ 0 ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_xs 0.98 0.10 0.81 1.18 1.00 2128 2303 ## sigma_ys 0.99 0.10 0.82 1.19 1.00 2426 2507 ## sigma_zs 1.01 0.10 0.84 1.23 1.00 2749 2157 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## rescor(xs,ys) 0.90 0.03 0.83 0.94 1.00 2497 2317 ## rescor(xs,zs) 0.55 0.09 0.35 0.72 1.00 2865 2147 ## rescor(ys,zs) 0.26 0.13 -0.00 0.50 1.00 2631 2201 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you wanna get silly, we can prune even further. Did you notice how the estimates for \\(\\sigma\\) are all hovering around 1? Since we have no predictors, \\(\\sigma\\) is just an estimate of the population standard deviation. And since we’re working with standardized data, the population standard deviation has to be 1. Any other estimate would be nonsensical. So why not fix it to 1?\nWith brms, we can fix those \\(\\sigma\\)s to 1 with a trick of the nonlinear distributional modeling syntax. Recall when you model \\(\\sigma\\), the brms default is to actually model its log. As is turns out, the log of 1 is zero.\nlog(1) ## [1] 0 Here’s how to make use of that within brm().\nf9 \u0026lt;- brm(data = d, family = gaussian, bf(cbind(x_s, y_s, z_s) ~ 0, sigma ~ 0), prior = c(prior(lkj(2), class = rescor)), chains = 4, cores = 4, seed = 1) ## Warning: Using \u0026#39;cbind\u0026#39; for multivariate models is deprecated. Please use ## \u0026#39;mvbind\u0026#39; instead. ## Warning: Using \u0026#39;cbind\u0026#39; for multivariate models is deprecated. Please use ## \u0026#39;mvbind\u0026#39; instead. Other than the sigma ~ 0 syntax, the main thing to notice is we’ve wrapped the entire model formula into the bf() function. Here are the results.\nprint(f9) ## Family: MV(gaussian, gaussian, gaussian) ## Links: mu = identity; sigma = log ## mu = identity; sigma = log ## mu = identity; sigma = log ## Formula: x_s ~ 0 ## sigma ~ 0 ## y_s ~ 0 ## sigma ~ 0 ## z_s ~ 0 ## sigma ~ 0 ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## rescor(xs,ys) 0.91 0.02 0.87 0.93 1.00 2984 2786 ## rescor(xs,zs) 0.57 0.07 0.42 0.69 1.00 3255 2990 ## rescor(ys,zs) 0.29 0.09 0.11 0.46 1.00 2854 2804 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The correlations are the only things left in the model.\nJust to be clear, the multivariate approach does not require standardized data. To demonstrate, here we refit f7, but with the unstandardized variables. And, since we’re no longer in the standardized metric, we’ll be less certain with our priors.\nf10 \u0026lt;- brm(data = d, family = gaussian, cbind(x, y, z) ~ 1, prior = c(prior(normal(0, 10), class = Intercept), prior(student_t(3, 0, 10), class = sigma, resp = x), prior(student_t(3, 0, 10), class = sigma, resp = y), prior(student_t(3, 0, 10), class = sigma, resp = z), prior(lkj(2), class = rescor)), chains = 4, cores = 4, seed = 1) ## Warning: Using \u0026#39;cbind\u0026#39; for multivariate models is deprecated. Please use ## \u0026#39;mvbind\u0026#39; instead. ## Warning: Using \u0026#39;cbind\u0026#39; for multivariate models is deprecated. Please use ## \u0026#39;mvbind\u0026#39; instead. ## Warning: Specifying global priors for regression coefficients in multivariate ## models is deprecated and may not work as expected. See, the ‘rescor()’ results are about the same as with f7.\nprint(f10) ## Family: MV(gaussian, gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: x ~ 1 ## y ~ 1 ## z ~ 1 ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## x_Intercept 9.65 1.21 7.20 12.01 1.00 2188 2301 ## y_Intercept 15.61 2.46 10.81 20.39 1.00 2554 2435 ## z_Intercept 14.85 3.43 7.94 21.40 1.00 2849 2571 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_x 8.96 0.86 7.46 10.81 1.00 1760 2061 ## sigma_y 18.14 1.78 15.01 22.04 1.00 1992 2509 ## sigma_z 26.07 2.58 21.68 31.70 1.00 2921 2633 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## rescor(x,y) 0.89 0.03 0.83 0.94 1.00 2244 2470 ## rescor(x,z) 0.54 0.09 0.34 0.71 1.00 3330 2827 ## rescor(y,z) 0.25 0.12 -0.01 0.48 1.00 2824 2438 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1).  It’s time to compare methods. To recap, we’ve compared several ways to fit correlations in brms. Some of the methods were with univariate syntax, others were with the multivariate syntax. Some of the models had all free parameters, others included fixed intercepts and sigmas. Whereas all the univariate models required standardized data, the multivariate approach can work with unstandardized data, too.\nNow it might be of help to compare the results from each of the methods to get a sense of which ones you might prefer. Before we do so, we’ll define a couple custom functions to streamline the data wrangling.\nget_rho \u0026lt;- function(fit) { posterior_samples(fit) %\u0026gt;% select(starts_with(\u0026quot;b_\u0026quot;), -contains(\u0026quot;Intercept\u0026quot;)) %\u0026gt;% set_names(\u0026quot;rho\u0026quot;) } get_rescor \u0026lt;- function(fit) { posterior_samples(fit) %\u0026gt;% select(starts_with(\u0026quot;rescor\u0026quot;)) %\u0026gt;% set_names(\u0026quot;x with y\u0026quot;, \u0026quot;x with z\u0026quot;, \u0026quot;y with z\u0026quot;) %\u0026gt;% gather(label, rho) %\u0026gt;% select(rho, label) } Now let’s put those functions to work and plot.\nlibrary(tidybayes) # collect the posteriors from the univariate models tibble(name = str_c(\u0026quot;f\u0026quot;, 1:6)) %\u0026gt;% mutate(fit = map(name, get)) %\u0026gt;% mutate(rho = map(fit, get_rho)) %\u0026gt;% unnest(rho) %\u0026gt;% mutate(predictor = rep(c(\u0026quot;x\u0026quot;, \u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;), each = 4000) %\u0026gt;% rep(., times = 2), criterion = rep(c(\u0026quot;y\u0026quot;, \u0026quot;z\u0026quot;, \u0026quot;z\u0026quot;), each = 4000) %\u0026gt;% rep(., times = 2)) %\u0026gt;% mutate(label = str_c(predictor, \u0026quot; with \u0026quot;, criterion)) %\u0026gt;% select(-c(predictor:criterion)) %\u0026gt;% # add in the posteriors from the multivariate models bind_rows( tibble(name = str_c(\u0026quot;f\u0026quot;, 7:10)) %\u0026gt;% mutate(fit = map(name, get)) %\u0026gt;% mutate(post = map(fit, get_rescor)) %\u0026gt;% unnest(post) ) %\u0026gt;% # wrangle a bit just to make the y axis easier to understand mutate(name = factor(name, levels = c(str_c(\u0026quot;f\u0026quot;, 1:10)), labels = c(\u0026quot;1. standardized, univariate\u0026quot;, \u0026quot;2. standardized, univariate\u0026quot;, \u0026quot;3. standardized, univariate\u0026quot;, \u0026quot;4. standardized, univariate, fixed intercepts\u0026quot;, \u0026quot;5. standardized, univariate, fixed intercepts\u0026quot;, \u0026quot;6. standardized, univariate, fixed intercepts\u0026quot;, \u0026quot;7. standardized, multivariate, fixed intercepts\u0026quot;, \u0026quot;8. standardized, multivariate, fixed intercepts\u0026quot;, \u0026quot;9. standardized, multivariate, fixed intercepts/sigmas\u0026quot;, \u0026quot;10. unstandardized, multivariate\u0026quot;))) %\u0026gt;% # plot ggplot(aes(x = rho, y = name)) + geom_vline(data = tibble(label = c(\u0026quot;x with y\u0026quot;, \u0026quot;x with z\u0026quot;, \u0026quot;y with z\u0026quot;), rho = r), aes(xintercept = rho), color = \u0026quot;white\u0026quot;) + geom_halfeyeh(.width = .95, size = 5/4) + scale_x_continuous(breaks = c(0, r)) + labs(x = expression(rho), y = NULL) + coord_cartesian(0:1) + theme(axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) + facet_wrap(~label, ncol = 3) To my eye, a few patterns emerged. First, the point estimates were about the same across methods. Second, fixing the intercepts didn’t seem to effect things, much. But, third, it appears that fixing the sigmas in the multivariate models did narrow the posteriors a bit.\nFourth, and perhaps most importantly, notice how the posteriors for the multivariate models were more asymmetric when they approached 1. Hopefully this makes intuitive sense. Correlations are bound between -1 and 1. However, standardized regression coefficients are not so bound. Accordingly, notice how the posteriors from the univariate models stayed symmetric when approaching 1 and some of their right tails even crossed over 1. So while the univariate approach did a reasonable job capturing the correlation point estimates, their posteriors weren’t quite in a correlation metric. Alternately, the univariate approach did make it convenient to express the correlations with fitted regression lines in scatter plots.\nBoth univariate and multivariate approaches appear to have their strengths and weaknesses. Choose which methods seems most appropriate for your correlation needs.\nHappy modeling.\nsessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.1.9000 GGally_1.4.0 forcats_0.4.0 ## [4] stringr_1.4.0 dplyr_0.8.4 purrr_0.3.3 ## [7] readr_1.3.1 tidyr_1.0.2 tibble_2.1.3 ## [10] ggplot2_3.2.1 tidyverse_1.3.0 brms_2.12.0 ## [13] Rcpp_1.0.3 mvtnorm_1.0-12 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ## [3] ggridges_0.5.2 rsconnect_0.8.16 ## [5] markdown_1.1 base64enc_0.1-3 ## [7] fs_1.3.1 rstudioapi_0.10 ## [9] farver_2.0.3 rstan_2.19.2 ## [11] svUnit_0.7-12 DT_0.11 ## [13] fansi_0.4.1 lubridate_1.7.4 ## [15] xml2_1.2.2 codetools_0.2-16 ## [17] bridgesampling_0.8-1 knitr_1.26 ## [19] shinythemes_1.1.2 bayesplot_1.7.1 ## [21] jsonlite_1.6.1 broom_0.5.3 ## [23] dbplyr_1.4.2 shiny_1.4.0 ## [25] compiler_3.6.2 httr_1.4.1 ## [27] backports_1.1.5 assertthat_0.2.1 ## [29] Matrix_1.2-18 fastmap_1.0.1 ## [31] lazyeval_0.2.2 cli_2.0.1 ## [33] later_1.0.0 htmltools_0.4.0 ## [35] prettyunits_1.1.1 tools_3.6.2 ## [37] igraph_1.2.4.2 coda_0.19-3 ## [39] gtable_0.3.0 glue_1.3.1 ## [41] reshape2_1.4.3 cellranger_1.1.0 ## [43] vctrs_0.2.2 nlme_3.1-142 ## [45] blogdown_0.17 crosstalk_1.0.0 ## [47] xfun_0.12 ps_1.3.0 ## [49] rvest_0.3.5 mime_0.8 ## [51] miniUI_0.1.1.1 lifecycle_0.1.0 ## [53] gtools_3.8.1 zoo_1.8-7 ## [55] scales_1.1.0 colourpicker_1.0 ## [57] hms_0.5.3 promises_1.1.0 ## [59] Brobdingnag_1.2-6 parallel_3.6.2 ## [61] inline_0.3.15 shinystan_2.5.0 ## [63] RColorBrewer_1.1-2 yaml_2.2.1 ## [65] gridExtra_2.3 loo_2.2.0 ## [67] StanHeaders_2.19.0 reshape_0.8.8 ## [69] stringi_1.4.6 dygraphs_1.1.1.6 ## [71] pkgbuild_1.0.6 rlang_0.4.5 ## [73] pkgconfig_2.0.3 matrixStats_0.55.0 ## [75] evaluate_0.14 lattice_0.20-38 ## [77] rstantools_2.0.0 htmlwidgets_1.5.1 ## [79] labeling_0.3 processx_3.4.1 ## [81] tidyselect_1.0.0 plyr_1.8.5 ## [83] magrittr_1.5 bookdown_0.17 ## [85] R6_2.4.1 generics_0.0.2 ## [87] DBI_1.1.0 pillar_1.4.3 ## [89] haven_2.2.0 withr_2.1.2 ## [91] xts_0.12-0 abind_1.4-5 ## [93] modelr_0.1.5 crayon_1.3.4 ## [95] arrayhelpers_1.0-20160527 utf8_1.1.4 ## [97] rmarkdown_2.0 grid_3.6.2 ## [99] readxl_1.3.1 callr_3.4.1 ## [101] threejs_0.3.3 reprex_0.3.0 ## [103] digest_0.6.23 xtable_1.8-4 ## [105] httpuv_1.5.2 stats4_3.6.2 ## [107] munsell_0.5.0 shinyjs_1.1  ","date":1550275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550275200,"objectID":"f70534a142af3425568fd55e32d46a6b","permalink":"/post/bayesian-correlations-let-s-talk-options/","publishdate":"2019-02-16T00:00:00Z","relpermalink":"/post/bayesian-correlations-let-s-talk-options/","section":"post","summary":"tl;dr There’s more than one way to fit a Bayesian correlation in brms.\n Here’s the deal. In the last post, we considered how we might estimate correlations when our data contain influential outlier values. Our big insight was that if we use variants of Student’s \\(t\\)-distribution as the likelihood rather than the conventional normal distribution, our correlation estimates were less influenced by those outliers. And we mainly did that as Bayesians using the brms package.","tags":["Bayesian","brms","R","tutorial"],"title":"Bayesian Correlations: Let’s Talk Options.","type":"post"},{"authors":null,"categories":[],"content":" [edited June 18, 2019]\nIn this post, we’ll show how Student’s \\(t\\)-distribution can produce better correlation estimates when your data have outliers. As is often the case, we’ll do so as Bayesians.\nThis post is a direct consequence of Adrian Baez-Ortega’s great blog, “Bayesian robust correlation with Stan in R (and why you should use Bayesian methods)”. Baez-Ortega worked out the approach and code for direct use with Stan computational environment. That solution is great because Stan is free, open source, and very flexible. However, Stan’s interface might be prohibitively technical for non-statistician users. Happily, the brms package allows users to access the computational power of Stan through a simpler interface. In this post, we show how to extend Baez-Ortega’s method to brms. To pay respects where they’re due, the synthetic data, priors, and other model settings are largely the same as those Baez-Ortega used in his blog.\nI make assumptions For this post, I’m presuming you are vaguely familiar with linear regression, know about the basic differences between frequentist and Bayesian approaches to fitting models, and have a sense that the issue of outlier values is a pickle worth contending with. All code in is R, with a heavy use of the tidyverse–which you might learn a lot about here, especially chapter 5–, and, of course, Bürkner’s brms.\nIf you’d like a warmup, consider checking out my related post, Robust Linear Regression with Student’s \\(t\\)-Distribution.\n What’s the deal? Pearson’s correlations are designed to quantify the linear relationship between two normally distributed variables. The normal distribution and its multivariate generalization, the multivariate normal distribution, are sensitive to outliers. When you have well-behaved synthetic data, this isn’t an issue. But if you work real-world data, this can be a problem. One can have data for which the vast majority of cases are well-characterized by a nice liner relationship, but have a few odd cases for which that relationship does not hold. And if those odd cases happen to be overly influential–sometimes called leverage points–the resulting Pearson’s correlation coefficient might look off.\nRecall that the normal distribution is a special case of Student’s \\(t\\)-distribution with the \\(\\nu\\) parameter (i.e., nu, degree of freedom) set to infinity. As it turns out, when \\(\\nu\\) is small, Student’s \\(t\\)-distribution is more robust to multivariate outliers. It’s less influenced by them. I’m not going to cover why in any detail. For that you’ve got Baez-Ortega’s blog, an even earlier blog from Rasmus Bååth, and textbook treatments on the topic by Gelman \u0026amp; Hill (2007, chapter 6) and Kruschke (2014, chapter 16). Here we’ll get a quick sense of how vulnerable Pearson’s correlations–with their reliance on the Gaussian–are to outliers, we’ll demonstrate how fitting correlations within the Bayesian paradigm using the conventional Gaussian likelihood is similarly vulnerable to distortion, and then see how Student’s \\(t\\)-distribution can save the day. And importantly, we’ll do the bulk of this with the brms package.\n We need data To start off, we’ll make a multivariate normal simulated data set using the same steps Baez-Ortega’s used.\nlibrary(mvtnorm) library(tidyverse) sigma \u0026lt;- c(20, 40) # the variances rho \u0026lt;- -.95 # the desired correlation # here\u0026#39;s the variance/covariance matrix cov.mat \u0026lt;- matrix(c(sigma[1] ^ 2, sigma[1] * sigma[2] * rho, sigma[1] * sigma[2] * rho, sigma[2] ^ 2), nrow = 2, byrow = T) # after setting our seed, we\u0026#39;re ready to simulate with `rmvnorm()` set.seed(210191) x.clean \u0026lt;- rmvnorm(n = 40, sigma = cov.mat) %\u0026gt;% as_tibble() %\u0026gt;% rename(x = V1, y = V2) Here we make our second data set, x.noisy, which is identical to our well-behaved x.clean data, but with the first three cases transformed to outlier values.\nx.noisy \u0026lt;- x.clean x.noisy[1:3,] \u0026lt;- matrix(c(-40, -60, 20, 100, 40, 40), nrow = 3, byrow = T) Finally, we’ll add an outlier index to the data sets, which will help us with plotting.\nx.clean \u0026lt;- x.clean %\u0026gt;% mutate(outlier = factor(0)) x.noisy \u0026lt;- x.noisy %\u0026gt;% mutate(outlier = c(rep(1, 3), rep(0, 37)) %\u0026gt;% as.factor(.)) The plot below shows what the x.clean data look like. I’m a fan of FiveThirtyEight, so we’ll use a few convenience functions from the handy ggthemes package to give our plots a FiveThirtyEight-like feel.\nlibrary(ggthemes) x.clean %\u0026gt;% ggplot(aes(x = x, y = y, color = outlier, fill = outlier)) + geom_point() + stat_ellipse(geom = \u0026quot;polygon\u0026quot;, alpha = .15, size = .15, level = .5) + stat_ellipse(geom = \u0026quot;polygon\u0026quot;, alpha = .15, size = .15, level = .95) + scale_color_fivethirtyeight() + scale_fill_fivethirtyeight() + coord_cartesian(xlim = -50:50, ylim = -100:100) + theme_fivethirtyeight() + theme(legend.position = \u0026quot;none\u0026quot;) And here are the x.noisy data.\nx.noisy %\u0026gt;% ggplot(aes(x = x, y = y, color = outlier, fill = outlier)) + geom_point() + stat_ellipse(geom = \u0026quot;polygon\u0026quot;, alpha = .15, size = .15, level = .5) + stat_ellipse(geom = \u0026quot;polygon\u0026quot;, alpha = .15, size = .15, level = .95) + scale_color_fivethirtyeight() + scale_fill_fivethirtyeight() + coord_cartesian(xlim = -50:50, ylim = -100:100) + theme_fivethirtyeight() + theme(legend.position = \u0026quot;none\u0026quot;) The three outliers are in red. Even in their presence, the old interocular trauma test suggests there is a pronounced overall trend in the data. I would like a correlation procedure that’s capable of capturing that overall trend. Let’s examine some candidates.\n How does old Pearson hold up? A quick way to get a Pearson’s correlation coefficient in R is with the cor() function, which does a nice job recovering the correlation we simulated the x.clean data with:\ncor(x.clean$x, x.clean$y) ## [1] -0.959702 However, things fall apart if you use cor() on the x.noisy data.\ncor(x.noisy$x, x.noisy$y) ## [1] -0.6365649 So even though most of the x.noisy data continue to show a clear strong relation, three outlier values reduced the Pearson’s correlation a third of the way toward zero. Let’s see what happens when we go Bayesian.\n Bayesian correlations in brms Bürkner’s brms is a general purpose interface for fitting all manner of Bayesian regression models with Stan as the engine under the hood. It has popular lme4-like syntax and offers a variety of convenience functions for post processing. Let’s load it up.\nlibrary(brms) First with the Gaussian likelihood. I’m not going to spend a lot of time walking through the syntax in the main brms function, brm(). You can learn all about that here or with my project Statistical Rethinking with brms, ggplot2, and the tidyverse. But our particular use of brm() requires we make a few fine points.\nOne doesn’t always think about bivariate correlations within the regression paradigm. But they work just fine. Within brms, you would typically specify the conventional Gaussian likelihood (i.e., family = gaussian), use the mvbind() syntax to set up a multivariate model, and fit that model without predictors. For each variable specified in cbind(), you’ll estimate an intercept (i.e., mean, \\(\\mu\\)) and sigma (i.e., \\(\\sigma\\), often called a residual variance). Since there are no predictors in the model, the residual variance is just the variance and the brms default for multivariate models is to allow the residual variances to covary. But since variances are parameterized in the standard deviation metric in brms, the residual variances and their covariance are SDs and their correlation, respectively.\nHere’s what it looks like in practice.\nf0 \u0026lt;- brm(data = x.clean, family = gaussian, mvbind(x, y) ~ 1, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 100), class = sigma, resp = x), prior(normal(0, 100), class = sigma, resp = y), prior(lkj(1), class = rescor)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 210191) ## Warning: Specifying global priors for regression coefficients in multivariate ## models is deprecated and may not work as expected. In a typical Bayesian workflow, you’d examine the quality of the chains with trace plots. The easy way to do that in brms is with plot(). E.g., to get the trace plots for our first model, you’d code plot(f0). Happily, the trace plots look fine for all models in this post. For the sake of space, I’ll leave their inspection as exercises for interested readers.\nOur priors and such mirror those in Baez-Ortega’s blog. Here are the results.\nprint(f0) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: x ~ 1 ## y ~ 1 ## Data: x.clean (Number of observations: 40) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## x_Intercept -2.83 3.33 -9.33 3.69 1.00 3000 3111 ## y_Intercept 3.55 6.65 -9.45 16.60 1.00 2978 2928 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_x 21.47 2.47 17.29 26.99 1.00 2514 3094 ## sigma_y 42.93 4.86 34.55 53.51 1.00 2477 3144 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## rescor(x,y) -0.95 0.02 -0.98 -0.92 1.00 2686 3218 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Way down there in the last line in the ‘Family Specific Parameters’ section we have rescor(x,y), which is our correlation. And indeed, our Gaussian intercept-only multivariate model did a great job recovering the correlation we used to simulate the x.clean data with. Look at what happens when we try this approach with x.noisy.\nf1 \u0026lt;- update(f0, newdata = x.noisy, iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 210191) print(f1) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: x ~ 1 ## y ~ 1 ## Data: x.noisy (Number of observations: 40) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## x_Intercept -2.95 3.75 -10.39 4.57 1.00 4515 3963 ## y_Intercept 6.52 7.45 -8.31 20.98 1.00 4727 4057 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_x 23.65 2.76 18.97 29.83 1.00 4536 4351 ## sigma_y 47.20 5.42 37.94 59.03 1.00 4619 4222 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## rescor(x,y) -0.61 0.10 -0.78 -0.39 1.00 4344 4033 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). And the correlation estimate is -.61. As it turns out, data = x.noisy + family = gaussian in brm() failed us just like Pearson’s correlation failed us. Time to leave failure behind.\n Now with Student’s \\(t\\)-distribution. Before we jump into using family = student, we should talk a bit about \\(\\nu\\). This is our new parameter which is silently fixed to infinity when we use the Gaussian likelihood. The \\(\\nu\\) parameter is bound at zero but, as discussed in Baez-Ortega’s blog, is somewhat nonsensical for values below 1. As it turns out, \\(\\nu\\) is constrained to be equal to or greater than 1 in brms. So nothing for us to worry about, there. The Stan team currently recommends the gamma(2, 0.1) prior for \\(\\nu\\), which is also the current brms default. This is what that distribution looks like.\ntibble(x = seq(from = 1, to = 120, by = .5)) %\u0026gt;% ggplot(aes(x = x, fill = factor(0))) + geom_ribbon(aes(ymin = 0, ymax = dgamma(x, 2, 0.1))) + scale_y_continuous(NULL, breaks = NULL) + scale_fill_fivethirtyeight() + coord_cartesian(xlim = 0:100) + ggtitle(\u0026quot;gamma(2, 0.1)\u0026quot;) + theme_fivethirtyeight() + theme(legend.position = \u0026quot;none\u0026quot;) So gamma(2, 0.1) should gently push the \\(\\nu\\) posterior toward low values, but it’s slowly-sloping right tail will allow higher values to emerge.\nFollowing the Stan team’s recommendation, the brms default and Baez-Ortega’s blog, here’s our robust Student’s \\(t\\) model for the x.noisy data.\nf2 \u0026lt;- brm(data = x.noisy, family = student, mvbind(x, y) ~ 1, prior = c(prior(gamma(2, .1), class = nu), prior(normal(0, 100), class = Intercept), prior(normal(0, 100), class = sigma, resp = x), prior(normal(0, 100), class = sigma, resp = y), prior(lkj(1), class = rescor)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 210191) ## Warning: Specifying global priors for regression coefficients in multivariate ## models is deprecated and may not work as expected. print(f2) ## Family: MV(student, student) ## Links: mu = identity; sigma = identity; nu = identity ## mu = identity; sigma = identity; nu = identity ## Formula: x ~ 1 ## y ~ 1 ## Data: x.noisy (Number of observations: 40) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## x_Intercept -2.17 3.69 -9.56 5.15 1.00 3003 3307 ## y_Intercept 2.03 7.31 -12.41 16.40 1.00 3114 3030 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_x 18.39 3.06 13.10 24.97 1.00 2858 3298 ## sigma_y 36.60 6.05 25.96 49.69 1.00 2861 3302 ## nu 2.64 0.92 1.39 4.91 1.00 3740 3017 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## rescor(x,y) -0.93 0.03 -0.97 -0.85 1.00 3824 4117 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Whoa, look at that correlation, rescore(x,y)! It’s right about what we’d hope for. Sure, it’s not a perfect -.95, but that’s way better than -.61.\nWhile we’re at it, we may as well see what happens when we fit a Student’s \\(t\\) model when we have perfectly multivariate normal data. Here it is with the x.clean data.\nf3 \u0026lt;- update(f2, newdata = x.clean, iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 210191) print(f3) ## Family: MV(student, student) ## Links: mu = identity; sigma = identity; nu = identity ## mu = identity; sigma = identity; nu = identity ## Formula: x ~ 1 ## y ~ 1 ## Data: x.clean (Number of observations: 40) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## x_Intercept -2.41 3.45 -9.31 4.23 1.00 2790 2774 ## y_Intercept 2.86 6.87 -10.29 16.18 1.00 2786 2808 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_x 20.68 2.61 16.17 26.36 1.00 2686 3105 ## sigma_y 41.13 5.22 32.15 52.45 1.00 2705 2923 ## nu 22.68 13.71 5.50 57.49 1.00 4563 3909 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## rescor(x,y) -0.96 0.01 -0.98 -0.92 1.00 3415 3243 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). So when you don’t need Student’s \\(t\\), it yields the right answer anyways. That’s a nice feature.\nWe should probably compare the posteriors of the correlations across the four models. First we’ll collect the posterior samples into a tibble.\nposts \u0026lt;- tibble(model = str_c(\u0026quot;f\u0026quot;, 0:3)) %\u0026gt;% mutate(fit = map(model, get)) %\u0026gt;% mutate(post = map(fit, posterior_samples)) %\u0026gt;% unnest(post) head(posts) ## # A tibble: 6 x 9 ## model fit b_x_Intercept b_y_Intercept sigma_x sigma_y rescor__x__y lp__ ## \u0026lt;chr\u0026gt; \u0026lt;lis\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 f0 \u0026lt;brm… 1.31 -5.60 18.2 37.8 -0.947 -353. ## 2 f0 \u0026lt;brm… -7.41 10.6 25.2 50.5 -0.941 -357. ## 3 f0 \u0026lt;brm… -4.51 5.65 23.3 49.4 -0.975 -354. ## 4 f0 \u0026lt;brm… -2.65 -0.597 18.3 37.3 -0.929 -354. ## 5 f0 \u0026lt;brm… -2.76 -1.50 18.4 37.5 -0.923 -355. ## 6 f0 \u0026lt;brm… -9.84 15.2 26.3 45.1 -0.953 -358. ## # … with 1 more variable: nu \u0026lt;dbl\u0026gt; With the posterior draws in hand, we just need to wrangle a bit before showing the correlation posteriors in a coefficient plot. To make things easier, we’ll do so with a couple convenience functions from the tidybayes package.\nlibrary(tidybayes) # wrangle posts %\u0026gt;% group_by(model) %\u0026gt;% median_qi(rescor__x__y, .width = c(.5, .95)) %\u0026gt;% mutate(key = recode(model, f0 = \u0026quot;Gaussian likelihood with clean data\u0026quot;, f1 = \u0026quot;Gaussian likelihood with noisy data\u0026quot;, f2 = \u0026quot;Student likelihood with noisy data\u0026quot;, f3 = \u0026quot;Student likelihood with clean data\u0026quot;), clean = ifelse(model %in% c(\u0026quot;f0\u0026quot;, \u0026quot;f3\u0026quot;), \u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;)) %\u0026gt;% # plot ggplot(aes(x = rescor__x__y, y = key, color = clean)) + geom_pointintervalh() + scale_color_fivethirtyeight() + coord_cartesian(xlim = -1:0) + labs(subtitle = expression(paste(\u0026quot;The posterior for \u0026quot;, rho, \u0026quot; depends on the likelihood. Why not go robust and use Student\u0026#39;s \u0026quot;, italic(t), \u0026quot;?\u0026quot;))) + theme_fivethirtyeight() + theme(axis.text.y = element_text(hjust = 0), legend.position = \u0026quot;none\u0026quot;) From our tidybayes::median_qi() code, the dots are the posterior medians, the thick inner lines the 50% intervals, and the thinner outer lines the 95% intervals. The posteriors for the x.noisy data are in red and those for the x.clean data are in blue. If the data are clean multivariate normal Gaussian or if they’re dirty but fit with robust Student’s \\(t\\), everything is pretty much alright. But whoa, if you fit a correlation with a combination of family = gaussian and noisy outlier-laden data, man that’s just a mess.\nDon’t let a few overly-influential outliers make a mess of your analyses. Try the robust Student’s \\(t\\).\nsessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.1.9000 brms_2.12.0 Rcpp_1.0.3 ## [4] ggthemes_4.2.0 forcats_0.4.0 stringr_1.4.0 ## [7] dplyr_0.8.4 purrr_0.3.3 readr_1.3.1 ## [10] tidyr_1.0.2 tibble_2.1.3 ggplot2_3.2.1 ## [13] tidyverse_1.3.0 mvtnorm_1.0-12 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.1.5 ## [3] Hmisc_4.3-0 plyr_1.8.5 ## [5] igraph_1.2.4.2 lazyeval_0.2.2 ## [7] svUnit_0.7-12 splines_3.6.2 ## [9] crosstalk_1.0.0 rstantools_2.0.0 ## [11] inline_0.3.15 digest_0.6.23 ## [13] htmltools_0.4.0 rsconnect_0.8.16 ## [15] gdata_2.18.0 fansi_0.4.1 ## [17] magrittr_1.5 checkmate_1.9.4 ## [19] cluster_2.1.0 modelr_0.1.5 ## [21] matrixStats_0.55.0 xts_0.12-0 ## [23] prettyunits_1.1.1 jpeg_0.1-8.1 ## [25] colorspace_1.4-1 rvest_0.3.5 ## [27] pan_1.6 haven_2.2.0 ## [29] xfun_0.12 callr_3.4.1 ## [31] crayon_1.3.4 jsonlite_1.6.1 ## [33] lme4_1.1-21 survival_3.1-8 ## [35] zoo_1.8-7 glue_1.3.1 ## [37] gtable_0.3.0 pkgbuild_1.0.6 ## [39] weights_1.0.1 rstan_2.19.2 ## [41] jomo_2.6-10 abind_1.4-5 ## [43] scales_1.1.0 DBI_1.1.0 ## [45] miniUI_0.1.1.1 xtable_1.8-4 ## [47] htmlTable_1.13.3 foreign_0.8-72 ## [49] Formula_1.2-3 stats4_3.6.2 ## [51] StanHeaders_2.19.0 DT_0.11 ## [53] htmlwidgets_1.5.1 httr_1.4.1 ## [55] threejs_0.3.3 arrayhelpers_1.0-20160527 ## [57] RColorBrewer_1.1-2 acepack_1.4.1 ## [59] mice_3.7.0 pkgconfig_2.0.3 ## [61] loo_2.2.0 farver_2.0.3 ## [63] nnet_7.3-12 dbplyr_1.4.2 ## [65] utf8_1.1.4 tidyselect_1.0.0 ## [67] labeling_0.3 rlang_0.4.5 ## [69] reshape2_1.4.3 later_1.0.0 ## [71] munsell_0.5.0 cellranger_1.1.0 ## [73] tools_3.6.2 cli_2.0.1 ## [75] generics_0.0.2 broom_0.5.3 ## [77] ggridges_0.5.2 evaluate_0.14 ## [79] fastmap_1.0.1 yaml_2.2.1 ## [81] processx_3.4.1 knitr_1.26 ## [83] fs_1.3.1 mitml_0.3-7 ## [85] nlme_3.1-142 mime_0.8 ## [87] xml2_1.2.2 compiler_3.6.2 ## [89] bayesplot_1.7.1 shinythemes_1.1.2 ## [91] rstudioapi_0.10 png_0.1-7 ## [93] reprex_0.3.0 stringi_1.4.6 ## [95] ps_1.3.0 blogdown_0.17 ## [97] Brobdingnag_1.2-6 lattice_0.20-38 ## [99] Matrix_1.2-18 nloptr_1.2.1 ## [101] markdown_1.1 shinyjs_1.1 ## [103] vctrs_0.2.2 pillar_1.4.3 ## [105] lifecycle_0.1.0 bridgesampling_0.8-1 ## [107] data.table_1.12.8 httpuv_1.5.2 ## [109] R6_2.4.1 latticeExtra_0.6-29 ## [111] bookdown_0.17 promises_1.1.0 ## [113] gridExtra_2.3 boot_1.3-23 ## [115] colourpicker_1.0 MASS_7.3-51.4 ## [117] gtools_3.8.1 assertthat_0.2.1 ## [119] withr_2.1.2 shinystan_2.5.0 ## [121] parallel_3.6.2 hms_0.5.3 ## [123] grid_3.6.2 rpart_4.1-15 ## [125] minqa_1.2.4 coda_0.19-3 ## [127] rmarkdown_2.0 shiny_1.4.0 ## [129] lubridate_1.7.4 base64enc_0.1-3 ## [131] dygraphs_1.1.1.6   ","date":1549756800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549756800,"objectID":"903cf2640e55664b16933121a277d179","permalink":"/post/bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/","publishdate":"2019-02-10T00:00:00Z","relpermalink":"/post/bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/","section":"post","summary":"[edited June 18, 2019]\nIn this post, we’ll show how Student’s \\(t\\)-distribution can produce better correlation estimates when your data have outliers. As is often the case, we’ll do so as Bayesians.\nThis post is a direct consequence of Adrian Baez-Ortega’s great blog, “Bayesian robust correlation with Stan in R (and why you should use Bayesian methods)”. Baez-Ortega worked out the approach and code for direct use with Stan computational environment.","tags":["Bayesian","brms","outlier","R","robust","tutorial"],"title":"Bayesian robust correlations with brms (and why you should love Student’s $t$)","type":"post"},{"authors":null,"categories":[],"content":" I’ve been reading David France’s How to Survive a Plague: The Inside Story of How Citizens and Science Tamed AIDS. It’s a masterwork. And it’s devastating. Two months in and I haven’t cracked 100 pages. HSP is the kind of book I can only take 10—20 pages at a time. But like anything soulful and hard and true, it’s worth it.\nFor example, what do you know about the first baby on record to have died of AIDS?\n In San Francisco [November 1982], Dr. Arthur Ammann was feeling frustrated as he studied the results of a bone marrow test he had ordered on a very sick toddler. As an expert in pediatric immunology who traveled regularly throughout Africa, Ammann thought he’d either seen or read about every immune disorder that cold plague a child… But this little boy baffled him. He was born prematurely on March 3, 1981, with pronounced jaundice, a problem caused by toxins accumulating in the blood. This was not extraordinary, and the standard course of treatment was followed: every ounce of his contaminated blood was replaced with donated supplies. The process was repeated five times over a four-day period, followed by additional infusions of blood products like packed red blood cells and platelets. (p. 71)\n Just that alone—fuck.\nAfter the initial bout of treatment, it looked like he was getting better. Dr. Ammann sent him home with his parents.\n But at four months of age his health began to dive. He suffered an enlarged spleen and liver. Jaundice returned, followed by hepatitis of no known origin, then anemia and diarrhea. Now the little boy was twenty months old and in intensive care. (p. 71)\n By mid-1982, gay men, Haitians, and intravenous drug users were known to be at risk for AIDS. But it wasn’t yet clear why.\n Ammann suspected an infection in the child’s bone marrow, which would be highly unusual. Test results were even more surprising than he’d imagined. The child’s culture was positive for Mycobacterium avium-intracellulare, the dreaded cause of wasting syndrome in adults with AIDS. Recently there had been a number of reports of babies who seemed to inherit the disease at birth from their sick mothers, but that was not the case here. Ammann wrote in his case notes that both parents of his patient were “heterosexual non-Haitians and do not have a history of intravenous drug abuse.” He submitted both [parents] to extensive testing, and found no signs of immune deficiency.\nAll he could think was: The blood supply is contaminated. (p. 71, emphasis in the original)\n As is turned out, gay men were particularly generous blood donors at that time.\n Gay men, he learned, were extremely avid blood donors. In fact, in recent months an unnoticed and massive blood drive had been under way in LA’s gay neighborhoods in response to the mounting GRID [i.e., gay-related immune deficiency, as AIDS was known by in the early days] crisis, there. Week after week long lines of men rolled up their sleeves to donate blood, dutifully offering up pint after pint of harm they never dreamed of. (p. 59)\n This was the precursor to a 30-year ban on gay men donating blood—which was overturned in 2015 based on advances in the relevant scientific literature.\n“Pulling the boy’s hospital records, he saw that blood donations from twenty-one separate people had been transfused into the child. Their identities were masked” (p. 71). After some efforts, Ammann and his team determined the relevant donor.\n A man in his late forties… He had donated blood in early 1981. His health had remained unremarkable until that October, when he complained of fatigue, swollen lymph glands, and clouded vision in one eye—classic AIDS symptoms. Doctors diagnosed PCP [i.e., pneumocystis carinii pneumonia, an inflammation and fluid buildup in the lungs most of us conquer in early childhood] in December, and he was dead nine months later.\nThis was the first irrefutable evidence of transmission through the blood supply. (p. 72)\n Shortly after, Ammann and notified the medical community via the CDC’s widely-read Morbidity and Mortality Weekly Report platform. His publication choice was crucial. Had he tried to get published at a more prestigious outlet, such as the New England Journal of Medicine, the peer-review process could have held up the message for months or more. Ammann needed to get the message out to practitioners as soon as possible. The publication lag for the MMWR was trivial. Here’s the report. In an editorial note at the end of the report, we read\n Of the 788 definite AIDS cases among adults reported thus far to CDC, 42 (5.3%) belong to no known risk group (i.e., they are not known to be homosexually active men, intravenous drug abusers, Haitians, or hemophiliacs). Two cases received blood products within 2 years of the onset of their illnesses and are currently under investigation.\n Seven hundred and seventy-eight.\nAccording to the World Health Organization, about 37 million people were living with AIDS in 2017.\nShortly after Ammann’s piece was published,\n a scrum of journalists burst through the CDC’s door for the first time, including correspondents from countless television networks and affiliates. The attention was long overdue. But with images of bouncing toddlers, the reporters warned America that the gay disease was now killing children. It unleashed a torrent of anti-gay violence the likes of which the community had never seen before. (p. 73)\n If you find this topic sad and compelling and worth the heartbreak, there’s more to come.\n","date":1549065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549065600,"objectID":"1bc3af967479ce16b94b9b69f3e1603b","permalink":"/post/how-to-survive-a-plague-part-1-n-of-a-premature-book-report/","publishdate":"2019-02-02T00:00:00Z","relpermalink":"/post/how-to-survive-a-plague-part-1-n-of-a-premature-book-report/","section":"post","summary":"I’ve been reading David France’s How to Survive a Plague: The Inside Story of How Citizens and Science Tamed AIDS. It’s a masterwork. And it’s devastating. Two months in and I haven’t cracked 100 pages. HSP is the kind of book I can only take 10—20 pages at a time. But like anything soulful and hard and true, it’s worth it.\nFor example, what do you know about the first baby on record to have died of AIDS?","tags":["HIV/AIDS"],"title":"\"How to Survive a Plague\": Part 1/$n$ of a premature book report","type":"post"},{"authors":null,"categories":[],"content":" [edited Feb 3, 2019]\nThe purpose of this post is to demonstrate the advantages of the Student’s \\(t\\)-distribution for regression with outliers, particularly within a Bayesian framework.\nI make assumptions I’m presuming you are familiar with linear regression, familiar with the basic differences between frequentist and Bayesian approaches to fitting regression models, and have a sense that the issue of outlier values is a pickle worth contending with. All code in is R, with a heavy use of the tidyverse–which you might learn a lot about here, especially chapter 5– and Paul Bürkner’s brms package.\n The problem Simple regression models typically use the Gaussian likelihood. Say you have some criterion variable \\(y\\), which you can reasonably describe with a mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Further, you’d like to describe \\(y\\) with a predictor \\(x\\). Using the Gaussian likelihood, we can describe the model as\n\\[ \\begin{eqnarray} y_i \u0026amp; \\sim \u0026amp; \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i \u0026amp; = \u0026amp; \\beta_0 + \\beta_1 x_i \\end{eqnarray} \\]\nWith this formulation, we use \\(x\\) to model the mean of \\(y\\). The \\(\\beta_0\\) parameter is the intercept of the regression model and \\(\\beta_1\\) is its slope with respect to \\(x\\). After accounting for \\(y\\)’s relation with \\(x\\), the leftover variability in \\(y\\) is described by \\(\\sigma\\), often called error or residual variance. The reason we describe the model in terms of \\(\\mu\\) and \\(\\sigma\\) is because those are the two parameters by which we define the Normal distribution, the Gaussian likelihood.\nThe Gaussian is a sensible default choice for many data types. You might say it works unreasonably well. Unfortunately, the normal (i.e., Gaussian) distribution is sensitive to outliers.\nThe normal distribution is a special case of Student’s \\(t\\)-distribution with the \\(\\nu\\) parameter (i.e., the degree of freedom) set to infinity. However, when \\(\\nu\\) is small, Student’s \\(t\\)-distribution is more robust to multivariate outliers. See Gelman \u0026amp; Hill (2007, chapter 6) or Kruschke (2014, chapter 16) for textbook treatments on the topic.\nIn this post, we demonstrate how vulnerable the Gaussian likelihood is to outliers and then compare it to different ways of using Student’s \\(t\\)-likelihood for the same data.\nFirst, we’ll get a sense of the distributions with a plot.\nlibrary(tidyverse) tibble(x = seq(from = -6, to = 6, by = .01)) %\u0026gt;% expand(x, nu = c(1, 2.5, 5, 10, Inf)) %\u0026gt;% mutate(density = dt(x = x, df = nu), nu = factor(nu, levels = c(\u0026quot;Inf\u0026quot;, \u0026quot;10\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;2.5\u0026quot;, \u0026quot;1\u0026quot;))) %\u0026gt;% ggplot(aes(x = x, y = density, group = nu, color = nu)) + geom_line() + scale_color_viridis_d(expression(nu), direction = 1, option = \u0026quot;C\u0026quot;, end = .85) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = -5:5) + xlab(NULL) + theme(panel.grid = element_blank()) So the difference is that a Student’s \\(t\\)-distribution with a low \\(\\nu\\) will have notably heavier tails than the conventional Gaussian distribution. It’s easiest to see the difference when \\(\\nu\\) approaches 1. Even then, the difference can be subtle when looking at a plot. Another way is to compare how probable relatively extreme values are in a Student’s \\(t\\)-distribution relative to the Gaussian. For the sake of demonstration, here we’ll compare Gauss with Student’s \\(t\\) with a \\(\\nu\\) of 5. In the plot above, they are clearly different, but not shockingly so. However, that difference is very notable in the tails.\nLet’s look more closely with a table. Below, we compare the probability of a given z-score or lower within the Gaussian and a \\(\\nu = 5\\) Student’s \\(t\\). In the rightmost column, we compare the probabilities in a ratio.\n# Here we pic our nu nu \u0026lt;- 5 tibble(z_score = 0:-5, p_Gauss = pnorm(z_score, mean = 0, sd = 1), p_Student_t = pt(z_score, df = nu), `Student/Gauss ratio` = p_Student_t/p_Gauss) %\u0026gt;% mutate_if(is.double, round, digits = 5) %\u0026gt;% knitr::kable()   z_score p_Gauss p_Student_t Student/Gauss ratio    0 0.50000 0.50000 1.00000  -1 0.15866 0.18161 1.14468  -2 0.02275 0.05097 2.24042  -3 0.00135 0.01505 11.14871  -4 0.00003 0.00516 162.97775  -5 0.00000 0.00205 7159.76534    Note how low z-scores are more probable in this Student’s \\(t\\) than in the Gaussian. This is most apparent in the Student/Gauss ratio column on the right. A consequence of this is that extreme scores are less influential to your solutions when you use a small-\\(\\nu\\) Student’s \\(t\\)-distribution in place of the Gaussian. That is, the small-\\(\\nu\\) Student’s \\(t\\) is more robust than the Gaussian to unusual and otherwise influential observations.\nIn order to demonstrate, let’s simulate our own. We’ll start by creating multivariate normal data.\n Let’s create our initial tibble of well-behaved data, d First, we’ll need to define our variance/covariance matrix.\ns \u0026lt;- matrix(c(1, .6, .6, 1), nrow = 2, ncol = 2) By the two .6s on the off-diagonal positions, we indicated we’d like our two variables to have a correlation of .6.\nSecond, our variables also need means, which we’ll define with a mean vector.\nm \u0026lt;- c(0, 0) With means of 0 and variances of 1, our data are in a standardized metric.\nThird, we’ll use the mvrnorm() function from the MASS package to simulate our data.\nset.seed(3) d \u0026lt;- MASS::mvrnorm(n = 100, mu = m, Sigma = s) %\u0026gt;% as_tibble() %\u0026gt;% rename(y = V1, x = V2) The first few rows look like so:\nhead(d) ## # A tibble: 6 x 2 ## y x ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -1.14 -0.584 ## 2 -0.0805 -0.443 ## 3 -0.239 0.702 ## 4 -1.30 -0.761 ## 5 -0.280 0.630 ## 6 -0.245 0.299 As an aside, check out this nice r-bloggers post for more information on simulating data with this method.\nAnyway, this line reorders our data by x, placing the smallest values on top.\nd \u0026lt;- d %\u0026gt;% arrange(x) head(d) ## # A tibble: 6 x 2 ## y x ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -2.21 -1.84 ## 2 -1.27 -1.71 ## 3 -0.168 -1.60 ## 4 -0.292 -1.46 ## 5 -0.785 -1.40 ## 6 -0.157 -1.37  Let’s create our outlier tibble, o Here we’ll make two outlying and unduly influential values.\no \u0026lt;- d o[c(1:2), 1] \u0026lt;- c(5, 4) head(o) ## # A tibble: 6 x 2 ## y x ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 5 -1.84 ## 2 4 -1.71 ## 3 -0.168 -1.60 ## 4 -0.292 -1.46 ## 5 -0.785 -1.40 ## 6 -0.157 -1.37 With the code, above, we replaced the first two values of our first variable, y. They both started out quite negative. Now they are positive values of a large magnitude within the standardized metric.\n Frequentist OLS models To get a quick sense of what we’ve done, we’ll first fit two models with OLS regression via the lm() function. The first model, ols0, is of the multivariate normal data, d. The second model, ols1, is on the otherwise identical data with the two odd and influential values, o. Here is our model code.\nols0 \u0026lt;- lm(data = d, y ~ 1 + x) ols1 \u0026lt;- lm(data = o, y ~ 1 + x) We’ll use the broom package to assist with model summaries and other things.\nHere are the parameter estimates for the first model.\nlibrary(broom) tidy(ols0) %\u0026gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -0.01 0.09 -0.08 0.94 ## 2 x 0.45 0.1 4.55 0 And now the parameters for the second model, the one based on the o outlier data.\ntidy(ols1) %\u0026gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.12 0.11 1.12 0.26 ## 2 x 0.15 0.13 1.21 0.23 Just two odd and influential values dramatically changed the model parameters, particularly the slope. Let’s plot the data and the models to get a visual sense of what happened.\n# The well-behaived data p1 \u0026lt;- ggplot(data = d, aes(x = x, y = y)) + stat_smooth(method = \u0026quot;lm\u0026quot;, color = \u0026quot;grey92\u0026quot;, fill = \u0026quot;grey67\u0026quot;, alpha = 1, fullrange = T) + geom_point(size = 1, alpha = 3/4) + scale_x_continuous(limits = c(-4, 4)) + coord_cartesian(xlim = -3:3, ylim = -3:5) + labs(title = \u0026quot;No Outliers\u0026quot;) + theme(panel.grid = element_blank()) # The data with two outliers p2 \u0026lt;- ggplot(data = o, aes(x = x, y = y, color = y \u0026gt; 3)) + stat_smooth(method = \u0026quot;lm\u0026quot;, color = \u0026quot;grey92\u0026quot;, fill = \u0026quot;grey67\u0026quot;, alpha = 1, fullrange = T) + geom_point(size = 1, alpha = 3/4) + scale_color_viridis_d(option = \u0026quot;A\u0026quot;, end = 4/7) + scale_x_continuous(limits = c(-4, 4)) + coord_cartesian(xlim = -3:3, ylim = -3:5) + labs(title = \u0026quot;Two Outliers\u0026quot;) + theme(panel.grid = element_blank(), legend.position = \u0026quot;none\u0026quot;) library(gridExtra) grid.arrange(p1, p2, ncol = 2) The two outliers were quite influential on the slope. It went from a nice clear diagonal to almost horizontal. You’ll also note how the 95% intervals (i.e., the bowtie shapes) were a bit wider when based on the o data.\nOne of the popular ways to quantify outlier status is with Mahalanobis’ distance. However, the Mahalanobis distance is primarilly valid for multivariate normal data. Though the data in this example are indeed multivariate normal–or at least they were before we injected two outlying values into them–I am going to resist relying on Mahalanobis’ distance. There are other more general approaches that will be of greater use when you need to explore other variants of the generalized linear model. The broom::augment() function will give us access to one.\naug0 \u0026lt;- augment(ols0) aug1 \u0026lt;- augment(ols1) glimpse(aug1) ## Observations: 100 ## Variables: 9 ## $ y \u0026lt;dbl\u0026gt; 5.00000000, 4.00000000, -0.16783167, -0.29164105, -0.78491… ## $ x \u0026lt;dbl\u0026gt; -1.8439208, -1.7071418, -1.5996509, -1.4601550, -1.3954395… ## $ .fitted \u0026lt;dbl\u0026gt; -0.155937416, -0.135213012, -0.118926273, -0.097790209, -0… ## $ .se.fit \u0026lt;dbl\u0026gt; 0.2581834, 0.2427649, 0.2308204, 0.2155907, 0.2086463, 0.2… ## $ .resid \u0026lt;dbl\u0026gt; 5.15593742, 4.13521301, -0.04890540, -0.19385084, -0.69693… ## $ .hat \u0026lt;dbl\u0026gt; 0.05521164, 0.04881414, 0.04412882, 0.03849763, 0.03605748… ## $ .sigma \u0026lt;dbl\u0026gt; 0.964211, 1.017075, 1.104423, 1.104253, 1.102081, 1.104410… ## $ .cooksd \u0026lt;dbl\u0026gt; 6.809587e-01, 3.820802e-01, 4.783890e-05, 6.480561e-04, 7.… ## $ .std.resid \u0026lt;dbl\u0026gt; 4.82755612, 3.85879897, -0.04552439, -0.17992001, -0.64603… Here we can compare the observations with Cook’s distance, \\(D_i\\) (i.e., .cooksd). Cook’s \\(D_i\\) is a measure of the influence of a given observation on the model. To compute \\(D_i\\), the model is fit once for each \\(n\\) case, after first dropping that case. Then the difference in the model with all observations and the model with all observations but the \\(i\\)th observation, as defined by the Euclidian distance between the estimators. Fahrmeir et al (2013, p. 166) suggest that within the OLS framework “as a rule of thumb, observations with \\(D_i\\) \u0026gt; 0.5 are worthy of attention, and observations with \\(D_i\\) \u0026gt; 1 should always be examined.” Here we plot \\(D_i\\) against our observation index, \\(i\\), for both models.\naug0 %\u0026gt;% # The well-behaived data mutate(i = 1:n()) %\u0026gt;% bind_rows( # The data with two outliers aug1 %\u0026gt;% mutate(i = 1:n()) ) %\u0026gt;% mutate(fit = rep(c(\u0026quot;fit b0\u0026quot;, \u0026quot;fit b1\u0026quot;), each = n()/2)) %\u0026gt;% ggplot(aes(x = i, y = .cooksd)) + geom_hline(yintercept = .5, color = \u0026quot;white\u0026quot;) + geom_point(alpha = .5) + geom_text(data = tibble(i = 46, .cooksd = .53, fit = \u0026quot;fit b0\u0026quot;), label = \u0026quot;Fahrmeir et al said we might worry around here\u0026quot;, color = \u0026quot;grey50\u0026quot;) + coord_cartesian(ylim = c(0, .7)) + theme(panel.grid = element_blank(), axis.title.x = element_text(face = \u0026quot;italic\u0026quot;, family = \u0026quot;Times\u0026quot;)) + facet_wrap(~fit) For the model of the well-behaved data, ols0, we have \\(D_i\\) values all hovering near zero. However, the plot for ols1 shows one \\(D_i\\) value well above the 0.5 level and another not quite that high but deviant relative to the rest. Our two outlier values look quite influential for the results of ols1.\n Switch to a Bayesian framework In this project, we’ll use the brms package to fit our Bayesian regression models. You can learn a lot about brms here and here. Bayesian models, of course, require us to use priors. To keep things simple, we’ll use weakly-regularizing priors.\nlibrary(brms) Stick with Gauss. For our first two Bayesian models, b0 and b1, we’ll use the conventional Gaussian likelihood (i.e., family = gaussian in the brm() function). Like with ols0, above, the first model is based on the nice d data. The second, b1, is based on the more-difficult o data.\nb0 \u0026lt;- brm(data = d, family = gaussian, y ~ 1 + x, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), seed = 1) b1 \u0026lt;- update(b0, newdata = o) Here are the model summaries.\ntidy(b0) %\u0026gt;% slice(1:3) %\u0026gt;% mutate_if(is.double, round, digits = 2) ## term estimate std.error lower upper ## 1 b_Intercept -0.01 0.09 -0.15 0.14 ## 2 b_x 0.45 0.10 0.29 0.61 ## 3 sigma 0.86 0.06 0.77 0.98 tidy(b1) %\u0026gt;% slice(1:3) %\u0026gt;% mutate_if(is.double, round, digits = 2) ## term estimate std.error lower upper ## 1 b_Intercept 0.12 0.11 -0.06 0.30 ## 2 b_x 0.15 0.13 -0.05 0.36 ## 3 sigma 1.11 0.08 0.98 1.25 These should look familiar. They’re very much like the results from the OLS models. Hopefully this isn’t surprising. Our priors were quite weak, so there’s no reason to suspect the results would differ much.\nThe LOO and other goodies help with diagnostics. With the loo() function, we’ll extract loo objects, which contain some handy output.\nloo_b0 \u0026lt;- loo(b0) loo_b1 \u0026lt;- loo(b1) We’ll use str() to get a sense of what’s all in there, using loo_b1 as an example.\nstr(loo_b1) ## List of 10 ## $ estimates : num [1:3, 1:2] -155.39 6.33 310.78 15.27 3.64 ... ## ..- attr(*, \u0026quot;dimnames\u0026quot;)=List of 2 ## .. ..$ : chr [1:3] \u0026quot;elpd_loo\u0026quot; \u0026quot;p_loo\u0026quot; \u0026quot;looic\u0026quot; ## .. ..$ : chr [1:2] \u0026quot;Estimate\u0026quot; \u0026quot;SE\u0026quot; ## $ pointwise : num [1:100, 1:4] -14.02 -9 -1.04 -1.06 -1.24 ... ## ..- attr(*, \u0026quot;dimnames\u0026quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:4] \u0026quot;elpd_loo\u0026quot; \u0026quot;mcse_elpd_loo\u0026quot; \u0026quot;p_loo\u0026quot; \u0026quot;looic\u0026quot; ## $ diagnostics:List of 2 ## ..$ pareto_k: num [1:100] 0.638 0.38 0.176 0.16 0.178 ... ## ..$ n_eff : num [1:100] 73.9 439.7 3060.3 3190 3361.3 ... ## $ psis_object: NULL ## $ elpd_loo : num -155 ## $ p_loo : num 6.33 ## $ looic : num 311 ## $ se_elpd_loo: num 15.3 ## $ se_p_loo : num 3.64 ## $ se_looic : num 30.5 ## - attr(*, \u0026quot;dims\u0026quot;)= int [1:2] 4000 100 ## - attr(*, \u0026quot;class\u0026quot;)= chr [1:3] \u0026quot;psis_loo\u0026quot; \u0026quot;importance_sampling_loo\u0026quot; \u0026quot;loo\u0026quot; ## - attr(*, \u0026quot;yhash\u0026quot;)= chr \u0026quot;52ff8761c3c05e9988f323114940494f936ab1bb\u0026quot; ## - attr(*, \u0026quot;model_name\u0026quot;)= chr \u0026quot;b1\u0026quot; For a detailed explanation of all those elements, see the reference manual. For our purposes, we’ll focus on the pareto_k. Here’s a glimpse of what it contains for the b1 model.\nloo_b1$diagnostics$pareto_k %\u0026gt;% as_tibble() ## Warning: Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. Use `tibble::enframe(name = NULL)` instead. ## This warning is displayed once per session. ## # A tibble: 100 x 1 ## value ## \u0026lt;dbl\u0026gt; ## 1 0.638 ## 2 0.380 ## 3 0.176 ## 4 0.160 ## 5 0.178 ## 6 0.0760 ## 7 0.0349 ## 8 0.179 ## 9 0.202 ## 10 -0.0365 ## # … with 90 more rows We’ve got us a numeric vector of as many values as our data had observations–100 in this case. The pareto_k values can be used to examine overly-influential cases. See, for example this discussion on stackoverflow.com in which several members of the Stan team weighed in. The issue is also discussed in this paper, in the loo reference manual, and in this presentation by Aki Vehtari. If we explicitly open the loo package, we can use a few convenience functions to leverage pareto_k for diagnostic purposes. The pareto_k_table() function will categorize the pareto_k values and give us a sense of how many values are in problematic ranges.\nlibrary(loo) pareto_k_table(loo_b1) ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 99 99.0% 440 ## (0.5, 0.7] (ok) 1 1.0% 74 ## (0.7, 1] (bad) 0 0.0% \u0026lt;NA\u0026gt; ## (1, Inf) (very bad) 0 0.0% \u0026lt;NA\u0026gt; ## ## All Pareto k estimates are ok (k \u0026lt; 0.7). Happily, most of our cases were in the “good” range. One pesky case was in the “bad” range [can you guess which one?] and another case was only “ok” [and can you guess that one, too?]. The pareto_k_ids() function will tell exactly us which cases we’ll want to look at.\npareto_k_ids(loo_b1) ## [1] 1 Those numbers correspond to the row numbers in the data, o. These are exactly the cases that plagued our second OLS model, fit1, and are also the ones we hand coded to be outliers.\nWith the simple plot() function, we can get a diagnostic plot for the pareto_k values.\nplot(loo_b1) There they are, cases 1 and 2, lurking in the “bad” and “[just] ok” ranges. We can also make a similar plot with ggplot2. Though it takes a little more work, ggplot2 makes it easy to compare pareto_k plots across models with a little faceting.\nloo_b0$diagnostics$pareto_k %\u0026gt;% # The well-behaived data as_tibble() %\u0026gt;% mutate(i = 1:n()) %\u0026gt;% bind_rows( # The data with two outliers loo_b1$diagnostics$pareto_k %\u0026gt;% as_tibble() %\u0026gt;% mutate(i = 1:n()) ) %\u0026gt;% rename(pareto_k = value) %\u0026gt;% mutate(fit = rep(c(\u0026quot;fit b0\u0026quot;, \u0026quot;fit b1\u0026quot;), each = n()/2)) %\u0026gt;% ggplot(aes(x = i, y = pareto_k)) + geom_hline(yintercept = c(.5, .7, 1), color = \u0026quot;white\u0026quot;) + geom_point(alpha = .5) + geom_text(data = tibble(i = c(3, 6, 2), pareto_k = c(.45, .65, .95), label = c(\u0026quot;good\u0026quot;, \u0026quot;[just] ok\u0026quot;, \u0026quot;bad\u0026quot;), fit = \u0026quot;fit b0\u0026quot;), aes(label = label), color = \u0026quot;grey50\u0026quot;) + theme(panel.grid = element_blank(), axis.title.x = element_text(face = \u0026quot;italic\u0026quot;, family = \u0026quot;Times\u0026quot;)) + facet_wrap(~fit) So with b0–the model based on the well-behaved multivariate normal data, d–, all the pareto_k values hovered around zero in the “good” range. Things got concerning with model b1. But we know all that. Let’s move forward.\n What do we do with those overly-influential outlying values? A typical way to handle outlying values is to delete them based on some criterion, such as the Mahalanobis distance, Cook’s \\(D_i\\), or our new friend the pareto_k. In our next two models, we’ll do that. In our data arguments, we can use the slice() function to omit cases. In model b1.1, we simply omit the first and most influential case. In model b1.2, we omitted both unduly-influential cases, the values from rows 1 and 2.\nb1.1 \u0026lt;- update(b1, newdata = o %\u0026gt;% slice(2:100)) b1.2 \u0026lt;- update(b1, newdata = o %\u0026gt;% slice(3:100)) Here are the summaries for our models based on the slice[d] data.\ntidy(b1.1) %\u0026gt;% slice(1:3) %\u0026gt;% mutate_if(is.double, round, digits = 2) ## term estimate std.error lower upper ## 1 b_Intercept 0.07 0.10 -0.10 0.23 ## 2 b_x 0.28 0.11 0.09 0.47 ## 3 sigma 0.97 0.07 0.86 1.09 tidy(b1.2) %\u0026gt;% slice(1:3) %\u0026gt;% mutate_if(is.double, round, digits = 2) ## term estimate std.error lower upper ## 1 b_Intercept 0.02 0.09 -0.13 0.16 ## 2 b_x 0.40 0.10 0.23 0.56 ## 3 sigma 0.86 0.06 0.77 0.97 They are closer to the true data generating model (i.e., the code we used to make d), especially b1.2. However, there are other ways to handle the influential cases without dropping them. Finally, we’re ready to switch to Student’s \\(t\\)!\n  Time to leave Gauss for the more general Student’s \\(t\\) Recall that the normal distribution is equivalent to a Student’s \\(t\\) with the degrees of freedom parameter, \\(\\nu\\), set to infinity. That is, \\(\\nu\\) is fixed. Here we’ll relax that assumption and estimate \\(\\nu\\) from the data just like we estimate \\(\\mu\\) with the linear model and \\(\\sigma\\) as the residual spread. Since \\(\\nu\\)’s now a parameter, we’ll have to give it a prior. For our first Student’s \\(t\\) model, we’ll estimate \\(\\nu\\) with the brms default gamma(2, 0.1) prior.\nb2 \u0026lt;- brm(data = o, family = student, y ~ 1 + x, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(gamma(2, 0.1), class = nu), prior(cauchy(0, 1), class = sigma)), seed = 1) For the next model, we’ll switch out that weak gamma(2, 0.1) for a stronger gamma(4, 1). In some disciplines, the gamma distribution is something of an exotic bird. So before fitting the model, it might be useful to take a peek at what these gamma priors looks like. In the plot, below, the orange density in the background is the default gamma(2, 0.1) and the purple density in the foreground is the stronger gamma(4, 1).\n# data tibble(x = seq(from = 0, to = 60, by = .1)) %\u0026gt;% expand(x, nesting(alpha = c(2, 4), beta = c(0.1, 1))) %\u0026gt;% mutate(density = dgamma(x, alpha, beta), group = rep(letters[1:2], times = n() / 2)) %\u0026gt;% # plot ggplot(aes(x = x, ymin = 0, ymax = density, group = group, fill = group)) + geom_ribbon(size = 0, alpha = 3/4) + scale_fill_viridis_d(option = \u0026quot;B\u0026quot;, direction = -1, begin = 1/3, end = 2/3) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:50) + theme(panel.grid = element_blank(), legend.position = \u0026quot;none\u0026quot;) So the default prior is centered around values in the 2 to 30 range, but has a long gentle-sloping tail, allowing the model to yield much larger values for \\(\\nu\\), as needed. The prior we use below is almost entirely concentrated in the single-digit range. In this case, that will preference Student’s \\(t\\) likelihoods with very small \\(\\nu\\) parameters and correspondingly thick tails–easily allowing for extreme values.\nb3 \u0026lt;- update(b2, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(gamma(4, 1), class = nu), prior(cauchy(0, 1), class = sigma)), seed = 1) For our final model, we’ll fix the \\(\\nu\\) parameter in a bf() statement.\nb4 \u0026lt;- brm(data = o, family = student, bf(y ~ 1 + x, nu = 4), prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), seed = 1) Now we’ve got all those models, we can gather their results into a sole tibble.\nb_estimates \u0026lt;- tibble(model = c(\u0026quot;b0\u0026quot;, \u0026quot;b1\u0026quot;, \u0026quot;b1.1\u0026quot;, \u0026quot;b1.2\u0026quot;, \u0026quot;b2\u0026quot;, \u0026quot;b3\u0026quot;, \u0026quot;b4\u0026quot;)) %\u0026gt;% mutate(fit = map(model, get)) %\u0026gt;% mutate(tidy = map(fit, tidy)) %\u0026gt;% unnest(tidy) %\u0026gt;% filter(term %in% c(\u0026quot;b_Intercept\u0026quot;, \u0026quot;b_x\u0026quot;)) %\u0026gt;% arrange(term) To get a sense of what we’ve done, let’s take a peek at our models tibble.\nb_estimates %\u0026gt;% mutate_if(is.double, round, digits = 2) # This is just to round the numbers ## # A tibble: 14 x 7 ## model fit term estimate std.error lower upper ## \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 b0 \u0026lt;brmsfit\u0026gt; b_Intercept -0.01 0.09 -0.15 0.14 ## 2 b1 \u0026lt;brmsfit\u0026gt; b_Intercept 0.12 0.11 -0.06 0.3 ## 3 b1.1 \u0026lt;brmsfit\u0026gt; b_Intercept 0.07 0.1 -0.1 0.23 ## 4 b1.2 \u0026lt;brmsfit\u0026gt; b_Intercept 0.02 0.09 -0.13 0.16 ## 5 b2 \u0026lt;brmsfit\u0026gt; b_Intercept 0.05 0.09 -0.11 0.2 ## 6 b3 \u0026lt;brmsfit\u0026gt; b_Intercept 0.04 0.09 -0.12 0.18 ## 7 b4 \u0026lt;brmsfit\u0026gt; b_Intercept 0.04 0.09 -0.11 0.19 ## 8 b0 \u0026lt;brmsfit\u0026gt; b_x 0.45 0.1 0.290 0.61 ## 9 b1 \u0026lt;brmsfit\u0026gt; b_x 0.15 0.13 -0.05 0.36 ## 10 b1.1 \u0026lt;brmsfit\u0026gt; b_x 0.28 0.11 0.09 0.47 ## 11 b1.2 \u0026lt;brmsfit\u0026gt; b_x 0.4 0.1 0.23 0.56 ## 12 b2 \u0026lt;brmsfit\u0026gt; b_x 0.35 0.11 0.17 0.53 ## 13 b3 \u0026lt;brmsfit\u0026gt; b_x 0.36 0.1 0.19 0.53 ## 14 b4 \u0026lt;brmsfit\u0026gt; b_x 0.37 0.1 0.2 0.54 The models differ by their intercepts, slopes, sigmas, and \\(\\nu\\)s. For the sake of this post, we’ll focus on the slopes. Here we compare the different Bayesian models’ slopes by their posterior means and 95% intervals in a coefficient plot.\nb_estimates %\u0026gt;% filter(term == \u0026quot;b_x\u0026quot;) %\u0026gt;% # b_Intercept b_x ggplot(aes(x = model)) + geom_pointrange(aes(y = estimate, ymin = lower, ymax = upper), shape = 20) + coord_flip(ylim = c(-.2, 1)) + labs(title = \u0026quot;The x slope, varying by model\u0026quot;, subtitle = \u0026quot;The dots are the posterior means and the lines the percentile-based 95% intervals.\u0026quot;, x = NULL, y = NULL) + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) You might think of the b0 slope as the “true” slope. That’s the one estimated from the well-behaved multivariate normal data, d. That estimate’s just where we’d want it to be. The b1 slope is a disaster–way lower than the others. The slopes for b1.1 and b1.2 get better, but at the expense of deleting data. All three of our Student’s \\(t\\) models produced slopes that were pretty close to the b0 slope. They weren’t perfect, but, all in all, Student’s \\(t\\)-distribution did pretty okay.\n We need more LOO and more pareto_k. We already have loo objects for our first two models, b0 and b1. Let’s get some for models b2 through b4.\nloo_b2 \u0026lt;- loo(b2) loo_b3 \u0026lt;- loo(b3) loo_b4 \u0026lt;- loo(b4) With a little data wrangling, we can compare our models by how they look in our custom pareto_k diagnostic plots.\n# make a custom function to work with the loo objects in bulk get_pareto_k \u0026lt;- function(l) { l$diagnostics$pareto_k %\u0026gt;% as_tibble() %\u0026gt;% mutate(i = 1:n()) %\u0026gt;% rename(pareto_k = value) } # wrangle tibble(name = str_c(\u0026quot;loo_b\u0026quot;, 1:4)) %\u0026gt;% mutate(loo_object = map(name, get)) %\u0026gt;% mutate(pareto_k = map(loo_object, get_pareto_k)) %\u0026gt;% unnest(pareto_k) %\u0026gt;% mutate(fit = rep(c(\u0026quot;fit b1\u0026quot;, \u0026quot;fit b2\u0026quot;, \u0026quot;fit b3\u0026quot;, \u0026quot;fit b4\u0026quot;), each = n() / 4)) %\u0026gt;% # plot ggplot(aes(x = i, y = pareto_k)) + geom_hline(yintercept = c(.5, .7), color = \u0026quot;white\u0026quot;) + geom_point(alpha = .5) + scale_y_continuous(breaks = c(0, .5, .7)) + theme(panel.grid = element_blank(), axis.title.x = element_text(face = \u0026quot;italic\u0026quot;, family = \u0026quot;Times\u0026quot;)) + facet_wrap(~fit) Oh man, those Student’s \\(t\\) models worked sweet! In a succession from b2 through b4, each model looked better by pareto_k. All were way better than the typical Gaussian model, b1. While we’re at it, we might compare those by their LOO values.\ncompare_ic(loo_b1, loo_b2, loo_b3, loo_b4) ## Warning: \u0026#39;compare_ic\u0026#39; is deprecated and will be removed in the future. Please ## use \u0026#39;loo_compare\u0026#39; instead. ## LOOIC SE ## b1 310.78 30.54 ## b2 290.19 23.02 ## b3 287.61 20.93 ## b4 286.29 20.19 ## b1 - b2 20.60 11.16 ## b1 - b3 23.17 13.99 ## b1 - b4 24.50 14.92 ## b2 - b3 2.57 2.94 ## b2 - b4 3.90 3.91 ## b3 - b4 1.33 0.99 In terms of the LOO, b2 through b4 were about the same, but all looked better than b1. In fairness, though, the standard errors for the difference scores were a bit on the wide side. If you’re new to using information criteria to compare models, you might sit down and soak in this lecture on the topic and this vignette on the LOO in particular. For a more technical introduction, you might check out the references in the loo package’s reference manual.\nFor one final LOO-related comparison, we can use the brms::model_weights() function to see how much relative weight we might put on each of those four models if we were to use a model averaging approach. Here we use the default method, which is model averaging via posterior predictive stacking.\nmodel_weights(b1, b2, b3, b4) ## b1 b2 b3 b4 ## 2.146343e-06 6.741901e-06 1.111761e-05 9.999800e-01 If you’re not a fan of scientific notation, just tack on round(digits = 2). The stacking method suggests that we should place virtually all the weight on b4, the model in which we fixed our Student-\\(t\\) \\(\\nu\\) parameter at 4. To learn more about model stacking, check out Yao, Vehtari, Simpson, and Gelman’s (2018) paper, Using stacking to average Bayesian predictive distributions.\n Let’s compare a few Bayesian models. That’s enough with coefficients, pareto_k, and the LOO. Let’s get a sense of the implications of the models by comparing a few in plots. Here we use convenience functions from Matthew Kay’s tidybayes package to streamline the data wrangling and plotting. [The method came from a kind twitter suggesion from Kay.]\nlibrary(tidybayes) # These are the values of x we\u0026#39;d like model-implied summaries for nd \u0026lt;- tibble(x = seq(from = -4, to = 4, length.out = 50)) # here\u0026#39;s another way to arrange the models list(b0 = b0, b1 = b1, b3 = b3) %\u0026gt;% # with help from 1tidybayes::add_fitted_draws()`, here we use `fitted()` in bulk map_dfr(add_fitted_draws, newdata = nd, .id = \u0026quot;model\u0026quot;) %\u0026gt;% # plot ggplot(aes(x = x)) + stat_lineribbon(aes(y = .value), .width = .95, color = \u0026quot;grey92\u0026quot;, fill = \u0026quot;grey67\u0026quot;) + geom_point(data = d %\u0026gt;% bind_rows(o, o) %\u0026gt;% mutate(model = rep(c(\u0026quot;b0\u0026quot;, \u0026quot;b1\u0026quot;, \u0026quot;b3\u0026quot;), each = 100)), aes(y = y, color = y \u0026gt; 3), size = 1, alpha = 3/4) + scale_color_viridis_d(option = \u0026quot;A\u0026quot;, end = 4/7) + coord_cartesian(xlim = -3:3, ylim = -3:5) + ylab(NULL) + theme(panel.grid = element_blank(), legend.position = \u0026quot;none\u0026quot;) + facet_wrap(~model) For each subplot, the gray band is the 95% interval band and the overlapping light gray line is the posterior mean. Model b0, recall, is our baseline comparison model. This is of the well-behaved no-outlier data, d, using the good old Gaussian likelihood. Model b1 is of the outlier data, o, but still using the non-robust Gaussian likelihood. Model b3 uses a robust Student’s \\(t\\) likelihood with \\(\\nu\\) estimated with the fairly narrow gamma(4, 1) prior. For my money, b3 did a pretty good job.\nsessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.1.9000 loo_2.2.0 brms_2.12.0 ## [4] Rcpp_1.0.3 gridExtra_2.3 broom_0.5.3 ## [7] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 ## [10] purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 ## [13] tibble_2.1.3 ggplot2_3.2.1 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.2 ## [3] rsconnect_0.8.16 markdown_1.1 ## [5] base64enc_0.1-3 fs_1.3.1 ## [7] rstudioapi_0.10 farver_2.0.3 ## [9] rstan_2.19.2 svUnit_0.7-12 ## [11] DT_0.11 fansi_0.4.1 ## [13] mvtnorm_1.0-12 lubridate_1.7.4 ## [15] xml2_1.2.2 bridgesampling_0.8-1 ## [17] knitr_1.26 shinythemes_1.1.2 ## [19] bayesplot_1.7.1 jsonlite_1.6.1 ## [21] dbplyr_1.4.2 shiny_1.4.0 ## [23] compiler_3.6.2 httr_1.4.1 ## [25] backports_1.1.5 assertthat_0.2.1 ## [27] Matrix_1.2-18 fastmap_1.0.1 ## [29] lazyeval_0.2.2 cli_2.0.1 ## [31] later_1.0.0 prettyunits_1.1.1 ## [33] htmltools_0.4.0 tools_3.6.2 ## [35] igraph_1.2.4.2 coda_0.19-3 ## [37] gtable_0.3.0 glue_1.3.1 ## [39] reshape2_1.4.3 cellranger_1.1.0 ## [41] vctrs_0.2.2 nlme_3.1-142 ## [43] blogdown_0.17 crosstalk_1.0.0 ## [45] xfun_0.12 ps_1.3.0 ## [47] rvest_0.3.5 mime_0.8 ## [49] miniUI_0.1.1.1 lifecycle_0.1.0 ## [51] gtools_3.8.1 MASS_7.3-51.4 ## [53] zoo_1.8-7 scales_1.1.0 ## [55] colourpicker_1.0 hms_0.5.3 ## [57] promises_1.1.0 Brobdingnag_1.2-6 ## [59] parallel_3.6.2 inline_0.3.15 ## [61] shinystan_2.5.0 yaml_2.2.1 ## [63] StanHeaders_2.19.0 stringi_1.4.6 ## [65] highr_0.8 dygraphs_1.1.1.6 ## [67] checkmate_1.9.4 pkgbuild_1.0.6 ## [69] rlang_0.4.5 pkgconfig_2.0.3 ## [71] matrixStats_0.55.0 evaluate_0.14 ## [73] lattice_0.20-38 rstantools_2.0.0 ## [75] htmlwidgets_1.5.1 labeling_0.3 ## [77] tidyselect_1.0.0 processx_3.4.1 ## [79] plyr_1.8.5 magrittr_1.5 ## [81] bookdown_0.17 R6_2.4.1 ## [83] generics_0.0.2 DBI_1.1.0 ## [85] pillar_1.4.3 haven_2.2.0 ## [87] withr_2.1.2 xts_0.12-0 ## [89] abind_1.4-5 modelr_0.1.5 ## [91] crayon_1.3.4 arrayhelpers_1.0-20160527 ## [93] utf8_1.1.4 rmarkdown_2.0 ## [95] grid_3.6.2 readxl_1.3.1 ## [97] callr_3.4.1 threejs_0.3.3 ## [99] reprex_0.3.0 digest_0.6.23 ## [101] xtable_1.8-4 httpuv_1.5.2 ## [103] stats4_3.6.2 munsell_0.5.0 ## [105] viridisLite_0.3.0 shinyjs_1.1   ","date":1549065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549065600,"objectID":"356c8f89f51fbdda00a2925cf4ffac4a","permalink":"/post/robust-linear-regression-with-the-robust-student-s-t-distribution/","publishdate":"2019-02-02T00:00:00Z","relpermalink":"/post/robust-linear-regression-with-the-robust-student-s-t-distribution/","section":"post","summary":"[edited Feb 3, 2019]\nThe purpose of this post is to demonstrate the advantages of the Student’s \\(t\\)-distribution for regression with outliers, particularly within a Bayesian framework.\nI make assumptions I’m presuming you are familiar with linear regression, familiar with the basic differences between frequentist and Bayesian approaches to fitting regression models, and have a sense that the issue of outlier values is a pickle worth contending with. All code in is R, with a heavy use of the tidyverse–which you might learn a lot about here, especially chapter 5– and Paul Bürkner’s brms package.","tags":["R","Bayesian","brms","outlier","robust","tutorial"],"title":"Robust Linear Regression with Student’s $t$-Distribution","type":"post"},{"authors":null,"categories":[],"content":" [edited Dec 23, 2018]\ntl;dr You too can make sideways Gaussian density curves within the tidyverse. Here’s how.\n Here’s the deal: I like making pictures. Over the past several months, I’ve been slowly chipping away at John Kruschke’s Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Kruschke has a unique plotting style. One of the quirks is once in a while he likes to express the results of his analyses in plots where he shows the data alongside density curves of the model-implied data-generating distributions. Here’s an example from chapter 19 (p. 563).\nIn this example, he has lifespan data (i.e., Longevity) for fruit flies from five experimental conditions (i.e., CompanionNumber). Those are the black circles. In this section of the chapter, he used a Gaussian multilevel model in which the mean value for Longevity had a grand mean in addition to random effects for the five experimental conditions. Those sideways-turned blue Gaussians are his attempt to express the model-implied data generating distributions for each group.\nIf you haven’t gone through Kruschke’s text, you should know he relies on base R and all its loopy glory. If you carefully go through his code, you can reproduce his plots in that fashion. I’m a tidyverse man and prefer to avoid writing a for() loop at all costs. At first, I tried to work with convenience functions within ggplot2 and friends, but only had limited success. After staring long and hard at Kruschke’s base code, I came up with a robust solution, which I’d like to share here.\nIn this post, we’ll practice making sideways Gaussians in the Kruschke style. We’ll do so with a simple intercept-only single-level model and then expand our approach to an intercept-only multilevel model like the one in the picture, above.\n My assumptions For the sake of this post, I’m presuming you’re familiar with R, aware of the tidyverse, and have fit a Bayesian model or two. Yes. I admit that’s a narrow crowd. Sometimes the target’s a small one.\n We need data. First, we need data. Here we’ll borrow code from Matthew Kay’s nice tutorial on how to use his great tidybayes package.\nlibrary(tidyverse) set.seed(5) n \u0026lt;- 10 n_condition \u0026lt;- 5 abc \u0026lt;- tibble(condition = rep(letters[1:5], times = n), response = rnorm(n * 5, mean = c(0, 1, 2, 1, -1), sd = 0.5)) The data structure looks like so.\nstr(abc) ## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 50 obs. of 2 variables: ## $ condition: chr \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;d\u0026quot; ... ## $ response : num -0.42 1.692 1.372 1.035 -0.144 ... With Kay’s code, we have response values for five conditions. All follow the normal distribution and share a common standard deviation. However, they differ in their group means.\nabc %\u0026gt;% group_by(condition) %\u0026gt;% summarise(mean = mean(response) %\u0026gt;% round(digits = 2)) ## # A tibble: 5 x 2 ## condition mean ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 a 0.18 ## 2 b 1.01 ## 3 c 1.87 ## 4 d 1.03 ## 5 e -0.94 Altogether, the data look like this.\ntheme_set(theme_grey() + theme(panel.grid = element_blank())) abc %\u0026gt;% ggplot(aes(y = condition, x = response)) + geom_point(shape = 1) Let’s get ready to model.\n Just one intercept If you’ve read this far, you know we’re going Bayesian. Let’s open up our favorite Bayesian modeling package, Bürkner’s brms.\nlibrary(brms) For our first model, we’ll ignore the groups and just estimate a grand mean and a standard deviation. Relative to the scale of the abc data, our priors are modestly regularizing.\nfit1 \u0026lt;- brm(data = abc, response ~ 1, prior = c(prior(normal(0, 1), class = Intercept), prior(student_t(3, 0, 1), class = sigma))) Extract the posterior draws and save them as a data frame we’ll call post.\npost \u0026lt;- posterior_samples(fit1) glimpse(post) ## Observations: 4,000 ## Variables: 3 ## $ b_Intercept \u0026lt;dbl\u0026gt; 0.5987480, 0.5987480, 0.7314609, 0.6445826, 0.5576265, 0.7100258, 0.5000373, … ## $ sigma \u0026lt;dbl\u0026gt; 1.1177155, 1.1177155, 1.0092032, 1.0192299, 1.1700033, 1.0615338, 1.0849874, … ## $ lp__ \u0026lt;dbl\u0026gt; -76.96158, -76.96158, -77.46269, -77.09759, -77.26516, -77.10602, -77.20185, … If all you want is a quick and dirty way to plot a few of the model-implied Gaussians from the simple model, you can just nest stat_function() within mapply() and tack on the original data in a geom_jitter().\n# How many Gaussians would you like? n_iter \u0026lt;- 20 tibble(response = c(-4, 4)) %\u0026gt;% ggplot(aes(x = response)) + mapply(function(mean, sd) { stat_function(fun = dnorm, args = list(mean = mean, sd = sd), alpha = 1/2, color = \u0026quot;steelblue\u0026quot;) }, # Enter means and standard deviations here mean = post[1:n_iter, \u0026quot;b_Intercept\u0026quot;], sd = post[1:n_iter, \u0026quot;sigma\u0026quot;] ) + geom_jitter(data = abc, aes(y = -0.02), height = .025, shape = 1, alpha = 2/3) + scale_y_continuous(NULL, breaks = NULL) This works pretty okay. But notice the orientation is the usual horizontal. Kruschke’s Gaussians were on their sides. If we switch out our scale_y_continuous() line with scale_y_reverse() and add in coord_flip(), we’ll have it.\ntibble(response = c(-4, 4)) %\u0026gt;% ggplot(aes(x = response)) + mapply(function(mean, sd) { stat_function(fun = dnorm, args = list(mean = mean, sd = sd), alpha = 1/2, color = \u0026quot;steelblue\u0026quot;) }, mean = post[1:n_iter, \u0026quot;b_Intercept\u0026quot;], sd = post[1:n_iter, \u0026quot;sigma\u0026quot;] ) + geom_jitter(data = abc, aes(y = -0.02), height = .025, shape = 1, alpha = 2/3) + scale_y_reverse(NULL, breaks = NULL) + coord_flip()  Boom. It won’t always be this easy, though.\n Multiple intercepts Since the response values are from a combination of five condition groups, we can fit a multilevel model to compute both the grand mean and the group-level deviations from the grand mean.\nfit2 \u0026lt;- brm(data = abc, response ~ 1 + (1 | condition), prior = c(prior(normal(0, 1), class = Intercept), prior(student_t(3, 0, 1), class = sigma), prior(student_t(3, 0, 1), class = sd)), cores = 4) “Wait. Whoa. I’m so confused”—you say. “What’s a multilevel model, again?” Read this book, or this book; start here on this lecture series; or even check out my project, starting with chapter 12.\nOnce again, extract the posterior draws and save them as a data frame, post.\npost \u0026lt;- posterior_samples(fit2) str(post) ## \u0026#39;data.frame\u0026#39;: 4000 obs. of 9 variables: ## $ b_Intercept : num 0.0379 0.102 -0.048 -1.0037 0.0231 ... ## $ sd_condition__Intercept : num 2.08 2.13 1.94 1.99 2.21 ... ## $ sigma : num 0.473 0.468 0.475 0.648 0.527 ... ## $ r_condition[a,Intercept]: num -0.01062 0.00219 0.23764 1.40189 0.08825 ... ## $ r_condition[b,Intercept]: num 0.877 0.911 0.781 1.944 1.233 ... ## $ r_condition[c,Intercept]: num 1.46 1.62 2.03 3.15 1.74 ... ## $ r_condition[d,Intercept]: num 0.995 1.096 1.014 2.217 0.876 ... ## $ r_condition[e,Intercept]: num -1.111 -1.144 -1.005 0.234 -0.906 ... ## $ lp__ : num -51.8 -49.2 -50 -53.5 -48.7 ... This is where our task becomes difficult. Now each level of condition has its own mean estimate, which is a combination of the grand mean b_Intercept and the group-specific deviation, r_condition[a,Intercept] through r_condition[e,Intercept]. If all we wanted to do was show the model-implied Gaussians for, say, condition == a, that’d be a small extension of our last approach.\ntibble(response = c(-4, 4)) %\u0026gt;% ggplot(aes(x = response)) + mapply(function(mean, sd) { stat_function(fun = dnorm, args = list(mean = mean, sd = sd), alpha = 1/2, color = \u0026quot;steelblue\u0026quot;) }, # Here\u0026#39;s the small extension, part a mean = post[1:n_iter, \u0026quot;b_Intercept\u0026quot;] + post[1:n_iter, \u0026quot;r_condition[a,Intercept]\u0026quot;], sd = post[1:n_iter, \u0026quot;sigma\u0026quot;] ) + # The small extension, part b: geom_jitter(data = abc %\u0026gt;% filter(condition == \u0026quot;a\u0026quot;), aes(y = 0), height = .025, shape = 1, alpha = 2/3) + scale_y_reverse(NULL, breaks = NULL) + coord_flip() + labs(subtitle = \u0026quot;This is just for condition a\u0026quot;) The main thing we did was add to the definition of the mean within mapply(): mean = post[1:n_iter, \u0026quot;b_Intercept\u0026quot;] + post[1:n_iter, \u0026quot;r_condition[a,Intercept]\u0026quot;]. Within geom_jitter(), we also isolated the condition == \u0026quot;a\u0026quot; cases with filter(). Simple. However, it’s more of a pickle if we want multiple densities stacked atop/next to one another within the same plot.\nUnfortunately, we can’t extend our mapply(stat_function()) method to the group-level estimates–at least not that I’m aware. But there are other ways. We’ll need a little help from tidybayes::spread_draws(), about which you can learn more here.\nlibrary(tidybayes) sd \u0026lt;- fit2 %\u0026gt;% spread_draws(b_Intercept, sigma, r_condition[condition,]) head(sd) ## # A tibble: 6 x 7 ## # Groups: condition [5] ## .chain .iteration .draw b_Intercept sigma condition r_condition ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 1 0.0379 0.473 a -0.0106 ## 2 1 1 1 0.0379 0.473 b 0.877 ## 3 1 1 1 0.0379 0.473 c 1.46 ## 4 1 1 1 0.0379 0.473 d 0.995 ## 5 1 1 1 0.0379 0.473 e -1.11 ## 6 1 2 2 0.102 0.468 a 0.00219 In our sp tibble, we have much of the same information we’d get from brms::posterior_samples(), but in the long format with respect to the random effects for condition. Also notice that each row is indexed by the chain, iteration, and draw number. Among those, .draw is the column that corresponds to a unique row like what we’d get from brms::posterior_samples(). This is the index that ranges from 1 to the number of chains multiplied by the number of post-warmup iterations (i.e., default 4000 in our case).\nBut we need to wrangle a bit. Within the expand() function, we’ll select the columns we’d like to keep within the nesting() function and then expand the tibble by adding a sequence of response values ranging from -4 to 4, for each. This sets us up to use the dnorm() function in the next line to compute the density for each of those response values based on 20 unique normal distributions for each of the five condition groups. “Why 20?” Because we need some reasonably small number and 20’s the one Kruschke tended to use in his text and because, well, we set filter(.draw \u0026lt; 21). But choose whatever number you like.\nThe difficulty, however, is that all of these densities will have a minimum value of around 0 and all will be on the same basic scale. So we need a way to serially shift the density values up the y-axis in such a way that they’ll be sensibly separated by group. As far as I can figure, this’ll take us a couple steps. For the first step, we’ll create an intermediary variable, g, with which we’ll arbitrarily assign each of our five groups an integer index ranging from 0 to 4.\nThe second step is tricky. There we use our g integers to sequentially shift the density values up. Since our g value for a == 0, those we’ll keep 0 as their baseline. As our g value for b == 1, the baseline for those will now increase by 1. And so on for the other groups. But we still need to do a little more fiddling. What we want is for the maximum values of the density estimates to be a little lower than the baselines of the ones one grouping variable up. That is, we want the maximum values for the a densities to fall a little bit below 1 on the y-axis. It’s with the * .75 / max(density) part of the code that we accomplish that task. If you want to experiment with more or less room between the top and bottom of each density, play around with increasing/decreasing that .75 value.\nsd \u0026lt;- sd %\u0026gt;% filter(.draw \u0026lt; 21) %\u0026gt;% expand(nesting(.draw, b_Intercept, sigma, condition, r_condition), response = seq(from = -4, to = 4, length.out = 200)) %\u0026gt;% mutate(density = dnorm(response, mean = b_Intercept + r_condition, sd = sigma), g = recode(condition, a = 0, b = 1, c = 2, d = 3, e = 4)) %\u0026gt;% mutate(density = g + density * .75 / max(density)) glimpse(sd) ## Observations: 20,000 ## Variables: 8 ## Groups: condition [5] ## $ .draw \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ b_Intercept \u0026lt;dbl\u0026gt; 0.03787325, 0.03787325, 0.03787325, 0.03787325, 0.03787325, 0.03787325, 0.037… ## $ sigma \u0026lt;dbl\u0026gt; 0.4734374, 0.4734374, 0.4734374, 0.4734374, 0.4734374, 0.4734374, 0.4734374, … ## $ condition \u0026lt;chr\u0026gt; \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a… ## $ r_condition \u0026lt;dbl\u0026gt; -0.0106207, -0.0106207, -0.0106207, -0.0106207, -0.0106207, -0.0106207, -0.01… ## $ response \u0026lt;dbl\u0026gt; -4.000000, -3.959799, -3.919598, -3.879397, -3.839196, -3.798995, -3.758794, … ## $ density \u0026lt;dbl\u0026gt; 1.435677e-16, 2.945670e-16, 6.000399e-16, 1.213514e-15, 2.436566e-15, 4.85713… ## $ g \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… Since we’ll now be using the same axis for both the densities and the five condition groups, we’ll need to add a density column to our abc data.\nabc \u0026lt;- abc %\u0026gt;% mutate(density = recode(condition, a = 0, b = 1, c = 2, d = 3, e = 4)) Time to plot.\nsd %\u0026gt;% ggplot(aes(x = response, y = density)) + # here we make our density lines geom_line(aes(group = interaction(.draw, g)), alpha = 1/2, size = 1/3, color = \u0026quot;steelblue\u0026quot;) + # use the original data for the jittered points geom_jitter(data = abc, height = .05, shape = 1, alpha = 2/3) + scale_y_continuous(\u0026quot;condition\u0026quot;, breaks = 0:4, labels = letters[1:5]) Now we’re rolling. Let’s make a cosmetic adjustment. Recall that the full range of the normal distribution spans from \\(-\\infty\\) to \\(\\infty\\). At a certain point, it’s just not informative to show the left and right tails. If you look back up at our motivating example, you’ll note Kruschke’s densities stopped well before trailing off into the tails. If you look closely to the code from his text, you’ll see he’s just showing the inner 95-percentile range for each. To follow suit, we can compute those ranges with qnorm().\nsd \u0026lt;- sd %\u0026gt;% mutate(ll = qnorm(.025, mean = b_Intercept + r_condition, sd = sigma), ul = qnorm(.975, mean = b_Intercept + r_condition, sd = sigma)) Now we have our lower- and upper-level points for each iteration, we can limit the ranges of our Gaussians with filter().\nsd %\u0026gt;% filter(response \u0026gt; ll, response \u0026lt; ul) %\u0026gt;% ggplot(aes(x = response, y = density)) + geom_line(aes(group = interaction(.draw, g)), alpha = 1/2, size = 1/3, color = \u0026quot;steelblue\u0026quot;) + geom_jitter(data = abc, height = .05, shape = 1, alpha = 2/3) + scale_y_continuous(\u0026quot;condition\u0026quot;, breaks = 0:4, labels = letters[1:5]) Oh man, just look how sweet that is. Although I prefer our current method, another difference between it and Kruschke’s example is all of his densities are the same relative height. In all our plots so far, though, the densities differ by their heights. We’ll need a slight adjustment in our sd workflow for that. All we need to do is insert a group_by() statement between the two mutate() lines.\nsd \u0026lt;- sd %\u0026gt;% mutate(density = dnorm(response, mean = b_Intercept + r_condition, sd = sigma), g = recode(condition, a = 0, b = 1, c = 2, d = 3, e = 4)) %\u0026gt;% # here\u0026#39;s the new line group_by(.draw) %\u0026gt;% mutate(density = g + density * .75 / max(density)) # now plot sd %\u0026gt;% filter(response \u0026gt; ll, response \u0026lt; ul) %\u0026gt;% ggplot(aes(x = response, y = density)) + geom_line(aes(group = interaction(.draw, g)), alpha = 1/2, size = 1/3, color = \u0026quot;steelblue\u0026quot;) + geom_jitter(data = abc, height = .05, shape = 1, alpha = 2/3) + scale_y_continuous(\u0026quot;condition\u0026quot;, breaks = 0:4, labels = letters[1:5]) Nice. “But wait!”, you say. “We wanted our Gaussians to be on their sides.” We can do that in at least two ways. At this point, the quickest way is to use our scale_y_reverse() + coord_flip() combo from before.\nsd %\u0026gt;% filter(response \u0026gt; ll, response \u0026lt; ul) %\u0026gt;% ggplot(aes(x = response, y = density)) + geom_line(aes(group = interaction(.draw, g)), alpha = 1/2, size = 1/3, color = \u0026quot;steelblue\u0026quot;) + geom_jitter(data = abc, height = .05, shape = 1, alpha = 2/3) + scale_y_reverse(\u0026quot;condition\u0026quot;, breaks = 0:4, labels = letters[1:5]) + coord_flip() Another way to get those sideways Gaussians is to alter our sd data workflow. The main differene is this time we change the original mutate(density = g + density * .75 / max(density)) line to mutate(density = g - density * .75 / max(density)). In case you missed it, the only difference is we changed the + to a -.\nsd \u0026lt;- sd %\u0026gt;% # step one: starting fresh mutate(density = dnorm(response, mean = b_Intercept + r_condition, sd = sigma)) %\u0026gt;% group_by(.draw) %\u0026gt;% # step two: now SUBTRACTING density from g within the equation mutate(density = g - density * .75 / max(density)) Now in our global aes() statement in the plot, we put density on the x and response on the y. We need to take a few other subtle steps:\n Switch out geom_line() for geom_path() (see here). Drop the height argument within geom_jitter() for width. Switch out scale_y_continuous() for scale_x_continuous().  Though totally not necessary, we’ll add a little something extra by coloring the Gaussians by their means.\nsd %\u0026gt;% filter(response \u0026gt; ll, response \u0026lt; ul) %\u0026gt;% ggplot(aes(x = density, y = response)) + geom_path(aes(group = interaction(.draw, g), color = b_Intercept + r_condition), alpha = 1/2, size = 1/3, show.legend = F) + geom_jitter(data = abc, width = .05, shape = 1, alpha = 2/3) + scale_x_continuous(\u0026quot;condition\u0026quot;, breaks = 0:4, labels = letters[1:5]) + scale_color_viridis_c(option = \u0026quot;A\u0026quot;, end = .92) There you have it–Kruschke-style sideways Gaussians for your model plots.\n Afterward After releasing the initial version of this post, some of us had a lively twitter discussion on how to improve the code.\nAh, hrm. Took some digging, but it looks like negative density + setting `min_height = NA` (otherwise negative values are cut off) might work pic.twitter.com/gmF9kpo2T7\n\u0026mdash; Matthew Kay (@mjskay) December 22, 2018  Part of that discussion had to do with the possibility of using functions from Claus Wilke’s great ggridges package. After some great efforts, especially from Matthew Kay, we came up with solutions. In this section, we’ll cover them in some detail.\nFirst, here’s a more compact way to prepare the data for the plot.\nabc %\u0026gt;% distinct(condition) %\u0026gt;% add_fitted_draws(fit2, n = 20, dpar = c(\u0026quot;mu\u0026quot;, \u0026quot;sigma\u0026quot;)) %\u0026gt;% mutate(lower = qnorm(.025, mean = mu, sd = sigma), upper = qnorm(.975, mean = mu, sd = sigma)) %\u0026gt;% mutate(response = map2(lower, upper, seq, length.out = 200)) %\u0026gt;% mutate(density = pmap(list(response, mu, sigma), dnorm)) %\u0026gt;% unnest() %\u0026gt;% group_by(.draw) %\u0026gt;% mutate(density = density * .75 / max(density)) %\u0026gt;% glimpse() ## Warning: `cols` is now required. ## Please use `cols = c(response, density)` ## Observations: 20,000 ## Variables: 12 ## Groups: .draw [20] ## $ condition \u0026lt;chr\u0026gt; \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;… ## $ .row \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ .chain \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .iteration \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .draw \u0026lt;int\u0026gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24… ## $ .value \u0026lt;dbl\u0026gt; 0.2041854, 0.2041854, 0.2041854, 0.2041854, 0.2041854, 0.2041854, 0.2041854, 0… ## $ mu \u0026lt;dbl\u0026gt; 0.2041854, 0.2041854, 0.2041854, 0.2041854, 0.2041854, 0.2041854, 0.2041854, 0… ## $ sigma \u0026lt;dbl\u0026gt; 0.5729852, 0.5729852, 0.5729852, 0.5729852, 0.5729852, 0.5729852, 0.5729852, 0… ## $ lower \u0026lt;dbl\u0026gt; -0.918845, -0.918845, -0.918845, -0.918845, -0.918845, -0.918845, -0.918845, -… ## $ upper \u0026lt;dbl\u0026gt; 1.327216, 1.327216, 1.327216, 1.327216, 1.327216, 1.327216, 1.327216, 1.327216… ## $ response \u0026lt;dbl\u0026gt; -0.9188450, -0.9075582, -0.8962715, -0.8849847, -0.8736980, -0.8624113, -0.851… ## $ density \u0026lt;dbl\u0026gt; 0.1098804, 0.1141834, 0.1186089, 0.1231581, 0.1278322, 0.1326322, 0.1375591, 0… This could use some walking out. With the first two lines, we made a \\(5 \\times 1\\) tibble containing the five levels of condition, a through f. The add_fitted_draws() function comes from tidybayes. The first argument took our brms model fit, fit2. With the n argument, we indicated we just wanted 20 draws. With dpar, we requested distributional regression parameters in the output. In our case, those were the \\(\\mu\\) and \\(\\sigma\\) values for each level of condition. Here’s what that looks like.\nabc %\u0026gt;% distinct(condition) %\u0026gt;% add_fitted_draws(fit2, n = 20, dpar = c(\u0026quot;mu\u0026quot;, \u0026quot;sigma\u0026quot;)) %\u0026gt;% head() ## # A tibble: 6 x 8 ## # Groups: condition, .row [1] ## condition .row .chain .iteration .draw .value mu sigma ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 a 1 NA NA 55 0.163 0.163 0.458 ## 2 a 1 NA NA 126 0.509 0.509 0.513 ## 3 a 1 NA NA 404 0.472 0.472 0.717 ## 4 a 1 NA NA 813 0.387 0.387 0.537 ## 5 a 1 NA NA 1111 0.154 0.154 0.515 ## 6 a 1 NA NA 1218 0.204 0.204 0.495 Next, we established the lower- and upper-bounds bounds for the density lines, which were 95% intervals in this example. Within the second mutate() function, we used the purrr::map2() function to feed those two values into the first two arguments of the seq() function. Those arguments, recall, are from and to. We then hard coded 200 into the length.out argument. As a result, we turned our regular old tibble into a nested tibble. In each row of our new response column, we now have a \\(200 \\times 1\\) data frame containing the seq() output. If you’re new to nested data structures, I recommend checking out Hadley Wickham’s Managing many models with R.\nabc %\u0026gt;% distinct(condition) %\u0026gt;% add_fitted_draws(fit2, n = 20, dpar = c(\u0026quot;mu\u0026quot;, \u0026quot;sigma\u0026quot;)) %\u0026gt;% mutate(lower = qnorm(.025, mean = mu, sd = sigma), upper = qnorm(.975, mean = mu, sd = sigma)) %\u0026gt;% mutate(response = map2(lower, upper, seq, length.out = 200)) %\u0026gt;% head() ## # A tibble: 6 x 11 ## # Groups: condition, .row [1] ## condition .row .chain .iteration .draw .value mu sigma lower upper response ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt; ## 1 a 1 NA NA 346 0.278 0.278 0.521 -0.743 1.30 \u0026lt;dbl [200]\u0026gt; ## 2 a 1 NA NA 696 0.193 0.193 0.498 -0.783 1.17 \u0026lt;dbl [200]\u0026gt; ## 3 a 1 NA NA 734 0.320 0.320 0.709 -1.07 1.71 \u0026lt;dbl [200]\u0026gt; ## 4 a 1 NA NA 1214 0.0686 0.0686 0.559 -1.03 1.16 \u0026lt;dbl [200]\u0026gt; ## 5 a 1 NA NA 1313 0.229 0.229 0.539 -0.828 1.29 \u0026lt;dbl [200]\u0026gt; ## 6 a 1 NA NA 1348 0.329 0.329 0.589 -0.826 1.48 \u0026lt;dbl [200]\u0026gt; Much as the purrr::map2() function allowed us to iterate over two arguments, the purrr::pmap() function will allow us to iterate over an arbitrary number of arguments. In the case of our third mutate() function, we’ll iterate over the first three arguments of the dnorm() function. In case you forgot, those arguments are x, mean, and sd, respectively. Within our list(), we indicated we wanted to insert into them the response, mu, and sigma values. This returns the desired density values. Since our map2() and pmap() operations returned a nested tibble, we then followed them up with the unnest() function to make it easier to access the results.\nBefore unnesting, our nested tibble had 100 observations. After unnest(), we converted it to the long format, resulting in \\(100 \\times 200 = 20,000\\) observations.\nabc %\u0026gt;% distinct(condition) %\u0026gt;% add_fitted_draws(fit2, n = 20, dpar = c(\u0026quot;mu\u0026quot;, \u0026quot;sigma\u0026quot;)) %\u0026gt;% mutate(lower = qnorm(.025, mean = mu, sd = sigma), upper = qnorm(.975, mean = mu, sd = sigma)) %\u0026gt;% mutate(response = map2(lower, upper, seq, length.out = 200)) %\u0026gt;% mutate(density = pmap(list(response, mu, sigma), dnorm)) %\u0026gt;% unnest() %\u0026gt;% glimpse() ## Warning: `cols` is now required. ## Please use `cols = c(response, density)` ## Observations: 20,000 ## Variables: 12 ## Groups: condition, .row [5] ## $ condition \u0026lt;chr\u0026gt; \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;… ## $ .row \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ .chain \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .iteration \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .draw \u0026lt;int\u0026gt; 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79… ## $ .value \u0026lt;dbl\u0026gt; -0.05142892, -0.05142892, -0.05142892, -0.05142892, -0.05142892, -0.05142892, … ## $ mu \u0026lt;dbl\u0026gt; -0.05142892, -0.05142892, -0.05142892, -0.05142892, -0.05142892, -0.05142892, … ## $ sigma \u0026lt;dbl\u0026gt; 0.5490717, 0.5490717, 0.5490717, 0.5490717, 0.5490717, 0.5490717, 0.5490717, 0… ## $ lower \u0026lt;dbl\u0026gt; -1.12759, -1.12759, -1.12759, -1.12759, -1.12759, -1.12759, -1.12759, -1.12759… ## $ upper \u0026lt;dbl\u0026gt; 1.024732, 1.024732, 1.024732, 1.024732, 1.024732, 1.024732, 1.024732, 1.024732… ## $ response \u0026lt;dbl\u0026gt; -1.1275896, -1.1167739, -1.1059582, -1.0951425, -1.0843269, -1.0735112, -1.062… ## $ density \u0026lt;dbl\u0026gt; 0.1064434, 0.1106119, 0.1148989, 0.1193059, 0.1238338, 0.1284836, 0.1332564, 0… Hopefully, our last two lines look familiar. We group_by(.draw) just like in previous examples. However, our final mutate() line is a little simpler than in previous versions. Before we had to make that intermediary variable, g. Because we intend to plot these data with help from ggridges, we no longer have need for g. You’ll see. But the upshot is the only reason we’re adding this last mutate() line is to scale all the Gaussians to have the same maximum height the way Kruschke did.\nafd \u0026lt;- abc %\u0026gt;% distinct(condition) %\u0026gt;% add_fitted_draws(fit2, n = 20, dpar = c(\u0026quot;mu\u0026quot;, \u0026quot;sigma\u0026quot;)) %\u0026gt;% mutate(lower = qnorm(.025, mean = mu, sd = sigma), upper = qnorm(.975, mean = mu, sd = sigma)) %\u0026gt;% mutate(response = map2(lower, upper, seq, length.out = 200)) %\u0026gt;% mutate(density = pmap(list(response, mu, sigma), dnorm)) %\u0026gt;% unnest() %\u0026gt;% group_by(.draw) %\u0026gt;% mutate(density = density * .75 / max(density)) ## Warning: `cols` is now required. ## Please use `cols = c(response, density)` glimpse(afd) ## Observations: 20,000 ## Variables: 12 ## Groups: .draw [20] ## $ condition \u0026lt;chr\u0026gt; \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;a\u0026quot;… ## $ .row \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ .chain \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .iteration \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ .draw \u0026lt;int\u0026gt; 292, 292, 292, 292, 292, 292, 292, 292, 292, 292, 292, 292, 292, 292, 292, 292… ## $ .value \u0026lt;dbl\u0026gt; -0.302783, -0.302783, -0.302783, -0.302783, -0.302783, -0.302783, -0.302783, -… ## $ mu \u0026lt;dbl\u0026gt; -0.302783, -0.302783, -0.302783, -0.302783, -0.302783, -0.302783, -0.302783, -… ## $ sigma \u0026lt;dbl\u0026gt; 0.5187913, 0.5187913, 0.5187913, 0.5187913, 0.5187913, 0.5187913, 0.5187913, 0… ## $ lower \u0026lt;dbl\u0026gt; -1.319595, -1.319595, -1.319595, -1.319595, -1.319595, -1.319595, -1.319595, -… ## $ upper \u0026lt;dbl\u0026gt; 0.7140294, 0.7140294, 0.7140294, 0.7140294, 0.7140294, 0.7140294, 0.7140294, 0… ## $ response \u0026lt;dbl\u0026gt; -1.3195953, -1.3093761, -1.2991569, -1.2889377, -1.2787185, -1.2684993, -1.258… ## $ density \u0026lt;dbl\u0026gt; 0.1098804, 0.1141834, 0.1186089, 0.1231581, 0.1278322, 0.1326322, 0.1375591, 0… Let’s open ggridges\nlibrary(ggridges) Note how contrary to before, we set the global y axis to our condition grouping variable. It’s within the geom_ridgeline() function that we now specify height = density. Other than that, the main thing to point out is you might want to adjust the ylim parameters. Otherwise the margins aren’t the best.\nafd %\u0026gt;% ggplot(aes(x = response, y = condition)) + geom_ridgeline(aes(height = density, group = interaction(condition, .draw)), fill = NA, size = 1/3, color = adjustcolor(\u0026quot;steelblue\u0026quot;, alpha.f = 1/2)) + geom_jitter(data = abc, height = .05, shape = 1, alpha = 2/3) + coord_cartesian(ylim = c(1.25, 5.5)) “But I wanted my Gaussians tipped to the left!”, you say. Yep, we can do that, too. Three things: First, we’ll want to adjust the height parameter to -density. We want our Gaussians to extend under their baselines. Along with that, we need to include min_height = NA. Finally, we’ll switch out coord_cartesian() for good old coord_flip(). And you can adjust your ylim parameters as desired.\nafd %\u0026gt;% ggplot(aes(x = response, y = condition)) + geom_ridgeline(aes(height = -density, group = interaction(condition, .draw)), fill = NA, size = 1/3, color = adjustcolor(\u0026quot;steelblue\u0026quot;, alpha.f = 1/2), min_height = NA) + geom_jitter(data = abc, height = .05, shape = 1, alpha = 2/3) + coord_flip(ylim = c(0.5, 4.75)) I think it’s important to note that I’ve never met any of the people who helped me with this project. Academic twitter, man–it’s a good place to be.\n Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## Random number generation: ## RNG: Mersenne-Twister ## Normal: Inversion ## Sample: Rounding ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggridges_0.5.2 tidybayes_2.0.1.9000 brms_2.12.0 Rcpp_1.0.3 ## [5] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 purrr_0.3.3 ## [9] readr_1.3.1 tidyr_1.0.2 tibble_2.1.3 ggplot2_3.2.1 ## [13] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 rsconnect_0.8.16 ## [4] markdown_1.1 base64enc_0.1-3 fs_1.3.1 ## [7] rstudioapi_0.10 farver_2.0.3 rstan_2.19.2 ## [10] svUnit_0.7-12 DT_0.11 fansi_0.4.1 ## [13] mvtnorm_1.0-12 lubridate_1.7.4 xml2_1.2.2 ## [16] bridgesampling_0.8-1 knitr_1.26 shinythemes_1.1.2 ## [19] bayesplot_1.7.1 jsonlite_1.6.1 broom_0.5.3 ## [22] dbplyr_1.4.2 shiny_1.4.0 compiler_3.6.2 ## [25] httr_1.4.1 backports_1.1.5 assertthat_0.2.1 ## [28] Matrix_1.2-18 fastmap_1.0.1 lazyeval_0.2.2 ## [31] cli_2.0.1 later_1.0.0 htmltools_0.4.0 ## [34] prettyunits_1.1.1 tools_3.6.2 igraph_1.2.4.2 ## [37] coda_0.19-3 gtable_0.3.0 glue_1.3.1 ## [40] reshape2_1.4.3 cellranger_1.1.0 vctrs_0.2.2 ## [43] nlme_3.1-142 blogdown_0.17 crosstalk_1.0.0 ## [46] xfun_0.12 ps_1.3.0 rvest_0.3.5 ## [49] mime_0.8 miniUI_0.1.1.1 lifecycle_0.1.0 ## [52] gtools_3.8.1 zoo_1.8-7 scales_1.1.0 ## [55] colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [58] Brobdingnag_1.2-6 parallel_3.6.2 inline_0.3.15 ## [61] shinystan_2.5.0 yaml_2.2.1 gridExtra_2.3 ## [64] StanHeaders_2.19.0 loo_2.2.0 stringi_1.4.6 ## [67] dygraphs_1.1.1.6 pkgbuild_1.0.6 rlang_0.4.5 ## [70] pkgconfig_2.0.3 matrixStats_0.55.0 evaluate_0.14 ## [73] lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 ## [76] labeling_0.3 tidyselect_1.0.0 processx_3.4.1 ## [79] plyr_1.8.5 magrittr_1.5 bookdown_0.17 ## [82] R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [85] pillar_1.4.3 haven_2.2.0 withr_2.1.2 ## [88] xts_0.12-0 abind_1.4-5 modelr_0.1.5 ## [91] crayon_1.3.4 arrayhelpers_1.0-20160527 utf8_1.1.4 ## [94] rmarkdown_2.0 grid_3.6.2 readxl_1.3.1 ## [97] callr_3.4.1 threejs_0.3.3 reprex_0.3.0 ## [100] digest_0.6.23 xtable_1.8-4 httpuv_1.5.2 ## [103] stats4_3.6.2 munsell_0.5.0 viridisLite_0.3.0 ## [106] shinyjs_1.1  ","date":1545264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545264000,"objectID":"42ba31c0d9f0e8429d86371da5384421","permalink":"/post/make-rotated-gaussians-kruschke-style/","publishdate":"2018-12-20T00:00:00Z","relpermalink":"/post/make-rotated-gaussians-kruschke-style/","section":"post","summary":"[edited Dec 23, 2018]\ntl;dr You too can make sideways Gaussian density curves within the tidyverse. Here’s how.\n Here’s the deal: I like making pictures. Over the past several months, I’ve been slowly chipping away at John Kruschke’s Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Kruschke has a unique plotting style. One of the quirks is once in a while he likes to express the results of his analyses in plots where he shows the data alongside density curves of the model-implied data-generating distributions.","tags":["Bayesian","brms","Kruschke","plot","R","tutorial","tidyverse"],"title":"Make rotated Gaussians, Kruschke style","type":"post"},{"authors":null,"categories":[],"content":" [edited Feb 27, 2019]\nPreamble I released the first bookdown version of my Statistical Rethinking with brms, ggplot2, and the tidyverse project a couple weeks ago. I consider it the 0.9.0 version. I wanted a little time to step back from the project before giving it a final edit for the first major edition. I also wanted to give others a little time to take a look and suggest edits, which some thankfully have.\nNow some time has passed, it’s become clear I’d like to add a bonus section on Bayesian meta-analysis. IMO, this is a natural extension of the hierarchical models McElreath introduced in chapter’s 12 and 13 of his text and of the measurement-error models he introduced in chapter 14. So the purpose of this post is to present a rough draft of how I’d like to introduce fitting meta-analyses with Bürkner’s great brms package.\nI intend to tack this section onto the end of chapter 14. If you have any constrictive criticisms, please pass them along.\nHere’s the rough draft (which I updated on 2018-11-12):\n Rough draft: Meta-analysis If your mind isn’t fully blown by those measurement-error and missing-data models, let’s keep building. As it turns out, meta-analyses are often just special kinds of multilevel measurement-error models. Thus, you can use brms::brm() to fit Bayesian meta-analyses, too.\nBefore we proceed, I should acknowledge that this section is heavily influenced by Matti Vourre’s great blog post, Meta-analysis is a special case of Bayesian multilevel modeling. And since McElreath’s text doesn’t directly address meta-analyses, we’ll take further inspiration from Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin’s Bayesian data analysis, Third edition. We’ll let Gelman and colleagues introduce the topic:\n Discussions of meta-analysis are sometimes imprecise about the estimands of interest in the analysis, especially when the primary focus is on testing the null hypothesis of no effect in any of the studies to be combined. Our focus is on estimating meaningful parameters, and for this objective there appear to be three possibilities, accepting the overarching assumption that the studies are comparable in some broad sense. The first possibility is that we view the studies as identical replications of each other, in the sense we regard the individuals in all the studies as independent samples from a common population, with the same outcome measures and so on. A second possibility is that the studies are so different that the results of any one study provide no information about the results of any of the others. A third, more general, possibility is that we regard the studies as exchangeable but not necessarily either identical or completely unrelated; in other words we allow differences from study to study, but such that the differences are not expected a priori to have predictable effects favoring one study over another.… This third possibility represents a continuum between the two extremes, and it is this exchangeable model (with unknown hyperparameters characterizing the population distribution) that forms the basis of our Bayesian analysis…\nThe first potential estimand of a meta-analysis, or a hierarchically structured problem in general, is the mean of the distribution of effect sizes, since this represents the overall ‘average’ effect across all studies that could be regarded as exchangeable with the observed studies. Other possible estimands are the effect size in any of the observed studies and the effect size in another, comparable (exchangeable) unobserved study. (pp. 125—126, emphasis in the original)\n The basic version of a Bayesian meta-analysis follows the form\n\\[y_i \\sim \\text{Normal}(\\theta_i, \\sigma_i)\\]\nwhere \\(y_i\\) = the point estimate for the effect size of a single study, \\(i\\), which is presumed to have been a draw from a Normal distribution centered on \\(\\theta_i\\). The data in meta-analyses are typically statistical summaries from individual studies. The one clear lesson from this chapter is that those estimates themselves come with error and those errors should be fully expressed in the meta-analytic model. Which we do. The standard error from study \\(i\\) is specified \\(\\sigma_i\\), which is also a stand-in for the standard deviation of the Normal distribution from which the point estimate was drawn. Do note, we’re not estimating \\(\\sigma_i\\), here. Those values we take directly from the original studies.\nBuilding on the model, we further presume that study \\(i\\) is itself just one draw from a population of related studies, each of which have their own effect sizes. As such. we presume \\(\\theta_i\\) itself has a distribution following the form\n\\[\\theta_i \\sim \\text{Normal} (\\mu, \\tau)\\]\nwhere \\(\\mu\\) is the meta-analytic effect (i.e., the population mean) and \\(\\tau\\) is the variation around that mean, what you might also think of as \\(\\sigma_\\tau\\).\nSince there’s no example of a meta-analysis in the text, we’ll have to look elsewhere. We’ll focus on Gershoff and Grogan-Kaylor’s (2016) paper, Spanking and Child Outcomes: Old Controversies and New Meta-Analyses. From their introduction, we read:\n Around the world, most children (80%) are spanked or otherwise physically punished by their parents (UNICEF, 2014). The question of whether parents should spank their children to correct misbehaviors sits at a nexus of arguments from ethical, religious, and human rights perspectives both in the U.S. and around the world (Gershoff, 2013). Several hundred studies have been conducted on the associations between parents’ use of spanking or physical punishment and children’s behavioral, emotional, cognitive, and physical outcomes, making spanking one of the most studied aspects of parenting. What has been learned from these hundreds of studies? (p. 453)\n Our goal will be to learn Bayesian meta-analysis by answering part of that question. I’ve transcribed the values directly from Gershoff and Grogan-Kaylor’s paper and saved them as a file called spank.xlsx. You can find the data in this project’s GitHub repository. Let’s load them and glimpse().\nspank \u0026lt;- readxl::read_excel(\u0026quot;spank.xlsx\u0026quot;) library(tidyverse) glimpse(spank) ## Observations: 111 ## Variables: 8 ## $ study \u0026lt;chr\u0026gt; \u0026quot;Bean and Roberts (1981)\u0026quot;, \u0026quot;Day and Roberts (1983)\u0026quot;, \u0026quot;Minton,… ## $ year \u0026lt;dbl\u0026gt; 1981, 1983, 1971, 1988, 1990, 1961, 1962, 1990, 2002, 2005, 1… ## $ outcome \u0026lt;chr\u0026gt; \u0026quot;Immediate defiance\u0026quot;, \u0026quot;Immediate defiance\u0026quot;, \u0026quot;Immediate defian… ## $ between \u0026lt;dbl\u0026gt; 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0… ## $ within \u0026lt;dbl\u0026gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1… ## $ d \u0026lt;dbl\u0026gt; -0.74, 0.36, 0.34, -0.08, 0.10, 0.63, 0.19, 0.47, 0.14, -0.18… ## $ ll \u0026lt;dbl\u0026gt; -1.76, -1.04, -0.09, -1.01, -0.82, 0.16, -0.14, 0.20, -0.42, … ## $ ul \u0026lt;dbl\u0026gt; 0.28, 1.77, 0.76, 0.84, 1.03, 1.10, 0.53, 0.74, 0.70, 0.13, 2… In this paper, the effect size of interest is a Cohen’s d, derived from the formula\n\\[d = \\frac{\\mu_\\text{treatment} - \\mu_\\text{comparison}}{\\sigma_{pooled}}\\]\nwhere\n\\[\\sigma_{pooled} = \\sqrt{\\frac{((n_1 - 1) \\sigma_1^2) + ((n_2 - 1) \\sigma_2^2)}{n_1 + n_2 -2}}\\]\nTo help make the equation for \\(d\\) clearer for our example, we might re-express it as\n\\[d = \\frac{\\mu_\\text{spanked} - \\mu_\\text{not spanked}}{\\sigma_{pooled}}\\]\nMcElreath didn’t really focus on effect sizes in his text. If you need a refresher, you might check out Kelley and Preacher’s On effect size. But in words, Cohen’s d is a standardized mean difference between two groups.\nSo if you look back up at the results of glimpse(spank), you’ll notice the column d, which is indeed a vector of Cohen’s d effect sizes. The last two columns, ll and ul are the lower and upper limits of the associated 95% frequentist confidence intervals. But we don’t want confidence intervals for our d-values; we want their standard errors. Fortunately, we can compute those with the following formula\n\\[SE = \\frac{\\text{upper limit } – \\text{lower limit}}{3.92}\\]\nHere it is in code.\nspank \u0026lt;- spank %\u0026gt;% mutate(se = (ul - ll) / 3.92) glimpse(spank) ## Observations: 111 ## Variables: 9 ## $ study \u0026lt;chr\u0026gt; \u0026quot;Bean and Roberts (1981)\u0026quot;, \u0026quot;Day and Roberts (1983)\u0026quot;, \u0026quot;Minton,… ## $ year \u0026lt;dbl\u0026gt; 1981, 1983, 1971, 1988, 1990, 1961, 1962, 1990, 2002, 2005, 1… ## $ outcome \u0026lt;chr\u0026gt; \u0026quot;Immediate defiance\u0026quot;, \u0026quot;Immediate defiance\u0026quot;, \u0026quot;Immediate defian… ## $ between \u0026lt;dbl\u0026gt; 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0… ## $ within \u0026lt;dbl\u0026gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1… ## $ d \u0026lt;dbl\u0026gt; -0.74, 0.36, 0.34, -0.08, 0.10, 0.63, 0.19, 0.47, 0.14, -0.18… ## $ ll \u0026lt;dbl\u0026gt; -1.76, -1.04, -0.09, -1.01, -0.82, 0.16, -0.14, 0.20, -0.42, … ## $ ul \u0026lt;dbl\u0026gt; 0.28, 1.77, 0.76, 0.84, 1.03, 1.10, 0.53, 0.74, 0.70, 0.13, 2… ## $ se \u0026lt;dbl\u0026gt; 0.52040816, 0.71683673, 0.21683673, 0.47193878, 0.47193878, 0… Now our data are ready, we can express our first Bayesian meta-analysis with the formula\n\\[ \\begin{eqnarray} \\text{d}_i \u0026amp; \\sim \u0026amp; \\text{Normal}(\\theta_i, \\sigma_i = \\text{se}_i) \\\\ \\theta_i \u0026amp; \\sim \u0026amp; \\text{Normal} (\\mu, \\tau) \\\\ \\mu \u0026amp; \\sim \u0026amp; \\text{Normal} (0, 1) \\\\ \\tau \u0026amp; \\sim \u0026amp; \\text{HalfCauchy} (0, 1) \\end{eqnarray} \\]\nThe last two lines, of course, spell out our priors. In psychology, it’s pretty rare to see Cohen’s d-values greater than the absolute value of \\(\\pm 1\\). So in the absence of more specific domain knowledge–which I don’t have–, it seems like \\(\\text{Normal} (0, 1)\\) is a reasonable place to start. And just like McElreath used \\(\\text{HalfCauchy} (0, 1)\\) as the default prior for the group-level standard deviations, it makes sense to use it here for our meta-analytic \\(\\tau\\) parameter.\nLet’s load brms.\nlibrary(brms) Here’s the code for the first model.\nb14.5 \u0026lt;- brm(data = spank, family = gaussian, d | se(se) ~ 1 + (1 | study), prior = c(prior(normal(0, 1), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 14) One thing you might notice is our se(se) function excluded the sigma argument. If you recall from section 14.1, we specified sigma = T in our measurement-error models. The brms default is that within se(), sigma = FALSE. As such, we have no estimate for sigma the way we would if we were doing this analysis with the raw data from the studies. Hopefully this makes sense. The uncertainty around the d-value for each study \\(i\\) has already been encoded in the data as se.\nThis brings us to another point. We typically perform meta-analyses on data summaries. In my field and perhaps in yours, this is due to the historical accident that it has not been the norm among researchers to make their data publically available. So effect size summaries were the best we typically had. However, times are changing (e.g., here, here). If the raw data from all the studies for your meta-analysis are available, you can just fit a multilevel model in which the data are nested in the studies. Heck, you could even allow the studies to vary by \\(\\sigma\\) by taking the distributional modeling approach and specify something like sigma ~ 0 + study or even sigma ~ 1 + (1 | study).\nBut enough technical talk. Let’s look at the model results.\nprint(b14.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: d | se(se) ~ 1 + (1 | study) ## Data: spank (Number of observations: 111) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~study (Number of levels: 76) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.26 0.03 0.21 0.33 1.00 726 972 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.37 0.04 0.30 0.44 1.01 419 843 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Thus, in our simple Bayesian meta-analysis, we have a population Cohen’s d of about 0.37. Our estimate for \\(\\tau\\), 0.26, suggests we have quite a bit of between-study variability. One question you might ask is: What exactly are these Cohen’s ds measuring, anyways? We’ve encoded that in the outcome vector of the spank data.\nspank %\u0026gt;% distinct(outcome) %\u0026gt;% knitr::kable()   outcome    Immediate defiance  Low moral internalization  Child aggression  Child antisocial behavior  Child externalizing behavior problems  Child internalizing behavior problems  Child mental health problems  Child alcohol or substance abuse  Negative parent–child relationship  Impaired cognitive ability  Low self-esteem  Low self-regulation  Victim of physical abuse  Adult antisocial behavior  Adult mental health problems  Adult alcohol or substance abuse  Adult support for physical punishment    There are a few things to note. First, with the possible exception of Adult support for physical punishment, all of the outcomes are negative. We prefer conditions associated with lower values for things like Child aggression and Adult mental health problems. Second, the way the data are coded, larger effect sizes are interpreted as more negative outcomes associated with children having been spanked. That is, our analysis suggests spanking children is associated with worse outcomes. What might not be immediately apparent is that even though there are 111 cases in the data, there are only 76 distinct studies.\nspank %\u0026gt;% distinct(study) %\u0026gt;% count() ## # A tibble: 1 x 1 ## n ## \u0026lt;int\u0026gt; ## 1 76 In other words, some studies have multiple outcomes. In order to better accommodate the study- and outcome-level variances, let’s fit a cross-classified Bayesian meta-analysis reminiscent of the cross-classified chimp model from Chapter 13.\nb14.6 \u0026lt;- brm(data = spank, family = gaussian, d | se(se) ~ 1 + (1 | study) + (1 | outcome), prior = c(prior(normal(0, 1), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 14) print(b14.6) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: d | se(se) ~ 1 + (1 | study) + (1 | outcome) ## Data: spank (Number of observations: 111) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~outcome (Number of levels: 17) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.08 0.03 0.04 0.14 1.00 938 1642 ## ## ~study (Number of levels: 76) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.25 0.03 0.20 0.32 1.01 818 1458 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.35 0.04 0.27 0.44 1.01 672 1128 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we have two \\(\\tau\\) parameters. We might plot them to get a sense of where the variance is at.\nposterior_samples(b14.6) %\u0026gt;% select(starts_with(\u0026quot;sd\u0026quot;)) %\u0026gt;% gather(key, tau) %\u0026gt;% mutate(key = str_remove(key, \u0026quot;sd_\u0026quot;) %\u0026gt;% str_remove(., \u0026quot;__Intercept\u0026quot;)) %\u0026gt;% ggplot(aes(x = tau, fill = key)) + geom_density(color = \u0026quot;transparent\u0026quot;, alpha = 2/3) + scale_fill_viridis_d(NULL, end = .85) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(tau)) + theme(panel.grid = element_blank()) So at this point, the big story is there’s more variability between the studies than there is the outcomes. But I still want to get a sense of the individual outcomes. Here we’ll use tidybayes::geom_halfeyeh() to help us make our version of a forest plot.\n# load tidybayes library(tidybayes) b14.6 %\u0026gt;% spread_draws(b_Intercept, r_outcome[outcome,]) %\u0026gt;% # add the grand mean to the group-specific deviations mutate(mu = b_Intercept + r_outcome) %\u0026gt;% ungroup() %\u0026gt;% mutate(outcome = str_replace_all(outcome, \u0026quot;[.]\u0026quot;, \u0026quot; \u0026quot;)) %\u0026gt;% # plot ggplot(aes(x = mu, y = reorder(outcome, mu))) + geom_vline(xintercept = fixef(b14.6)[1, 1], color = \u0026quot;white\u0026quot;, size = 1) + geom_vline(xintercept = fixef(b14.6)[1, 3:4], color = \u0026quot;white\u0026quot;, linetype = 2) + geom_halfeyeh(.width = .95, size = 2/3) + labs(x = expression(italic(\u0026quot;Cohen\u0026#39;s d\u0026quot;)), y = NULL) + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) The solid and dashed vertical white lines in the background mark off the grand mean (i.e., the meta-analytic effect) and its 95% intervals. But anyway, there’s not a lot of variability across the outcomes. Let’s go one step further with the model. Doubling back to Gelman and colleagues, we read:\n When assuming exchangeability we assume there are no important covariates that might form the basis of a more complex model, and this assumption (perhaps misguidedly) is widely adopted in meta-analysis. What if other information (in addition to the data \\((n, y)\\)) is available to distinguish among the \\(J\\) studies in a meta-analysis, so that an exchangeable model is inappropriate? In this situation, we can expand the framework of the model to be exchangeable in the observed data and covariates, for example using a hierarchical regression model. (p. 126)\n One important covariate Gershoff and Grogan-Kaylor addressed in their meta-analysis was the type of study. The 76 papers they based their meta-analysis on contained both between- and within-participants designs. In the spank data, we’ve dummy coded that information with the between and within vectors. Both are dummy variables and within = 1 - between. Here are the counts.\nspank %\u0026gt;% count(between) ## # A tibble: 2 x 2 ## between n ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 0 71 ## 2 1 40 When I use dummies in my models, I prefer to have the majority group stand as the reference category. As such, I typically name those variables by the minority group. In this case, most occasions are based on within-participant designs. Thus, we’ll go ahead and add the between variable to the model. While we’re at it, we’ll practice using the 0 + intercept syntax.\nb14.7 \u0026lt;- brm(data = spank, family = gaussian, d | se(se) ~ 0 + intercept + between + (1 | study) + (1 | outcome), prior = c(prior(normal(0, 1), class = b), prior(cauchy(0, 1), class = sd)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 14) print(b14.7) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: d | se(se) ~ 0 + intercept + between + (1 | study) + (1 | outcome) ## Data: spank (Number of observations: 111) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~outcome (Number of levels: 17) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.08 0.02 0.04 0.14 1.00 1589 2516 ## ## ~study (Number of levels: 76) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.25 0.03 0.20 0.32 1.00 1200 1875 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## intercept 0.38 0.05 0.28 0.48 1.00 1050 1621 ## between -0.08 0.07 -0.22 0.07 1.00 986 1449 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s take a closer look at b_between.\nposterior_samples(b14.7) %\u0026gt;% ggplot(aes(x = b_between, y = 0)) + geom_halfeyeh(point_interval = median_qi, .width = c(.5, .95)) + labs(x = \u0026quot;Overall difference for between- vs within-participant designs\u0026quot;, y = NULL) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) That difference isn’t as large I’d expect it to be. But then again, I’m no spanking researcher. So what do I know?\nThere are other things you might do with these data. For example, you might check for trends by year or, as the authors did in their manuscript, distinguish among different severities of corporal punishment. But I think we’ve gone far enough to get you started.\nIf you’d like to learn more about these methods, do check out Vourre’s Meta-analysis is a special case of Bayesian multilevel modeling. From his blog, you’ll learn additional tricks, like making a more traditional-looking forest plot with the brmstools::forest() function and how our Bayesian brms method compares with Frequentist meta-analyses via the metafor package. You might also check out Williams, Rast, and Bürkner’s manuscript, Bayesian Meta-Analysis with Weakly Informative Prior Distributions to give you an empirical justification for using a half-Cauchy prior for your meta-analysis \\(\\tau\\) parameters.\n Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.1.9000 brms_2.12.0 Rcpp_1.0.3 ## [4] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 ## [7] purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 ## [10] tibble_2.1.3 ggplot2_3.2.1 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ## [3] ggridges_0.5.2 rsconnect_0.8.16 ## [5] markdown_1.1 base64enc_0.1-3 ## [7] fs_1.3.1 rstudioapi_0.10 ## [9] farver_2.0.3 rstan_2.19.2 ## [11] svUnit_0.7-12 DT_0.11 ## [13] fansi_0.4.1 mvtnorm_1.0-12 ## [15] lubridate_1.7.4 xml2_1.2.2 ## [17] bridgesampling_0.8-1 knitr_1.26 ## [19] shinythemes_1.1.2 bayesplot_1.7.1 ## [21] jsonlite_1.6.1 broom_0.5.3 ## [23] dbplyr_1.4.2 shiny_1.4.0 ## [25] compiler_3.6.2 httr_1.4.1 ## [27] backports_1.1.5 assertthat_0.2.1 ## [29] Matrix_1.2-18 fastmap_1.0.1 ## [31] lazyeval_0.2.2 cli_2.0.1 ## [33] later_1.0.0 htmltools_0.4.0 ## [35] prettyunits_1.1.1 tools_3.6.2 ## [37] igraph_1.2.4.2 coda_0.19-3 ## [39] gtable_0.3.0 glue_1.3.1 ## [41] reshape2_1.4.3 cellranger_1.1.0 ## [43] vctrs_0.2.2 nlme_3.1-142 ## [45] blogdown_0.17 crosstalk_1.0.0 ## [47] xfun_0.12 ps_1.3.0 ## [49] rvest_0.3.5 mime_0.8 ## [51] miniUI_0.1.1.1 lifecycle_0.1.0 ## [53] gtools_3.8.1 zoo_1.8-7 ## [55] scales_1.1.0 colourpicker_1.0 ## [57] hms_0.5.3 promises_1.1.0 ## [59] Brobdingnag_1.2-6 parallel_3.6.2 ## [61] inline_0.3.15 shinystan_2.5.0 ## [63] yaml_2.2.1 gridExtra_2.3 ## [65] loo_2.2.0 StanHeaders_2.19.0 ## [67] stringi_1.4.6 highr_0.8 ## [69] dygraphs_1.1.1.6 pkgbuild_1.0.6 ## [71] rlang_0.4.5 pkgconfig_2.0.3 ## [73] matrixStats_0.55.0 evaluate_0.14 ## [75] lattice_0.20-38 labeling_0.3 ## [77] rstantools_2.0.0 htmlwidgets_1.5.1 ## [79] tidyselect_1.0.0 processx_3.4.1 ## [81] plyr_1.8.5 magrittr_1.5 ## [83] bookdown_0.17 R6_2.4.1 ## [85] generics_0.0.2 DBI_1.1.0 ## [87] pillar_1.4.3 haven_2.2.0 ## [89] withr_2.1.2 xts_0.12-0 ## [91] abind_1.4-5 modelr_0.1.5 ## [93] crayon_1.3.4 arrayhelpers_1.0-20160527 ## [95] utf8_1.1.4 rmarkdown_2.0 ## [97] grid_3.6.2 readxl_1.3.1 ## [99] callr_3.4.1 threejs_0.3.3 ## [101] reprex_0.3.0 digest_0.6.23 ## [103] xtable_1.8-4 httpuv_1.5.2 ## [105] stats4_3.6.2 munsell_0.5.0 ## [107] viridisLite_0.3.0 shinyjs_1.1  ","date":1539475200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539475200,"objectID":"e4e0cca48a080bc20b2083715d19b5bf","permalink":"/post/bayesian-meta-analysis/","publishdate":"2018-10-14T00:00:00Z","relpermalink":"/post/bayesian-meta-analysis/","section":"post","summary":"[edited Feb 27, 2019]\nPreamble I released the first bookdown version of my Statistical Rethinking with brms, ggplot2, and the tidyverse project a couple weeks ago. I consider it the 0.9.0 version. I wanted a little time to step back from the project before giving it a final edit for the first major edition. I also wanted to give others a little time to take a look and suggest edits, which some thankfully have.","tags":["Bayesian","brms","R","meta-analysis","spanking","Statistical Rethinking","tutorial"],"title":"Bayesian meta-analysis in brms","type":"post"},{"authors":null,"categories":[],"content":" The other day, my Twitter feed informed me Penn Jillette just clocked in 1000 consecutive days of meditation using the Headspace app. Now he’s considering checking out Sam Harris’s new Waking Up meditation app. Sam left a congratulatory comment on Penn’s tweet.\nWell done, Penn! https://t.co/SZkKocIPEH\n\u0026mdash; Sam Harris (@SamHarrisOrg) October 7, 2018  The whole thing was rainbows and kittens. And it reminded me to pass on some advice: Stop using fee-based meditation apps!\nWhat? The Headspace app is popular and highly-rated. It’s free to download and has some nice features, like reminders to meditate. However, if you want full access to its library of guided meditation audio recordings, you’ll need to pay a fee at a monthly, yearly, or lifetime rate.\nSam Harris’s Waking Up app app is also free to download. It’s new and, in fairness, we’ll have to wait and see how its format will unfold. But at present it has more of a course-type format. The free version gives you access to five guided meditations and three lessons. But if you want full access to the app’s content, you also have to subscribe.\nI have no problem with the Headspace and Waking Up apps. They have many fine features. And I’m even a fan of a lot Harris’s work. But we have a cheaper, high-quality option:\n Consider Insight Timer Insight Timer comes with a free and pay versions, too. But just download the free version. It’s excellent and all you need for your meditation needs. Let me list the reasons why.\nThe free library is extensive. At the time of this writing, my free subscription to Insight Timer gives me access to some 12,000 guided meditation audios. Most of them are in English. But many are offered in other languages, such as Hebrew, Malay, and Spanish.\nDuration. Their durations vary. The bulk of the guided meditations seem to be in the 5-to-20-minute range. But some last more than an hour.\n People and their voices. Insight Timer’s deep library boasts an impressive cast of meditation teachers. I was happy to see some familiar high-profile meditation teachers, such as Tara Brach and Joseph Goldstein. But I have also discovered new favorites, like Stephen Pende Wormland and Dawn Mauricio. To be sure, the quality of the audio recordings varies. But more importantly, they also vary in terms of vocal tone and pacing. There should be a vocal style to suit just about everyone.\n Emphasis. I’m an academic and generally prefer meditations that are secular and connected to the clinical literature (e.g., this recent meta-analysis). Happily, the Insight Timer library contains offerings based on mindfulness-based stress reduction (MBSR) and its derivatives (e.g., mindfulness-based cognitive therapy, mindfulness-based relapse prevention).\nBut you can also find meditations grounded within a number of faith traditions.\n Music. I’ve focused mainly on vocally-driven meditations. However, Insight Timer also contains music/sound-based recordings. I’m partial to recordings featuring Tibetan singing bowls.\n  It’s a timer, too. Sometimes you just want a silent meditation. For those occasions, Insight Timer offers a nice timer feature. You can set it like a stopwatch to whatever duration you prefer, and choose among an array of sounds to mark the beginning and end of your sit.\n It tracks. If you go to the Profile section of the app, you’ll discover it keeps track of your use and displays various summaries in attractive bar plots.\n[And yes, I generally agree with Richard McElreath: “The only problem with barplots is that they have bars” (p. 203, Statistical Rethinking). You can’t have everything.]\n  Insight Timer is great for researchers Insight Timer allows you to download your data as a CSV file, which you can keep for yourself or email to others. Over the years, I’ve used the app to run group meditations within my research protocols. Since I selected the audios from the app, it allowed me to standardize the instructions across meditation sessions. Although my research assistants and I used the app on our phones to play the audios, our participants would use the timer functions on the apps on their phones to record their sessions. This gave us duplicate attendance records: one on a sign-in sheet and another on their phones. In longitudinal studies, participants could use the app on their own time and each of those sessions were recorded in the app. At the end of our studies, we were then able to download their use records as CSV files ready for pre-analysis data wrangling.\n Parting thoughts Insight Timer has other functions, such as social networking and dharma talks. I just don’t care about those things, so you can learn about them on your own. But if you’re interested in learning about meditation or even if you’re a veteran meditator looking for a convenient app to augment your practice with, do consider Insight Timer. There’s no reason to spend your money on the alternatives before you capitalize on such a powerful resource that’s available to you for free.\n ","date":1538870400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538870400,"objectID":"798a524eefe980436a075f3a7f77ccee","permalink":"/post/dont-pay-to-meditate/","publishdate":"2018-10-07T00:00:00Z","relpermalink":"/post/dont-pay-to-meditate/","section":"post","summary":"The other day, my Twitter feed informed me Penn Jillette just clocked in 1000 consecutive days of meditation using the Headspace app. Now he’s considering checking out Sam Harris’s new Waking Up meditation app. Sam left a congratulatory comment on Penn’s tweet.\nWell done, Penn! https://t.co/SZkKocIPEH\n\u0026mdash; Sam Harris (@SamHarrisOrg) October 7, 2018  The whole thing was rainbows and kittens. And it reminded me to pass on some advice: Stop using fee-based meditation apps!","tags":["app","Insight Timer","MBCT","MBRP","MBSR","meditation","mindfulness","smartphone"],"title":"Don't pay to meditate","type":"post"},{"authors":null,"categories":[],"content":" tl;dr I just self-published a book-length version of my project Statistical Rethinking with brms, ggplot2, and the tidyverse. By using Yihui Xie’s bookdown package, I was able to do it for free. If you’ve never heard of it, bookdown enables R users to write books and other long-form articles with R Markdown. You can save your bookdown products in a variety of formats (e.g., PDF, HTML) and publish them in several ways, too. The purpose of this post is to give readers a sense of how I used bookdown to make my project. I propose there are three fundamental skill sets you need basic fluency in before playing with bookdown. Those three are\n R and R Studio, Scripts and R Markdown files, and Git and GitHub.   Start with R First things first. Since bookdown is a package for use in the R environment, you’re going to have to use R. If you’re unfamiliar with it, R is a freely-available programming language particularly well-suited for data analysis. If you’ve not used R before, learning how to self-publish books is a great incentive to start learning. But unless you already have a background in programming, I think bookdown is poorly-suited for novices. R newbies should check out Roger Peng’s R Programming for Data Science or Grolemund and Wickham’s R for Data Science. Both are freely available online and, as it would turn out, made with bookdown. Also, new users should be aware that although you can interact with R directly, there are a variety of other ways to interface with R. I recommend using R Studio. You can find some nice reasons, here. For basic instructions on how to install R and R Studio, you might start here. And if you prefer video tutorials to help you with the installation, just do a simple search in your favorite video-sharing website and several should pop up.\nPersonally, I started using R—via R Studio—during the 2015/2016 winter break before taking a spring semester statistics course based around an R package. [In case you’re curious, it was a structural equation modeling course based around a text by Beaujean which featured the lavaan package]. At the time, I was already familiar with structural equation modeling, so the course was a nice opportunity to learn R. In addition, I was concurrently enrolled in a course on multilevel modeling based on Singer and Willet’s classic text. The professor of that course primarily used SAS to teach the material, but he was flexible and allowed me to do the work with R, instead. So that was my introduction to R–a semester of immersion in #rstats. Here are some other tips on how to learn R.\n bookdown uses Markdown If you work with R through R Studio, you can do a handful of things through dropdowns. But really, if you’re going to be using R, you’re going to be coding. As it turns out, there are a variety of ways to code in R. One of the most basic ways is via the console, which I’m not going to cover in any detail.\nThe console is fine for quick operations, but you’re going to want to do most of your coding in some kind of a script. R Studio allows users to save and execute code in script files, which you can learn more about here. Basic script files are nice in that they allow you to both save and annotate your code.\nHowever, the annotation options in R Studio script files are limited. After using R Studio scripts for about a year, I learned about R Notebooks. These are special files that allow you to intermingle your R code with prose and the results of the code. R Notebooks also allow users to transform the working documents into professional-looking reports in various formats (e.g., PDF, HTML). And unlike the primitive annotation options with simple script files, R Notebooks use Markdown to allow users to format their prose with things like headers, italicized font, insert hyperlinks, and even embed images. So Markdown, then, is a simple language that allows for many of those functions.\nWithin the R Studio environment, you can use Markdown with two basic file types: R Markdown files and R Notebook files. R Notebook files are just special kinds of R Markdown files that have, IMO, a better interface. That is, R Notebooks are the newer nicer version of R Markdown files. The main point here is that when I say “bookdown uses Markdown”, I’m pointing out that one of the important skills you’ll want to develop before making content with bookdown is how to use Markdown within R Studio. It’s not terribly complicated to learn, and you can get an overview of the basics here or here or here, or an exhaustive treatment here.\nIf you’re a novice, it’ll take you a few days, weeks, or months to get a firm grasp of R. Not so with R Markdown files. You’ll have the basics of those down in an afternoon. That said, I had been an R Notebook user for more than a year before trying my hand at bookdown.\nThe first big edition of my Statistical Rethinking with brms, ggplot2, and the tidyverse project came in the form of R Notebook files and their HTML counterparts stored in one of my projects on the Open Science Framework. I don’t update it very often, but you can still find it here. If you’re not familiar with it, the OSF is a “free, open source web application that connects and supports the research workflow, enabling scientists to increase the efficiency and effectiveness of their research.” In addition to their wiki, you might check out some of their video tutorials.\n You’ll need GitHub, too I’m actually not sure whether you need to know how to use Git and GitHub to use bookdown. In his authoritative book, bookdown: Authoring Books and Technical Documents with R Markdown, Yihui Xie mentioned GitHub in every chapter. If you go to your favorite video-sharing website to look for instructional videos on bookdown, you’ll see the instructors take GitHub as a given, too. If you’re stubborn and have enough ingenuity, you might find a way to successfully use bookdown without GitHub, but you may as well go with the flow on this one.\nIf you’ve never heard of it before, Git is a system for version control. By version control, I mean a system by which you can keep track of changes to your code, over time. Even if you don’t have a background in programming, consider a scenario where you had to keep track of many versions of a writing project, perhaps saving your files as first_draft.docx, second_draft.docx, final_draft.docx, final_draft_2.docx… This was your own make-shift attempt at version control for writing. I’ve seen a lot of introductory material recommend Git and GitHub by leading with version control. And indeed, they do serve that purpose. But IMO, leading with version control is a rhetorical mistake when talking to non-programmers. I haven’t found Git and GitHub the most intuitive and if version control was the only benefit, they wouldn’t be worth the effort. But there are other good reasons to learn.\nIMO, the best reason to learn Git and GitHub is because they allow you to make your work publically available. When you just use Git, the work stays on your computer. But GitHub allows you to save your files online, too. This makes it easy for others to review them and give you feedback. GitHub also allows you to save things like data files online. So if you’re a working scientist, Git and GitHub might allow you to make a site—a repository—to house the de-identified data and statistical code for one of your projects. It’s another way to do open science. In addition, you can repurpose GitHub to work as blog or an analytic portfolio. And if you’d like to use bookdown, Git and GitHub will be a part of how you manage the files for your projects and make your work more accessible to others.\nIf you’re new to all this, you could probably blindly follow along with the steps in Yihui Xie’s bookdown manual or any of the online video tutorials. But I suspect that’d be pretty confusing. Before attempting a bookdown project, spend some time getting comfortable with Git and GitHub, first. The best introduction to the topic I’ve seen is Jenny Bryan’s Happy Git and GitHub for the useR, which, you guessed it, is also freely available and powered by bookdown.\nAs I hinted, I found Git and GitHub baffling, at first. I checked out a few online video tutorials, but found them of little help. It really was Bryan’s book that finally got me going. And I’m glad I did. I’ve been slowly working with GitHub for about a year—here’s my profile—and my first major project was putting together the files for the individual chapters in the Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse project. They originally lived as R Notebook files, eventually rendered in a GitHub-friendly .md file format. After a while, I started playing around with README-only projects, which are basically a poor man’s GitHub version of blog posts (e.g., check out this one). For me, and probably for your future bookdown projects, the most important GitHub skills to learn are commits, pushes, and forkes.\nI’d fooled around with GitHub a tiny bit before launching my Statistical Rethinking with brms, ggplot2, and the tidyverse project on the OSF. But it was confusing and after an hour or two of trying to make sense of it, I gave up and just figured the OSF would be good enough. After folks started noticing the project, I got a few comments that it’d be more accessible on GitHub. That was what finally influenced me to buckle down learn it in earnest. I’m still a little clunky with it, but I’m functional enough to do things like make this blog. With a little patience and practice, you can get there, too.\n Let Yihui Xie guide you So far we’ve covered\n R and R Studio Scripts and R Markdown files Git and GitHub  You don’t have become an expert, but you’ll need to become roughly fluent in all three to make good use of bookdown. Basically, if you are able to load data into R, document a rudimentary analysis in an R Notebook file, and then share the project in a non-embarrassing way in GitHub, you’re ready to use bookdown.\nI’ve already mentioned it, but the authoritative work on bookdown is Yihui Xie’s bookdown: Authoring Books and Technical Documents with R Markdown. Yihui Xie, of course, is the author of the package. It’s probably best to just start there, going bit by bit. He also gave an RStudio webinar, Authoring Books with R Markdown, which I found to be a helpful supplement.\nThe complete version of my Statistical Rethinking with brms, ggplot2, and the tidyverse project has 15 chapters and several preamble sections. Almost all the chapters files include a lot of computationally-intensive code, with the simulations for chapter 6 taking multiple hours to compute. I do not recommend starting off with a project like that, at least not all at once. If you follow along with Yihui Xie’s guide, you’ll practice stitching together simple files, first. After learning those basics, I then picked up other helpful tricks, like caching analyses.\nAlthough I didn’t use these resources while I was learning bookdown, you might also benefit from checking out\n Sean Kross’s How to Start a Bookdown Book, Karl Broman’s omg, bookdown!, Rachael Lappan’s Using Bookdown for tidy documentation, or Pablo Casas’s How to self-publish a book: A handy list of resources.   ","date":1538611200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538611200,"objectID":"f9676350628488b9d5a7ed32711cdf5f","permalink":"/post/how-bookdown/","publishdate":"2018-10-04T00:00:00Z","relpermalink":"/post/how-bookdown/","section":"post","summary":"tl;dr I just self-published a book-length version of my project Statistical Rethinking with brms, ggplot2, and the tidyverse. By using Yihui Xie’s bookdown package, I was able to do it for free. If you’ve never heard of it, bookdown enables R users to write books and other long-form articles with R Markdown. You can save your bookdown products in a variety of formats (e.g., PDF, HTML) and publish them in several ways, too.","tags":["Bayesian","bookdown","brms","Git","GitHub","Markdown","R","Statistical Rethinking","tidyverse","tutorial"],"title":"bookdown, My Process","type":"post"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536469200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536469200,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00-05:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":null,"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three  A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating.","tags":null,"title":"Slides","type":"slides"}]