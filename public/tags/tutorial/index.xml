<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tutorial on A. Solomon Kurz</title>
    <link>/tags/tutorial/</link>
    <description>Recent content in tutorial on A. Solomon Kurz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Mon, 09 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/tutorial/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Make model diagrams, Kruschke style</title>
      <link>/post/make-model-diagrams-kruschke-style/</link>
      <pubDate>Mon, 09 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/make-model-diagrams-kruschke-style/</guid>
      <description>tl;dr You too can make model diagrams with the tidyverse and patchwork packages. Here’s how.
 Diagrams can help us understand statistical models. I’ve been working through John Kruschke’s Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan and translating it into brms and tidyverse-style workflow. At this point, the bulk of the work is done and you can check it out at https://bookdown.org/content/3686/. One of Kruschke’s unique contributions was the way he used diagrams to depict his statistical models.</description>
    </item>
    
    <item>
      <title>Time-varying covariates in longitudinal multilevel models contain state- and trait-level information: This includes binary variables, too</title>
      <link>/post/time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/</link>
      <pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/time-varying-covariates-in-longitudinal-multilevel-models-contain-state-and-trait-level-information-this-includes-binary-variables-too/</guid>
      <description>tl;dr When you have a time-varying covariate you’d like to add to a multilevel growth model, it’s important to break that variable into two. One part of the variable will account for within-person variation. The other part will account for between person variation. Keep reading to learn how you might do so when your time-varying covariate is binary.
 I assume things. For this post, I’m presuming you are familiar with longitudinal multilevel models and vaguely familiar with the basic differences between frequentist and Bayesian statistics.</description>
    </item>
    
    <item>
      <title>Individuals are not small groups, II: The ecological fallacy</title>
      <link>/post/individuals-are-not-small-groups-ii-the-ecological-fallacy/</link>
      <pubDate>Mon, 14 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/individuals-are-not-small-groups-ii-the-ecological-fallacy/</guid>
      <description>tl;dr When people conclude results from group-level data will tell you about individual-level processes, they commit the ecological fallacy. This is true even of the individuals whose data contributed to those group-level results. This phenomenon can seem odd and counterintuitive. Keep reading to improve your intuition.
 We need history. The ecological fallacy is closely related to Simpson’s paradox 1. It is often attributed to sociologist William S. Robinson’s (1950) paper Ecological Correlations and the Behavior of Individuals.</description>
    </item>
    
    <item>
      <title>Individuals are not small groups, I: Simpson&#39;s paradox</title>
      <link>/post/individuals-are-not-small-groups-i-simpson-s-paradox/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/individuals-are-not-small-groups-i-simpson-s-paradox/</guid>
      <description>tl;dr If you are under the impression group-level data and group-based data analysis will inform you about within-person processes, you would be wrong. Stick around to learn why.
 This is gonna be a long car ride. Earlier this year I published a tutorial 1 on a statistical technique that will allow you to analyze the multivariate time series data of a single individual. It’s called the dynamic p-technique. The method has been around since at least the 80s (Molenaar, 1985) and its precursors date back to at least the 40s (Cattell, Cattell, &amp;amp; Rhymer, 1947).</description>
    </item>
    
    <item>
      <title>Bayesian power analysis: Part III.b. What about 0/1 data?</title>
      <link>/post/bayesian-power-analysis-part-iii-b/</link>
      <pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-power-analysis-part-iii-b/</guid>
      <description>Version 1.0.0 In the last post, we covered how the Poisson distribution is handy for modeling count data. Binary data are even weirder than counts. They typically only take on two values: 0 and 1. Sometimes 0 is a stand-in for “no” and 1 for “yes” (e.g., Are you an expert in Bayesian power analysis? For me that would be 0). You can also have data of this kind if you asked people whether they’d like to choose option A or B.</description>
    </item>
    
    <item>
      <title>Bayesian power analysis: Part III.a. Counts are special.</title>
      <link>/post/bayesian-power-analysis-part-iii-a/</link>
      <pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-power-analysis-part-iii-a/</guid>
      <description>Version 1.0.0  tl;dr So far we’ve covered Bayesian power simulations from both a null hypothesis orientation (see part I) and a parameter width perspective (see part II). In both instances, we kept things simple and stayed with Gaussian (i.e., normally distributed) data. But not all data follow that form, so it might do us well to expand our skill set a bit. In the next few posts, we’ll cover how we might perform power simulations with other kinds of data.</description>
    </item>
    
    <item>
      <title>Bayesian power analysis: Part II. Some might prefer precision to power</title>
      <link>/post/bayesian-power-analysis-part-ii/</link>
      <pubDate>Wed, 24 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-power-analysis-part-ii/</guid>
      <description>Version 1.0.0  tl;dr When researchers decide on a sample size for an upcoming project, there are more things to consider than null-hypothesis-oriented power. Bayesian researchers might like to frame their concerns in terms of precision. Stick around to learn what and how.
 Are Bayesians doomed to refer to \(H_0\) 1 with sample-size planning? If you read my last post, you may have found yourself thinking: Sure, last time you avoided computing \(p\)-values with your 95% Bayesian credible intervals.</description>
    </item>
    
    <item>
      <title>Bayesian power analysis: Part I. Prepare to reject $H_0$ with simulation.</title>
      <link>/post/bayesian-power-analysis-part-i/</link>
      <pubDate>Thu, 18 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-power-analysis-part-i/</guid>
      <description>Version 1.0.1  tl;dr If you’d like to learn how to do Bayesian power calculations using brms, stick around for this multi-part blog series. Here with part I, we’ll set the foundation.
 Power is hard, especially for Bayesians. Many journals, funding agencies, and dissertation committees require power calculations for your primary analyses. Frequentists have a variety of tools available to perform these calculations (e.g., here). Bayesians, however, have a more difficult time of it.</description>
    </item>
    
    <item>
      <title>Would you like all your posteriors in one plot?</title>
      <link>/post/would-you-like-all-your-posteriors-in-one-plot/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/would-you-like-all-your-posteriors-in-one-plot/</guid>
      <description>A colleague reached out to me earlier this week with a plotting question. They had fit a series of Bayesian models, all containing a common parameter of interest. They knew how to plot their focal parameter one model at a time, but were stumped on how to combine the plots across models into a seamless whole. It reminded me a bit of this gif
which I originally got from Jenny Bryan’s great talk, Behind every great plot there’s a great deal of wrangling.</description>
    </item>
    
    <item>
      <title>Stein’s Paradox and What Partial Pooling Can Do For You</title>
      <link>/post/stein-s-paradox-and-what-partial-pooling-can-do-for-you/</link>
      <pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/stein-s-paradox-and-what-partial-pooling-can-do-for-you/</guid>
      <description>tl;dr  Sometimes a mathematical result is strikingly contrary to generally held belief even though an obviously valid proof is given. Charles Stein of Stanford University discovered such a paradox in statistics in 1995. His result undermined a century and a half of work on estimation theory. (Efron &amp;amp; Morris, 1977, p. 119)
 The James-Stein estimator leads to better predictions than simple means. Though I don’t recommend you actually use the James-Stein estimator in applied research, understanding why it works might help clarify why it’s time social scientists consider defaulting to multilevel models for their work-a-day projects.</description>
    </item>
    
    <item>
      <title>Bayesian Correlations: Let’s Talk Options.</title>
      <link>/post/bayesian-correlations-let-s-talk-options/</link>
      <pubDate>Sat, 16 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-correlations-let-s-talk-options/</guid>
      <description>tl;dr There’s more than one way to fit a Bayesian correlation in brms.
 Here’s the deal. In the last post, we considered how we might estimate correlations when our data contain influential outlier values. Our big insight was that if we use variants of Student’s \(t\)-distribution as the likelihood rather than the conventional normal distribution, our correlation estimates were less influenced by those outliers. And we mainly did that as Bayesians using the brms package.</description>
    </item>
    
    <item>
      <title>Bayesian robust correlations with brms (and why you should love Student’s $t$)</title>
      <link>/post/bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/</guid>
      <description>[edited June 18, 2019]
In this post, we’ll show how Student’s \(t\)-distribution can produce better correlation estimates when your data have outliers. As is often the case, we’ll do so as Bayesians.
This post is a direct consequence of Adrian Baez-Ortega’s great blog, “Bayesian robust correlation with Stan in R (and why you should use Bayesian methods)”. Baez-Ortega worked out the approach and code for direct use with Stan computational environment.</description>
    </item>
    
    <item>
      <title>Robust Linear Regression with Student’s $t$-Distribution</title>
      <link>/post/robust-linear-regression-with-the-robust-student-s-t-distribution/</link>
      <pubDate>Sat, 02 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/robust-linear-regression-with-the-robust-student-s-t-distribution/</guid>
      <description>[edited Feb 3, 2019]
The purpose of this post is to demonstrate the advantages of the Student’s \(t\)-distribution for regression with outliers, particularly within a Bayesian framework.
I make assumptions I’m presuming you are familiar with linear regression, familiar with the basic differences between frequentist and Bayesian approaches to fitting regression models, and have a sense that the issue of outlier values is a pickle worth contending with. All code in is R, with a heavy use of the tidyverse–which you might learn a lot about here, especially chapter 5– and Paul Bürkner’s brms package.</description>
    </item>
    
    <item>
      <title>Make rotated Gaussians, Kruschke style</title>
      <link>/post/make-rotated-gaussians-kruschke-style/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/make-rotated-gaussians-kruschke-style/</guid>
      <description>[edited Dec 23, 2018]
tl;dr You too can make sideways Gaussian density curves within the tidyverse. Here’s how.
 Here’s the deal: I like making pictures. Over the past several months, I’ve been slowly chipping away at John Kruschke’s Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Kruschke has a unique plotting style. One of the quirks is once in a while he likes to express the results of his analyses in plots where he shows the data alongside density curves of the model-implied data-generating distributions.</description>
    </item>
    
    <item>
      <title>Bayesian meta-analysis in brms</title>
      <link>/post/bayesian-meta-analysis/</link>
      <pubDate>Sun, 14 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian-meta-analysis/</guid>
      <description>[edited Feb 27, 2019]
Preamble I released the first bookdown version of my Statistical Rethinking with brms, ggplot2, and the tidyverse project a couple weeks ago. I consider it the 0.9.0 version. I wanted a little time to step back from the project before giving it a final edit for the first major edition. I also wanted to give others a little time to take a look and suggest edits, which some thankfully have.</description>
    </item>
    
    <item>
      <title>bookdown, My Process</title>
      <link>/post/how-bookdown/</link>
      <pubDate>Thu, 04 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/how-bookdown/</guid>
      <description>tl;dr I just self-published a book-length version of my project Statistical Rethinking with brms, ggplot2, and the tidyverse. By using Yihui Xie’s bookdown package, I was able to do it for free. If you’ve never heard of it, bookdown enables R users to write books and other long-form articles with R Markdown. You can save your bookdown products in a variety of formats (e.g., PDF, HTML) and publish them in several ways, too.</description>
    </item>
    
  </channel>
</rss>