<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.49" />
  <meta name="author" content="A. Solomon Kurz">

  
  
  
  
    
  
  <meta name="description" content="In this post, we’ll show how Student’s \(t\)-distribution can produce better correlation estimates when you data have outliers. As is often the case, we’ll do so as Bayesians.
This post is a direct consequence of Adrian Baez-Ortega’s great blog, “Bayesian robust correlation with Stan in R (and why you should use Bayesian methods)”. Baez-Ortega worked out the approach and code for use with Stan, directly. Since I’m a fan of the brms package, it seemed only right that we might extend Baez-Ortega’s method to brms.">

  
  <link rel="alternate" hreflang="en-us" href="/post/bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/">

  


  

  

  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
      
    

    

    

  

  
  
  <link rel="stylesheet" href=//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono>
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="A. Solomon Kurz">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="A. Solomon Kurz">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@SolomonKurz">
  <meta property="twitter:creator" content="@SolomonKurz">
  
  <meta property="og:site_name" content="A. Solomon Kurz">
  <meta property="og:url" content="/post/bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t/">
  <meta property="og:title" content="Bayesian robust correlations with brms (and why you should love Student’s $t$) | A. Solomon Kurz">
  <meta property="og:description" content="In this post, we’ll show how Student’s \(t\)-distribution can produce better correlation estimates when you data have outliers. As is often the case, we’ll do so as Bayesians.
This post is a direct consequence of Adrian Baez-Ortega’s great blog, “Bayesian robust correlation with Stan in R (and why you should use Bayesian methods)”. Baez-Ortega worked out the approach and code for use with Stan, directly. Since I’m a fan of the brms package, it seemed only right that we might extend Baez-Ortega’s method to brms.">
  
  
    
  <meta property="og:image" content="/img/Solomon%20at%20Double%20Decker.jpg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-02-09T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2019-02-09T00:00:00&#43;00:00">
  

  

  

  <title>Bayesian robust correlations with brms (and why you should love Student’s $t$) | A. Solomon Kurz</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">A. Solomon Kurz</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/post/">
            
            <span>Blog posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/bookdown/">
            
            <span>Book-length projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/teaching/">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

      

        

        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  





  <div class="article-container">
    <h1 itemprop="name">Bayesian robust correlations with brms (and why you should love Student’s $t$)</h1>

    

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="A. Solomon Kurz">
  </span>
  

  <span class="article-date">
    
    <meta content="2019-02-09 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2019-02-09 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      Feb 9, 2019
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="A. Solomon Kurz">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    15 min read
  </span>
  

  
  

  
  
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Bayesian%20robust%20correlations%20with%20brms%20%28and%20why%20you%20should%20love%20Student%e2%80%99s%20%24t%24%29&amp;url=%2fpost%2fbayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpost%2fbayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2fbayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t%2f&amp;title=Bayesian%20robust%20correlations%20with%20brms%20%28and%20why%20you%20should%20love%20Student%e2%80%99s%20%24t%24%29"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpost%2fbayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t%2f&amp;title=Bayesian%20robust%20correlations%20with%20brms%20%28and%20why%20you%20should%20love%20Student%e2%80%99s%20%24t%24%29"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Bayesian%20robust%20correlations%20with%20brms%20%28and%20why%20you%20should%20love%20Student%e2%80%99s%20%24t%24%29&amp;body=%2fpost%2fbayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      <p>In this post, we’ll show how Student’s <span class="math inline">\(t\)</span>-distribution can produce better correlation estimates when you data have outliers. As is often the case, we’ll do so as Bayesians.</p>
<p>This post is a direct consequence of Adrian Baez-Ortega’s great blog, “<a href="https://baezortega.github.io/2018/05/28/robust-correlation/">Bayesian robust correlation with Stan in R (and why you should use Bayesian methods)</a>”. Baez-Ortega worked out the approach and code for use with <a href="http://mc-stan.org">Stan</a>, directly. Since I’m a fan of the <a href="https://github.com/paul-buerkner/brms">brms</a> package, it seemed only right that we might extend Baez-Ortega’s method to brms. To pay respects where they’re due, the synthetic data, priors, and other model settings are largely the same as those Baez-Ortega used in his blog.</p>
<div id="i-make-assumptions" class="section level2">
<h2>I make assumptions</h2>
<p>For this project, I’m presuming you are vaguely familiar with linear regression, familiar with the basic differences between frequentist and Bayesian approaches to fitting models, and have a sense that the issue of outlier values is a pickle worth contending with. All code in is <a href="https://www.r-bloggers.com/why-use-r-five-reasons/">R</a>, with a heavy use of the <a href="http://style.tidyverse.org">tidyverse</a>–which you might learn a lot about <a href="http://r4ds.had.co.nzhttp://r4ds.had.co.nz">here, especially chapter 5</a>–, and, of course, Bürkner’s <a href="https://github.com/paul-buerkner/brms">brms</a>.</p>
<p>If you’d like a warmup, consider checking out my related post, <a href="https://solomonkurz.netlify.com/post/robust-linear-regression-with-the-robust-student-s-t-distribution/">Robust Linear Regression with Student’s <span class="math inline">\(t\)</span>-Distribution</a>.</p>
</div>
<div id="whats-the-deal" class="section level2">
<h2>What’s the deal?</h2>
<p>Pearson’s correlations are designed to quantify the linear relationship between two normally distributed variables. The normal distribution and its multivariate generalization, the multivariate normal distribution, are sensitive to outliers. When you have well-behaved synthetic data, this isn’t an issue. But if you work with wild and rough real-world data, this can be a problem. One can have data for which the vast majority of cases are well-characterized by a nice liner relationship, but have a few odd cases for which that relationship does not hold. And if those odd cases happen to be overly influential–sometimes called leverage points–the resulting Pearson’s correlation coefficient might look off.</p>
<p>The normal distribution is a special case of Student’s <span class="math inline">\(t\)</span>-distribution with the <span class="math inline">\(\nu\)</span> parameter (i.e., <em>nu</em>, degree of freedom) set to infinity. However, when <span class="math inline">\(\nu\)</span> is small, Student’s <span class="math inline">\(t\)</span>-distribution is more robust to multivariate outliers. It’s less influenced by them. I’m not going to cover why in any detail. For that you’ve got <a href="https://baezortega.github.io/2018/05/28/robust-correlation/">Baez-Ortega’s blog</a>, an even earlier blog from <a href="http://www.sumsar.net/blog/2013/08/bayesian-estimation-of-correlation/">Rasmus Bååth</a>, and textbook treatments on the topic by <a href="http://www.stat.columbia.edu/~gelman/arm/">Gelman &amp; Hill (2007, chapter 6)</a> and <a href="https://sites.google.com/site/doingbayesiandataanalysis/">Kruschke (2014, chapter 16)</a>. Here we’ll get a quick sense of how vulnerable Pearson’s correlations–with their reliance on the Gaussian–are to outliers, we’ll demonstrate how fitting correlations within the Bayesian paradigm using the conventional Gaussian likelihood is similarly vulnerable to distortion, and then demonstrate how Student’s <span class="math inline">\(t\)</span>-distribution can save the day. And importantly, we’ll do the bulk of this with the brms package.</p>
</div>
<div id="we-need-data" class="section level2">
<h2>We need data</h2>
<p>To start off, we’ll make a multivariate normal simulated data set using the same steps Baez-Ortega’s used.</p>
<pre class="r"><code>library(mvtnorm)
library(tidyverse)

sigma &lt;- c(20, 40)  # the variances
rho   &lt;- -0.95      # the desired correlation

# here&#39;s the variance/covariance matrix
cov.mat &lt;- 
  matrix(c(sigma[1] ^ 2,
           sigma[1] * sigma[2] * rho,
           sigma[1] * sigma[2] * rho,
           sigma[2] ^ 2),
         nrow = 2, byrow = T)

# after setting our seed, we&#39;re ready to simulate with `rmvnorm()`
set.seed(210191)
x.clean &lt;- 
  rmvnorm(n = 40, sigma = cov.mat) %&gt;% 
  as_tibble() %&gt;% 
  rename(x = V1,
         y = V2)</code></pre>
<p>Here we make our second data set, <code>x.noisy</code>, which is identical to our well-behaved <code>x.clean</code> data, but with the first three cases transformed to outlier values.</p>
<pre class="r"><code>x.noisy &lt;- x.clean

x.noisy[1:3,] &lt;-
  matrix(c(-40, -60,
           20, 100,
           40, 40),
         nrow = 3, byrow = T)</code></pre>
<p>Finally, we’ll add an <code>outlier</code> index to the data sets, which will help us with plotting.</p>
<pre class="r"><code>x.clean &lt;-
  x.clean %&gt;% 
  mutate(outlier = factor(0))

x.noisy &lt;- 
  x.noisy %&gt;% 
  mutate(outlier = c(rep(1, 3), rep(0, 37)) %&gt;% as.factor(.))</code></pre>
<p>The plot below shows what the <code>x.clean</code> data look like. I’m a fan of <a href="http://fivethirtyeight.com">FiveThirtyEight</a>, so we’ll use a few convenience functions from the handy <a href="https://github.com/jrnold/ggthemes">ggthemes package</a> to give our plots a FiveThirtyEight-like feel.</p>
<pre class="r"><code>library(ggthemes)

x.clean %&gt;% 
  ggplot(aes(x = x, y = y, color = outlier, fill = outlier)) +
  geom_point() +
  stat_ellipse(geom = &quot;polygon&quot;, alpha = .15, size = .15, level = .5) +
  stat_ellipse(geom = &quot;polygon&quot;, alpha = .15, size = .15, level = .95) +
  scale_color_fivethirtyeight() +
  scale_fill_fivethirtyeight() +
  coord_cartesian(xlim = -50:50,
                  ylim = -100:100) +
  theme_fivethirtyeight() +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="/post/2019-02-09-bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t_files/figure-html/unnamed-chunk-4-1.png" width="312" /></p>
<p>And here are the <code>x.noisy</code> data.</p>
<pre class="r"><code>x.noisy %&gt;% 
  ggplot(aes(x = x, y = y, color = outlier, fill = outlier)) +
  geom_point() +
  stat_ellipse(geom = &quot;polygon&quot;, alpha = .15, size = .15, level = .5) +
  stat_ellipse(geom = &quot;polygon&quot;, alpha = .15, size = .15, level = .95) +
  scale_color_fivethirtyeight() +
  scale_fill_fivethirtyeight() +
  coord_cartesian(xlim = -50:50,
                  ylim = -100:100) +
  theme_fivethirtyeight() +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="/post/2019-02-09-bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t_files/figure-html/unnamed-chunk-5-1.png" width="312" /></p>
<p>The three outliers are in red. Even in their presence, the old interocular trauma test suggests there is a pronounced overall trend in the data. I would like a correlation procedure that’s capable of capturing that overall trend. Let’s examine some candidates.</p>
</div>
<div id="how-does-old-pearson-hold-up" class="section level2">
<h2>How does old Pearson hold up?</h2>
<p>A quick way to get a Pearson’s correlation coefficient in R is with the <code>cor()</code> function, which does a nice job recovering the correlation we simulated the <code>x.clean</code> data with:</p>
<pre class="r"><code>cor(x.clean$x, x.clean$y)</code></pre>
<pre><code>## [1] -0.959702</code></pre>
<p>However, things fall apart if you use <code>cor()</code> on the <code>x.noisy</code> data.</p>
<pre class="r"><code>cor(x.noisy$x, x.noisy$y)</code></pre>
<pre><code>## [1] -0.6365649</code></pre>
<p>So even though most of the <code>x.noisy</code> data continue to show a clear strong correlation, three outlier values reduce the Pearson’s correlation a third of the way toward zero Let’s see what happens when we go Bayesian.</p>
</div>
<div id="bayesian-correlations-in-brms" class="section level2">
<h2>Bayesian correlations in brms</h2>
<p><a href="https://twitter.com/paulbuerkner">Bürkner</a>’s brms is a general purpose interface for fitting all manner of Bayesian regression models with <a href="https://mc-stan.org">Stan</a> as the engine under the hood. It has popular <a href="https://cran.r-project.org/web/packages/lme4/index.html">lme4</a>-like syntax and offers a variety of convenience functions for post processing. Let’s load it up.</p>
<pre class="r"><code>library(brms)</code></pre>
<div id="first-with-the-gaussian-likelihood." class="section level3">
<h3>First with the Gaussian likelihood.</h3>
<p>I’m not going to spend a lot of time walking through the syntax in the main brms function, <code>brm()</code>. You can learn all about that <a href="https://github.com/paul-buerkner/brms">here</a> or with my project <a href="https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse#statistical-rethinking-with-brms-ggplot2-and-the-tidyverse"><em>Statistical Rethinking with brms, ggplot2, and the tidyverse</em></a>. But our particular use of <code>brm()</code> requires we make a few fine points.</p>
<p>One doesn’t always think about bivariate correlations within the regression paradigm. But they work just fine. Within brms, you can use the conventional Gaussian likelihood (i.e., <code>family = gaussian</code>), use the <code>cbind()</code> syntax to set up a <a href="https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html">multivariate model</a>, and fit that model without predictors. For each variable specified in <code>cbind()</code>, you’ll estimate an intercept (i.e., mean, <span class="math inline">\(\mu\)</span>) and sigma (i.e., <span class="math inline">\(\sigma\)</span>, often called a residual variance). Since there are no predictors in the model, the residual variance is just the variance and the brms default for multivariate models is to allow the residual variances to covary. But since variances are parameterized in the standard deviation metric in brms, the residual variances and their covariance are <em>SD</em>s and their correlation, respectively. Here’s what it looks like in practice.</p>
<pre class="r"><code>f0 &lt;- 
  brm(data = x.clean, 
      family = gaussian,
      cbind(x, y) ~ 1,
      prior = c(prior(normal(0, 100), class = Intercept),
                prior(normal(0, 100), class = sigma, resp = x),
                prior(normal(0, 100), class = sigma, resp = y),
                prior(lkj(1), class = rescor)),
      iter = 2000, warmup = 500, chains = 4, cores = 4, 
      seed = 210191)</code></pre>
<p>In a typical Bayesian workflow, you’d examine the quality of the chains with trace plots. The easy way to do that in brms is with <code>plot()</code>. E.g., to get the trace plots for our first model, you’d code <code>plot(f0)</code>. Happily, the trace plots look fine for all models in this post. For the sake of space, I’ll leave their inspection as an exercises for interested readers.</p>
<p>Our priors and such mirror those in Baez-Ortega’s blog. Here are the results.</p>
<pre class="r"><code>print(f0)</code></pre>
<pre><code>##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: x ~ 1 
##          y ~ 1 
##    Data: x.clean (Number of observations: 40) 
## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1;
##          total post-warmup samples = 6000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## x_Intercept    -2.83      3.33    -9.33     3.69       2995 1.00
## y_Intercept     3.55      6.65    -9.45    16.60       2972 1.00
## 
## Family Specific Parameters: 
##         Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma_x    21.47      2.47    17.29    26.99       2453 1.00
## sigma_y    42.93      4.86    34.55    53.51       2418 1.00
## 
## Residual Correlations: 
##             Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## rescor(x,y)    -0.95      0.02    -0.98    -0.92       2636 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Way down there in the last line in the ‘Family Specific Parameters’ section we have <code>rescor(x,y)</code>, which is our correlation. And indeed, our Gaussian intercept-only multivariate model did a great job recovering the correlation we used to simulate the <code>x.clean</code> data with. Look at what happens when we try this approach with <code>x.noisy</code>.</p>
<pre class="r"><code>f1 &lt;-
  update(f0,
         newdata = x.noisy,
         iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 210191)</code></pre>
<pre class="r"><code>print(f1)</code></pre>
<pre><code>##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: x ~ 1 
##          y ~ 1 
##    Data: x.noisy (Number of observations: 40) 
## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1;
##          total post-warmup samples = 6000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## x_Intercept    -2.95      3.75   -10.39     4.57       4477 1.00
## y_Intercept     6.52      7.45    -8.31    20.98       4692 1.00
## 
## Family Specific Parameters: 
##         Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma_x    23.65      2.76    18.97    29.83       4312 1.00
## sigma_y    47.20      5.42    37.94    59.03       4332 1.00
## 
## Residual Correlations: 
##             Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## rescor(x,y)    -0.61      0.10    -0.78    -0.39       4480 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>And the correlation estimate is -.6. As it turns out, <code>data = x.noisy</code> + <code>family = gaussian</code> in <code>brm()</code> failed us just like Pearson’s correlation failed us. Time to leave failure behind.</p>
</div>
<div id="now-with-students-t-distribution." class="section level3">
<h3>Now with Student’s <span class="math inline">\(t\)</span>-distribution.</h3>
<p>Before we jump into using <code>family = student</code>, we should talk a bit about <span class="math inline">\(\nu\)</span>. This is our new parameter which is silently fixed to infinity when we use the Gaussian likelihood. The <span class="math inline">\(\nu\)</span> parameter is bound at zero but, as discussed in Baez-Ortega’s blog, is somewhat nonsensical for values below 1. As it turns out, <span class="math inline">\(\nu\)</span> is constrained to be equal to or greater than 1 in brms. So nothing for us to worry about, there. The <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">Stan team currently recommends the gamma(2, 0.1) prior for <span class="math inline">\(\nu\)</span></a>, which is also the current brms default. This is what that distribution looks like.</p>
<pre class="r"><code>tibble(x = seq(from = 1, to = 120, by = .5)) %&gt;% 
  ggplot(aes(x = x, fill = factor(0))) +
  geom_ribbon(aes(ymin = 0, 
                  ymax = dgamma(x, 2, 0.1))) +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_fill_fivethirtyeight() +
  coord_cartesian(xlim = 0:100) +
  ggtitle(&quot;gamma(2, 0.1)&quot;) +
  theme_fivethirtyeight() +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="/post/2019-02-09-bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t_files/figure-html/unnamed-chunk-11-1.png" width="960" /></p>
<p>So gamma(2, 0.1) should gently push the <span class="math inline">\(\nu\)</span> posterior toward low values, but it’s slowly-sloping right tail will allow higher values to emerge.</p>
<p>Following the Stan team’s recommendation, the brms default and Baez-Ortega’s blog, here’s our robust Student’s <span class="math inline">\(t\)</span> model for the <code>x.noisy</code> data.</p>
<pre class="r"><code>f2 &lt;- 
  brm(data = x.noisy, 
      family = student,
      cbind(x, y) ~ 1,
      prior = c(prior(gamma(2, .1), class = nu),
                prior(normal(0, 100), class = Intercept),
                prior(normal(0, 100), class = sigma, resp = x),
                prior(normal(0, 100), class = sigma, resp = y),
                prior(lkj(1), class = rescor)),
      iter = 2000, warmup = 500, chains = 4, cores = 4, 
      seed = 210191)</code></pre>
<pre class="r"><code>print(f2)</code></pre>
<pre><code>##  Family: MV(student, student) 
##   Links: mu = identity; sigma = identity; nu = identity
##          mu = identity; sigma = identity; nu = identity 
## Formula: x ~ 1 
##          y ~ 1 
##    Data: x.noisy (Number of observations: 40) 
## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1;
##          total post-warmup samples = 6000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## x_Intercept    -2.11      3.61    -9.22     4.96       2936 1.00
## y_Intercept     1.93      7.12   -11.74    16.03       2949 1.00
## 
## Family Specific Parameters: 
##         Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma_x    18.26      2.92    13.06    24.48       3188 1.00
## sigma_y    36.31      5.79    26.08    48.60       3206 1.00
## nu          2.65      1.00     1.36     5.13       3905 1.00
## 
## Residual Correlations: 
##             Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## rescor(x,y)    -0.93      0.03    -0.97    -0.84       3484 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Whoa, look at that correlation, <code>rescore(x,y)</code>! It’s right about what we’d hope for. Sure, it’s not a perfect -.95, but that’s way way way better than -0.61.</p>
<p>While we’re at it, we may as well see what happens when we fit a Student’s <span class="math inline">\(t\)</span> model when we have perfectly multivariate normal data. Here it is with the <code>x.clean</code> data.</p>
<pre class="r"><code>f3 &lt;- 
  update(f2,
         newdata = x.clean, 
         iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 210191)</code></pre>
<pre class="r"><code>print(f3)</code></pre>
<pre><code>##  Family: MV(student, student) 
##   Links: mu = identity; sigma = identity; nu = identity
##          mu = identity; sigma = identity; nu = identity 
## Formula: x ~ 1 
##          y ~ 1 
##    Data: x.clean (Number of observations: 40) 
## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1;
##          total post-warmup samples = 6000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## x_Intercept    -2.37      3.50    -9.20     4.39       2909 1.00
## y_Intercept     2.71      6.98   -10.90    16.48       3032 1.00
## 
## Family Specific Parameters: 
##         Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma_x    20.79      2.60    16.28    26.60       2388 1.00
## sigma_y    41.34      5.17    32.33    52.62       2417 1.00
## nu         22.42     13.78     5.70    57.53       4384 1.00
## 
## Residual Correlations: 
##             Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## rescor(x,y)    -0.96      0.01    -0.98    -0.92       3045 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>So when you don’t need Student’s <span class="math inline">\(t\)</span>, it yields the right answer anyways. That’s a nice feature.</p>
<p>We should probably compare the posteriors of the correlations across the four models. First we’ll collect the posterior samples into a tibble.</p>
<pre class="r"><code>posts &lt;-
  tibble(model = str_c(&quot;f&quot;, 0:3)) %&gt;% 
  mutate(fit  = map(model, get)) %&gt;% 
  mutate(post = map(fit, posterior_samples)) %&gt;% 
  unnest(post)

head(posts)</code></pre>
<pre><code>## # A tibble: 6 x 8
##   model b_x_Intercept b_y_Intercept sigma_x sigma_y rescor__x__y  lp__
##   &lt;chr&gt;         &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;
## 1 f0             1.31        -5.60     18.2    37.8       -0.947 -353.
## 2 f0            -7.41        10.6      25.2    50.5       -0.941 -357.
## 3 f0            -4.51         5.65     23.3    49.4       -0.975 -354.
## 4 f0            -2.65        -0.597    18.3    37.3       -0.929 -354.
## 5 f0            -2.76        -1.50     18.4    37.5       -0.923 -355.
## 6 f0            -9.84        15.2      26.3    45.1       -0.953 -358.
## # ... with 1 more variable: nu &lt;dbl&gt;</code></pre>
<p>With the posterior draws in hand, we just need to wrangle a bit before showing the correlation posteriors in a coefficient plot. To make things easier, we’ll do so with a couple convenience functions from the <a href="https://github.com/mjskay/tidybayes">tidybayes</a> package.</p>
<pre class="r"><code>library(tidybayes)

# wrangle
posts %&gt;% 
  group_by(model) %&gt;% 
  median_qi(rescor__x__y, .width = c(.5, .95)) %&gt;% 
  mutate(key = recode(model, 
                      f0 = &quot;Gaussian likelihood with clean data&quot;,
                      f1 = &quot;Gaussian likelihood with noisy data&quot;,
                      f2 = &quot;Student likelihood with noisy data&quot;,
                      f3 = &quot;Student likelihood with clean data&quot;),
         clean = ifelse(model %in% c(&quot;f0&quot;, &quot;f3&quot;), &quot;0&quot;, &quot;1&quot;)) %&gt;%
  
  # plot
  ggplot(aes(x = rescor__x__y, y = key, color = clean)) +
  geom_pointintervalh() +
  scale_color_fivethirtyeight() +
  coord_cartesian(xlim = -1:0) +
  labs(subtitle = expression(paste(&quot;The posterior for &quot;, rho, &quot; depends on the likelihood. Why not go robust and use Student&#39;s &quot;, italic(t), &quot;?&quot;))) +
  theme_fivethirtyeight() +
  theme(axis.text.y     = element_text(hjust = 0),
        legend.position = &quot;none&quot;)</code></pre>
<p><img src="/post/2019-02-09-bayesian-robust-correlations-with-brms-and-why-you-should-love-student-s-t_files/figure-html/unnamed-chunk-15-1.png" width="960" /></p>
<p>From out <code>tidybayes::median_qi()</code> code, the dots are the posterior medians, the thick inner lines the 50% intervals, and the thinner outer lines the 95% intervals. The posteriors for the <code>x.noisy</code> data are in red and those for the <code>x.clean</code> data are in blue. If the data are clean multivariate normal Gaussian or if they’re dirty but fit with robust Student’s <span class="math inline">\(t\)</span>, everything is pretty much alright. But whoa, if you fit a correlation with a combination of <code>family = gaussian</code> and noisy outlier-laden data, man that’s just a mess.</p>
<p>Don’t make a mess of your data. Try the robust Student’s <span class="math inline">\(t\)</span>.</p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] tidybayes_1.0.3 brms_2.7.0      Rcpp_1.0.0      ggthemes_4.0.1 
##  [5] bindrcpp_0.2.2  forcats_0.3.0   stringr_1.3.1   dplyr_0.7.6    
##  [9] purrr_0.2.5     readr_1.1.1     tidyr_0.8.1     tibble_1.4.2   
## [13] ggplot2_3.1.0   tidyverse_1.2.1 mvtnorm_1.0-8  
## 
## loaded via a namespace (and not attached):
##   [1] colorspace_1.3-2          ggridges_0.5.0           
##   [3] rsconnect_0.8.8           rprojroot_1.3-2          
##   [5] ggstance_0.3              markdown_0.8             
##   [7] base64enc_0.1-3           rstudioapi_0.7           
##   [9] rstan_2.18.2              svUnit_0.7-12            
##  [11] DT_0.4                    lubridate_1.7.4          
##  [13] xml2_1.2.0                bridgesampling_0.4-0     
##  [15] mnormt_1.5-5              knitr_1.20               
##  [17] shinythemes_1.1.1         bayesplot_1.6.0          
##  [19] jsonlite_1.5              broom_0.4.5              
##  [21] shiny_1.1.0               compiler_3.5.1           
##  [23] httr_1.3.1                backports_1.1.2          
##  [25] assertthat_0.2.0          Matrix_1.2-14            
##  [27] lazyeval_0.2.1            cli_1.0.1                
##  [29] later_0.7.3               htmltools_0.3.6          
##  [31] prettyunits_1.0.2         tools_3.5.1              
##  [33] igraph_1.2.1              coda_0.19-2              
##  [35] gtable_0.2.0              glue_1.3.0               
##  [37] reshape2_1.4.3            cellranger_1.1.0         
##  [39] nlme_3.1-137              blogdown_0.8             
##  [41] crosstalk_1.0.0           psych_1.8.4              
##  [43] xfun_0.3                  ps_1.2.1                 
##  [45] rvest_0.3.2               mime_0.5                 
##  [47] miniUI_0.1.1.1            gtools_3.8.1             
##  [49] MASS_7.3-50               zoo_1.8-2                
##  [51] scales_1.0.0              colourpicker_1.0         
##  [53] hms_0.4.2                 promises_1.0.1           
##  [55] Brobdingnag_1.2-5         parallel_3.5.1           
##  [57] inline_0.3.15             shinystan_2.5.0          
##  [59] yaml_2.1.19               gridExtra_2.3            
##  [61] StanHeaders_2.18.0-1      loo_2.0.0                
##  [63] stringi_1.2.3             dygraphs_1.1.1.5         
##  [65] pkgbuild_1.0.2            rlang_0.3.0.1            
##  [67] pkgconfig_2.0.1           matrixStats_0.54.0       
##  [69] evaluate_0.10.1           lattice_0.20-35          
##  [71] bindr_0.1.1               rstantools_1.5.0         
##  [73] htmlwidgets_1.2           labeling_0.3             
##  [75] tidyselect_0.2.4          processx_3.2.1           
##  [77] plyr_1.8.4                magrittr_1.5             
##  [79] bookdown_0.7              R6_2.3.0                 
##  [81] pillar_1.2.3              haven_1.1.2              
##  [83] foreign_0.8-70            withr_2.1.2              
##  [85] xts_0.10-2                abind_1.4-5              
##  [87] modelr_0.1.2              crayon_1.3.4             
##  [89] arrayhelpers_1.0-20160527 utf8_1.1.4               
##  [91] rmarkdown_1.10            grid_3.5.1               
##  [93] readxl_1.1.0              callr_3.1.0              
##  [95] threejs_0.3.1             digest_0.6.18            
##  [97] xtable_1.8-2              httpuv_1.4.4.2           
##  [99] stats4_3.5.1              munsell_0.5.0            
## [101] shinyjs_1.0</code></pre>
</div>
</div>

    </div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/bayesian/">Bayesian</a>
  
  <a class="badge badge-light" href="/tags/brms/">brms</a>
  
  <a class="badge badge-light" href="/tags/outlier/">outlier</a>
  
  <a class="badge badge-light" href="/tags/r/">R</a>
  
  <a class="badge badge-light" href="/tags/robust/">robust</a>
  
  <a class="badge badge-light" href="/tags/tutorial/">tutorial</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/robust-linear-regression-with-the-robust-student-s-t-distribution/">Robust Linear Regression with Student’s $t$-Distribution</a></li>
        
        <li><a href="/post/make-rotated-gaussians-kruschke-style/">Make rotated Gaussians, Kruschke style</a></li>
        
        <li><a href="/post/bayesian-meta-analysis/">Bayesian meta-analysis in brms</a></li>
        
        <li><a href="/post/how-bookdown/">bookdown, My Process</a></li>
        
      </ul>
    </div>
    

    

    


  </div>
</article>

<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2018 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "Search Results",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    <script src="/js/search.js"></script>
    

    
    

  </body>
</html>

